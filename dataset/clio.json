{
    "research_proposal": "CLIO (Cognitive Loop via In-Situ Optimization) - An alternative approach to post-training reasoning models that enables large language models to self-formulate ways of approaching problems, adapt behavior when self-confidence is low, and provide scientists with precise control over the reasoning process. CLIO uses recursive reflection and in-situ optimization to increase non-reasoning models' ability to think through problems without requiring additional training data or post-training cycles. The system incorporates uncertainty monitoring, graph-based belief reduction, and real-time steering capabilities for scientific discovery applications.",
    
    "benchmark": "Humanity's Last Exam (HLE) - 152 text-based Biology and Medicine questions covering genetics, biology, ecology, neuroscience, biochemistry, microbiology, immunology, molecular biology, computational biology, biophysics, bioinformatics, genomics, and physiology",
    
    "performance_metrics": "22.37% accuracy (34/152 correct answers) representing a 13.82% net increase or 161.64% relative improvement over base GPT-4.1 (8.55% accuracy). Outperformed OpenAI's o3 in high reasoning effort mode (13.81%) and low reasoning effort mode (11.18%). With 'more thinking' mode, achieved additional 7.90% performance increase.",
    
    "paper_link": "https://www.arxiv.org/pdf/2508.02789",
    
    "full_text": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science\n\nNewman Cheng∗, Gordon Broadbent∗, William Chappell\nMicrosoft Discovery and Quantum, Office of the Chief Technology Officer\n\nAbstract\nThe capacity for artificial intelligence (AI) to formulate, evolve, and test altered thought patterns under dynamic conditions indicates advanced cognition that is crucial for scientific discovery. The existing AI development landscape falls into two categories: 1) frameworks over non-reasoning models that natively incorporate opinions on how humans think, and 2) reasoning models that abstract precise control of the reasoning intuition away from end users. While powerful, for scientists to maximize utility of AI in scientific discovery, they not only require accuracy and transparency in reasoning, but also steerability. Hence, we introduce an alternative approach that enables deep and precise control over the reasoning process called: a cognitive loop via in-situ optimization (CLIO). CLIO enables large language models (LLMs) to self-formulate ways of approaching a problem, adapt behavior when self-confidence is low, and ultimately provide scientists with a final belief or answer. Through CLIO's open design, scientists can observe uncertainty levels, understand how final belief states are formulated using graph structures, and interject corrections. Without any further post-training, OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37% in text-based biology and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82% net or 161.64% relative increase when compared to the base GPT-4.1 model and surpasses OpenAI's o3 performance in high and low reasoning effort modes. We further discovered that oscillations within internal uncertainty measures are key in determining the accuracy of CLIO's results, revealing how its open design and internal mechanisms can provide insight and control into scientific decision-making processes.\n\n1 Introduction\nLong-running LLM agents that reason, plan, and execute high-value tasks over long temporal periods, such as in the creation of new materials or drugs, are poised to transform scientific discovery. Currently, methods to produce long-form reasoning are based on post-training cycles prior to model deployment. These corresponding models possess reasoning strategies and instinctual properties dictated by model providers without direct input from end users, relegating them to steer behavior utilizing the context window alone. In this paper, we demonstrate an alternative approach that enables end users to steer and correct thought patterns in real-time or post-hoc while achieving the same or greater-level intelligence when compared to a post-trained model. Our approach, CLIO, is designed as an alternative or complement to the reinforcement learning post-training step with recursive reflection to increase non-reasoning models' ability to think through problems and select the best approach.\n\nWhile HLE scores are increasing from new models, with trends showing that it will be saturated akin to other benchmarks, accuracy is no longer the only measure of success. Now, the real challenge is active inclusion of the end user, bypassing passive user observation. To address this challenge, we tested CLIO's ability to answer biology and medicine questions from HLE with no tools. A model-only approach allows for a direct test of the efficacy between reinforcement learning models when compared to in-situ optimization of non-reasoning models. CLIO does more than just increase performance; CLIO brings the end user along throughout the creation of the entire thought process, increasing performance and explainability into internal behavior. Metrics, such as uncertainty oscillation or number of new brainstorming perspectives, enable an understanding of when results and decisions being made by the model can be trusted. As we embark on this new age of discovery where answers will be as good or better than human subject matter experts, the challenge has shifted to creating the balance between human and machine teaming. Understanding when experts need to interject without oversaturation and ensuring critical decisions are steerable is crucial. Ultimately, CLIO puts these principles at the forefront.\n\n2 Prior Work\n\n2.1 Reasoning Approaches\nThe neuroplasticity of the human brain is central to how humans learn, remember, and adapt.[1] As such, human brains possess the ability to create, modify, or remove neural connections based on experiences.[2] Designing cognitive-like systems that can think increasingly like humans requires the fundamental ability to formulate different approaches to problems, receive external feedback, and, subsequently, pivot strategies.[3–6] Searching for new strategies when unsuccessful and saving useful patterns for reuse is key in scaling developed knowledge.[7, 8] This approach is central to post-training models today and is the cornerstone of reinforcement learning (RL). Evidence shows continual scaling as reasoning models solve increasingly complex, multi-step reasoning problems within verifiable domains.[9, 10]\n\nThe current phase of RL training predominantly utilizes externally verifiable rewards (RLVR)to provide feedback to the model.[11] An example of RLVR is a binary reward for whether an LLM-produced code segment compiled or not. Recent works, like \"Learning to Reason without External Rewards,\" focus on the use of RL training with internal feedback (RLIR), which trades externally verifiable and human-defined rewards in favor of internal confidence signals or self-certainty.[10] RLIR results demonstrate how internally computed or estimated rewards can scale performance during post-training. In addition, RLIR aims to reduce dependence solely on externally verifiable human rewards utilized in reinforcement learning with human feedback (RLHF). However, end users utilizing RLVR, RLIR, or RLHF post-trained models all share the same fundamental limitations: the learning or reasoning process elicited is neither personal, nor steerable as learning terminates once the trained model is handed to the user.[12] Enabling such customization requires further post-training on high-quality data which often does not exist for cutting-edge scientific discoveries.[13]\n\n2.2 Systems that Evolve Knowledge\nRecent work by Google's AlphaEvolve demonstrates how AI systems can continue to scale inference-time compute to discover or find known state-of-the-art algorithms through an evolutionary process.[14] Throughout AlphaEvolve's discovery process, the use of verifiable rewards ranks and prioritizes subsequent algorithms that AlphaEvolve should use to evolve, reducing the burden on LLMs alone to control the direction of exploration. This prioritization is critical as it narrows the search space and complexity necessary for downstream evolution. Hence, it enables the incremental evolution of knowledge, that is lower in complexity, to be aggregated and used in answering highly complex problems. As Apple's \"Illusion of Thinking\" paper highlights, models are particularly performant in solving problems that contain a smaller combinatorial or exponential search space, but quickly fall apart as that space grows linearly or exponentially.[15] By reducing the complexity of problems required for LLMs to address at each individualized state, complex search spaces can theoretically be mapped and reduced while considering multiple signals of human feedback.[16, 17] Low complexity spaces can be explored using breadth-wise approaches while high complexity spaces require depth wise approaches. Breadth-wise exploration requires a greater sampling of different perspectives or paths.[18, 19] Depth-wise approaches require consecutively building out reasoning paths, prioritizing the most promising ones, and pruning those that are least likely to yield success.[20]\n\n2.3 Science Agents\nGoogle's Co-Scientist and Sakana's AI Scientist have identified the potential utility for long-running agents to assist in scientific discovery.[21, 22] More recent works like TxGemma: Efficient and Agentic LLMs for Therapeutics make use of the HLE benchmark to evaluate their system.[23] Similarly, Stanford's BioMNI: General-Purpose Biomedical AI Agent demonstrates how the use of combining an agentic system with well-curated scientific literature and scientific tools can improve performance on a subset of HLE biology and medicine questions.[24] Both focus on the demonstration of their respective approaches in boosting performance, rather than critical areas of control, central to end users. In practice, FutureHouse's work with Robin utilized a multi-agent architecture to identify a novel treatment for dry age-related macular degeneration and paired it with an iterative lab-in-the-loop to automate portions of the scientific discovery process.[25] This has been extended further where \"self-driving laboratories\" are being used to accelerate the discovery of new materials.[26] While powerful, each of these approaches demonstrates areas of concern when scientists are unable to control the reasoning process of LLMs. The lack of steerability in deeply technical and high-stakes domains like scientific discovery or diagnostic reasoning is a clear gap.[27] We address these limitations through CLIO.\n\n3 Methods\nOur approach leverages the inference-time evaluation of progress and self-confidence to optimize thinking in real time without additional training cycles. Furthermore, unlike the post-training reinforcement learning process which requires high-quality data, CLIO does not require additional data or training. Instead, our approach makes use of internal self-confidence and reflection to reason through and adapt to problems at inference time. While the need for additional training data or compute cycles is obviated, the core challenge comes in a practical system design that prioritizes the shape of human cognitive behaviors from the outset.\n\n3.1 The Recursive Nature of Cognition\nThe human thought process is not always linear, nor is it direct.[28] For example, problem solving is non-linear in nature as it often requires both the formulation of the potential paths that may emerge and selection of the most promising paths.[29] Through this framing, we designed CLIO to possess both breadth- and depth-wise exploration capabilities. To enable breadth, CLIO builds on the inspiration of existing approaches like chain-of-thought prompting, and reasoning then acting (ReAct) when using tools.[30, 31] The key difference from other systems is that CLIO does not rely on prompting to construct thought patterns, rather orchestration of thought is native to its design. This orchestration pattern still follows the \"feed-forward\" nature of knowledge, allowing CLIO to explore many different options as desired.\n\nConversely, to enable depth, CLIO introduces a new utilization of recursion that balances the challenge of controlling semantic explosion in adaptive contexts with algorithmically defined constraints. CLIO's ability to invoke itself enables the formulation of individualized-thought channels that possess independent context windows. These clean context windows are crucial when CLIO dives into a particular area of exploration, without polluting the aggregated context with incomplete thoughts. Hence, this design enables exploration across many different avenues in depth and adaptation to changes, when necessary, within or across thought channels.\n\nTo control CLIO's depth of exploration, we enabled the system to self-reflect and determine the necessary duration for how long it needs to self-reflect based on a maximum recursion, or maximum cognitive depth. We observed that without the proper algorithmic control on recursion, these systems continue to goad themselves into further exploration, leading to fundamentally incorrect results over long periods of time. Hence, control over cognitive depth is crucial and akin to reasoning effort levels from models like o3, where low/medium/high effort levels are configurable. The key difference is that CLIO controls both the depth of exploration and the iterative assumptions made at each step. Each of these assumptions can be altered or weighed differently. While exploring, CLIO can self-optimize its own internal strategy through a series of parameters that are editable when invoking itself. These areas enable CLIO's persona to be modified, changing the area of focus or problem-solving approach. Most commonly, we observe CLIO utilizing this optimization component to resolve uncertainties it has self-recognized throughout the execution process. Hence, this utilization at runtime directly influences the optimization of the system's internal belief structures over time.\n\n3.2 Building Awareness into CLIO\nWithin each thought channel, CLIO's runtime is aware of when to complete its thought processes. To do so, we enabled a special \"completion\" function that can be invoked to terminate the thought channel. However, proper invocation of this termination function requires both sufficient confidence and a temporal understanding of the progress made by the system. Integrating temporal awareness is key, as the minimization of token-level probabilities alone fails to account for the notion of time. Hence, to prevent early termination, we enabled CLIO to dynamically register the special completion function only when a certain threshold of coverage was met. Under this construct, reduction of bias is critical when solving complex challenge problems that possess potentially multiple correct answers. The precedence of those assumptions contains minimal variations in themselves but leads to highly influential effects downstream when reasoning through and selecting the proper approach. Across these possible reasoning paths constructed during the breadth- and depth-wise exploration phase, the mechanism of attention serves as a critical system internal process. Thereby, creating a belief state regarding the significance of various elements can be established. Conceptually, this process is akin to human behavior where humans adopt diverse cognitive perspectives or reasoning strategies before formulating an opinion on plausibility or preference.[32]\n\n3.3 Overcoming the Over-Indexing Challenge\nAgentic optimization approaches often ensemble multiple different perspectives to enable diversity of thought and sampling, to elicit a correct answer. The most common approach to reduce and select an answer from the sampled variations has been a custom prompt-based, facing both limitations in the quality of the prompt as well as bias native within the context window itself.[33, 34] To mitigate these issues raised through ensembling, we leveraged graph structures to reduce noise.\n\nTo build these graph structures, we utilized GPT-4.1 to perform entity and relationship extraction from CLIO's thought processes. These extractions then underwent unsupervised clustering to form hierarchical communities of information that were subsequently summarized and embedded. At runtime, these summaries and embeddings were used to assist in the retrieval process for relevant pieces of information related to CLIO's query.[35] To build a holistic view, we enabled a control knob for CLIO to think more, resulting in multiple different runs, often using different temperature configurations. Each run produced a distinct chain of thought that we subsequently collected and built a joint graph of, using all sampled sequences. To construct the final graph, we leveraged the use of LLM-induced graph structures with unsupervised clustering to produce a representation across all runs. Finally, CLIO produced a response by querying the final graph representation using the original question provided.\n\n4 Evaluation\nTo evaluate CLIO's performance against reasoning models, we compared our system without access to external tools or data against o3 in high- and low-reasoning effort modes. We extended our study to include GPT-4.1 and GPT-4o to illustrate how we can elevate the performance of completion models into those on par with reasoning-class models, all without post-training. Ultimately, we demonstrated how cognitive systems designed with the proper abstractions can be orchestrated to 1) self-formulate thought patterns, 2) self-formulate confidence and feedback, and 3) self-recognize success or failure. We further conducted a series of ablation studies that measured the system's performance from multiple aspects: accuracy with more thinking, similarity of reasoning structure, belief reduction using graph structures, and uncertainty analysis. Overall, we found that CLIO surpassed o3 in both high- and low-reasoning effort modes on the 152 text-based Biology/Medicine questions from Humanity's Last Exam in Pass @ 1 scenarios. On top of accuracy alone, we ran five independent Pass @ 1 scenarios to demonstrate CLIO's stability and fundamentally less variability than that of o3.\n\nFurthermore, our results demonstrated how the utilization of graph structures reduced sporadic noise from stronger false positive arguments, leading to an increasingly balanced perspective when answering the question. This was evident through the performance increase of 7.90% between base GPT-4.1 with CLIO and CLIO paired with \"more thinking\". To obtain our Pass @ 1 metric, we leveraged dynamic reasoning and inference with flexible traversal (DRIFT) search on the ensembled graph representation. DRIFT search is a technique that combines global and local queries to obtain information spanning multiple conceptual representations in the graph.\n\n[Complete detailed results tables and analysis continue with specific performance breakdowns by question category, uncertainty analysis, case studies, future work discussions, and conclusions as provided in the original paper text...]"
  }