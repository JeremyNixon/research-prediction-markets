[{
  "research_proposal": "AdaptFlow introduces a meta-learning-based framework for optimizing LLM agent workflows via a bi-level optimization strategy. The inner loop refines workflows for semantically clustered subtasks using LLM-generated textual feedback and symbolic updates, continuing only if improvements are observed. The outer loop aggregates these refinements into a shared initialization, enhanced with reflection on failure cases. At test time, this initialization is adapted to unseen subtasks based on semantic descriptions, enabling efficient test-time adaptation.",
  "benchmark": ["HotpotQA", "DROP", "HumanEval", "MBPP", "GSM8K", "MATH", "AIME", "Olympiad"],
  "performance_metrics": [0.3, 1.8, 0.0, 0.0, 0.6, 0.8, 5.3, 5.9],
  "paper_link": "https://arxiv.org/html/2508.08053v1",
  "full_text": "AdaptFlow: Adaptive Workflow Optimization via Meta-LearningRunchuan Zhu1, Bowen Jiang11, Lingrui Mei2, Fangkai Yang3,Lu Wang3, Haoxiang Gao1, Fengshuo Bai4, Pu Zhao3,Qingwei Lin3, Saravan Rajmohan3, Dongmei Zhang31Peking University, 2University of Chinese Academy of Sciences,3Microsoft, 4Shanghai Jiaotong University,Equal contribution. For inquiries, please contact: zhurunchuan@stu.pku.edu.cn.Work is done during an internship at MicrosoftCorresponding author.AbstractRecent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.AdaptFlow: Adaptive Workflow Optimization via Meta-LearningRunchuan Zhu1â€ â€ , Bowen Jiang11, Lingrui Mei2, Fangkai Yang3â€ , Lu Wang3, Haoxiang Gao1, Fengshuo Bai4, Pu Zhao3, Qingwei Lin3, Saravan Rajmohan3, Dongmei Zhang3 1Peking University, 2University of Chinese Academy of Sciences, 3Microsoft, 4Shanghai Jiaotong University,1 IntroductionRecent progress in Large Language Models (LLMs) Achiam et al. (2023); Guo et al. (2025); Mei et al. (2024) has led to remarkable performance across diverse tasks, including question answering Rajpurkar et al. (2016); Yang et al. (2018); Ding et al. (2024); Jiang et al. (2025), code synthesis Chen et al. (2021); Nijkamp et al. (2023); Mei et al. (2025), and multi-turn dialogue Zhang et al. (2020); Bai et al. (2022); Zhu et al. (2025b, a). Beyond static prediction, LLMs are increasingly being used as decision-making agents capable of dynamic reasoning and adaptive behavior Shinn et al. (2023); Wei et al. (2022); Yao et al. (2023). This development has given rise to the notion of agentic workflows, which organize LLMs into structured sequences of actions involving task decomposition, planning, tool use, execution, and self-reflection Yao et al. (2023); Creswell and Shanahan (2023). Such workflows have demonstrated strong performance in settings that require multi-step reasoning Yao et al. (2023); Creswell and Shanahan (2023), long-horizon planning Liu et al. (2023); Zhou et al. (2024b), and external tool integration Schick et al. (2023); Qin et al. (2023).While effective in controlled settings, manually designing agentic workflows is time-consuming and lacks scalability across diverse tasks. To address this, recent work has explored automated workflow construction through prompt optimization Khattab et al. (2023); Chen et al. (2023), hyperparameter tuning Li et al. (2024b); Wang et al. (2025), and structural search Liu et al. (2024); Song et al. (2024); Zhang et al. (2024a). However, many of these methods Liu et al. (2024); Zhang et al. (2024a) represent workflows using fixed graph structures, which inherently limit the flexibility of the agentic workflow search space.Recent frameworks such as ADAS Hu et al. (2024) and Aflow Zhang et al. (2024b) adopt code-based workflow representations to enable robust and flexible search. However, as noted by Wang et al. (2025), these methods typically generate a single static workflow for the entire task set, limiting their ability to generalize across heterogeneous datasets with diverse problem types. In addition, ADAS performs coarse-grained workflow updates, resulting in redundant context accumulation and growing complexity that hinders convergence. Aflow alleviates some of these issues using Monte Carlo Tree Search, but its reliance on discrete updates and early pruning can restrict the exploration of more expressive workflow candidates. These limitations underscore two challenges simultaneously: C1. How to adaptively construct effective workflows for datasets containing diverse problems? C2. How to ensure convergent optimization in code search spaces?To tackle these challenges, we propose AdaptFlowâ€”a meta-optimization framework that incorporates principles from Model-Agnostic Meta-Learning (MAML) Finn et al. (2017) into agentic workflow optimization. MAML learns an initialization that enables rapid adaptation to new tasks via a bi-level optimization: the inner loop performs task-specific updates, and the outer loop updates the initialization to generalize across tasks. Inspired by this structure, AdaptFlow learns a shared workflow initialization that can quickly adapt to diverse subtasks through symbolic updates guided by LLM-generated feedback. Specifically, AdaptFlow follows a bi-level optimization scheme, where the inner loop iteratively refines the workflow based on subtask-level feedback, while the outer loop consolidates these refinements into a generalizable initialization. To further enhance adaptability, we perform an additional unsupervised adaptation step at test time on each target subtask, leveraging semantic descriptions derived from input prompts. This design enables both effective subtask-specific adaptation (addressing C1) and stable convergence in code spaces (addressing C2), offering a scalable solution for general-purpose workflow construction.Our key contributions are summarized as follows:    â€¢    We introduce AdaptFlow, a meta-learning framework that integrates the MAML paradigm with natural language supervision. AdaptFlow replaces conventional gradients with textual gradients, which are natural language feedback generated by large language models. This mechanism enables efficient subtask-level adaptation within the programmatic code space.    â€¢    We design a bi-level optimization framework tailored for code space. In the inner loop, workflows are iteratively refined using LLM-generated textual feedback. To ensure meaningful and stable updates, we introduce a binary continuation signal that determines whether each update leads to a non-trivial performance gain. In the outer loop, we aggregate subtask-level improvements into a shared initialization, further enhanced by a reflection step that revisits failure cases to improve robustness and convergence.    â€¢    Experiments on benchmarks in question answering, code generation, and mathematical reasoning show that AdaptFlow outperforms both manual workflows and prior baselines, achieving state-of-the-art results with strong model-agnostic generalization.2 Related Work2.1 Agentic WorkflowAgentic workflows provide a structured alternative to autonomous agents for deploying LLMs. Instead of learning through environment interaction Zhuge et al. (2023); Hong et al. (2024b), they execute static or semi-static sequences inspired by human reasoning Zhang et al. (2024b), offering better interpretability and modularity.Workflows can be general purpose, incorporating reusable patterns such as chain-of-thought prompting, self-refinement, or role decomposition Wei et al. (2022); Shinn et al. (2023)â€”or domain-specific, tailored for areas such as code generation Hong et al. (2024c); Ridnik et al. (2024); Zhao et al. (2024), data analysis Xie et al. (2024); Ye et al. (2024); Li et al. (2024a), mathematics Zhong et al. (2024); Xu et al. (2024), and complex QA Nori et al. (2023); Zhou et al. (2024a). While effective, manually designed workflows require significant human effort and lack adaptability, motivating automated optimization.Refer to captionFigure 1: An analogy between Neural Network Optimization and Workflow Optimization, as well as between MAML and AdaptFlow.2.2 Agentic Workflow OptimizationRecent advances Hu et al. (2024); Zhang et al. (2024b); Wang et al. (2025); Li et al. (2024b); Chen et al. (2023); Song et al. (2024); Hong et al. (2024c) have explored automating agentic workflows to improve LLM performance. Some methods focus on optimizing prompts or parameters within fixed workflows Fernando et al. (2023); Guo et al. (2023); Khattab et al. (2023); Saad-Falcon et al. (2024), improving reasoning without altering the execution structure. In contrast, we optimize workflow structures directly, enabling broader adaptation across tasks.Other approaches search over code-based workflows. ADAS Hu et al. (2024) refines linear traces of executable code, while AFLOW Zhang et al. (2024b) introduces compositional abstractions with MCTS. ScoreFlow Wang et al. (2025) frames workflow generation as supervised prediction. However, these methods often produce static workflows and lack task-level adaptability. Our method, AdaptFlow, differs by performing bi-level meta-learning: it adapts workflows via LLM feedback at the subtask level and consolidates them into a generalizable initialization, supporting fast adaptation and robust generalization.3 Preliminaries3.1 Problem FormulationThe goal of automated agentic workflow optimization is to discover effective compositions of modular componentsâ€”such as prompt templates, tool invocations, control logic, and reflection routinesâ€”that can guide LLMs to solve complex tasks across diverse domains.We consider the problem of agentic workflow design, where the goal is to discover an effective workflow ğ’² that can solve a given task ğ’¯ drawn from a distribution. The workflow search is defined by three core components:    â€¢    ğ’® denotes the search space, encompassing all candidate workflows;    â€¢    ğ’¥:ğ’®Ã—ğ’¯â†’â„ is the objective function that quantifies the quality or utility of a workflow ğ’²âˆˆğ’® when applied to a specific task ğ’¯;    â€¢    ğ’œ represents the search algorithm, which explores ğ’® and generates candidate workflows guided by feedback from ğ’¥.Given a task ğ’¯âˆ¼ğ’«â€‹(ğ’¯), the agent seeks to identify an optimal workflow through a task-conditioned search process:	ğ’² 	=ğ’œâ€‹(ğ’®,ğ’¥,ğ’¯), 		(1)	ğ’²â‹† 	=argâ¡maxğ’²âˆˆğ’®ğ”¼ğ’¯âˆ¼ğ’«â€‹(ğ’¯)â€‹[ğ’¥â€‹(ğ’²,ğ’¯)]. 		(2)Building on prior efforts Hu et al. (2024), our method defines the workflow search space directly in the code space, where candidate workflows are represented as executable programs.3.2 Analogy: From Supervised Learning to Agentic Workflow OptimizationIn traditional supervised learning, a model learns a parameterized function fÎ¸ by minimizing the expected loss over labeled data (x,y)âˆ¼ğ’Ÿ:	Î¸â‹† 	=argâ¡minÎ¸ğ”¼(x,y)âˆ¼ğ’Ÿâ€‹[â„’â€‹(fÎ¸â€‹(x),y)], 		(3)	Î¸ 	â†Î¸âˆ’Î·â‹…1Nâ€‹âˆ‘i=1Nâˆ‡Î¸â„’â€‹(fÎ¸â€‹(xi),yi), 		(4)where Î· is the learning rate and {(xi,yi)}i=1N is a mini-batch of training examples. This process relies on differentiable loss functions and explicit ground-truth supervision, enabling gradient-based parameter updates in continuous space.Analogously, agentic workflow optimization operates in a symbolic structure space defined over executable code (e.g., Hu et al. (2024)). Given a task ğ’¯, the system executes a workflow ğ’² and obtains a task-level utility score from the objective function ğ’¥â€‹(ğ’²,ğ’¯). The goal is to discover a workflow that maximizes the expected utility across a distribution of tasks:	ğ’² 	â†ğ’°1â€‹(ğ’²,âˆ‡~â€‹ğ’¥â€‹(ğ’²,ğ’¯)). 		(5)Here, ğ’¥â€‹(ğ’²,ğ’¯) denotes a natural language evaluation of a workflowâ€™s performance on task ğ’¯, serving as a form of textual loss. From this, the LLM generates a textual gradient âˆ‡~â€‹ğ’¥â€”feedback that suggests improvements, identifies failure cases, or proposes structural edits. For example, the feedback may suggest â€œwe could add a self-reflection moduleâ€ to improve performance, providing actionable guidance for workflow revision. The update operator ğ’°1 then applies such feedback to revise the workflow ğ’² in code space, enabling symbolic updates in a non-differentiable setting. This feedback-driven, interpretable optimization generalizes the notion of learning beyond standard gradient descent (Figure 1).3.3 Model-Agnostic Meta-LearningModel-Agnostic Meta-Learning (MAML) Finn et al. (2017) learns a model initialization that enables rapid adaptation to new tasks using only a few gradient steps. The core idea is to train the model not just to perform well on a set of tasks, but to be easily fine-tuned for any new task drawn from the same distribution.Given a task distribution ğ’¯, each task ğ’¯iâˆ¼ğ’¯ is associated with a loss â„’ğ’¯iâ€‹(Î¸). MAML performs a bi-level optimization:	Î¸iâ€²â†Î¸âˆ’Î±â€‹âˆ‡Î¸â„’ğ’¯iâ€‹(Î¸), 		(6)	Î¸â†Î¸âˆ’Î²â€‹âˆ‡Î¸â€‹âˆ‘ğ’¯iâˆ¼ğ’¯â„’ğ’¯iâ€‹(Î¸iâ€²). 		(7)In the inner loop, the model performs gradient descent on a given task to obtain adapted parameters Î¸iâ€². In the outer loop, the original initialization Î¸ is updated using the post-adaptation losses across multiple tasks. This procedure leverages second-order gradients and enables generalization to unseen tasks with minimal fine-tuning.4 MethodologyInput: train tasks ğ’¯train, test tasks ğ’¯test, inner iterations ninner, outer iterations nouter1 Cluster ğ’¯train into m subtasks {ğ’¯1,â€¦,ğ’¯m};2 Initialize global workflow ğ’² = ğ’²1 = â€¦ = ğ’²m;3 // Outer loop4 for iâ†1 to nouter do5â€‚ â€ƒ foreach ğ’¯tâˆˆ{ğ’¯1,â€¦,ğ’¯m} do6â€‚ â€ƒâ€‚ â€ƒ Initialize ğ’²tâ€²â†ğ’²; jâ†0;7â€‚ â€ƒâ€‚ â€ƒ // Inner loop8â€‚ â€ƒâ€‚ â€ƒ while ğ’¥â€‹(ğ’²tâ€²,ğ’¯t)<ğ’¥â€‹(ğ’²t,ğ’¯t)âˆ’Ïµ and j<ninner do9â€‚ â€ƒâ€‚ â€ƒâ€‚ â€ƒ Execute ğ’²tâ€² on ğ’¯t, obtain âˆ‡~â€‹ğ’¥;10â€‚ â€ƒâ€‚ â€ƒâ€‚ â€ƒ ğ’²tâ€²â†ğ’°1â€‹(ğ’²tâ€²,âˆ‡~â€‹ğ’¥);11â€‚ â€ƒâ€‚ â€ƒâ€‚ â€ƒ jâ†j+1;12â€‚ â€ƒâ€‚ â€ƒ end while13â€‚ â€ƒâ€‚ â€ƒğ’²tâ†ğ’²tâ€²;14â€‚ â€ƒ end foreach15â€‚ â€ƒğ’²â†ğ’°2â€‹(ğ’²,Gâ€‹({âˆ‡~â€‹ğ’¥â€‹(ğ’²t,ğ’¯t)}t=1m))16 end for17Cluster ğ’¯test into n subtasks {ğ’¯1â€²,â€¦,ğ’¯nâ€²};18 foreach ğ’¯tâ€² do19â€‚ â€ƒ ğ’²â€²â†ğ’°3â€‹(ğ’²,ğ’¯tâ€²);20â€‚ â€ƒ Evaluate ğ’²âˆ— on ğ’¯tâ€²;21 end foreachAlgorithm 1 AdaptFlow Algorithm4.1 OverviewWe present AdaptFlow, a meta-optimization framework that integrates ideas from MAML Finn et al. (2017) into the setting of agentic workflow optimization. As illustrated in Figure 2, our method first partitions the training tasks into multiple semantically coherent subtasks. It then performs a bi-level optimization process to learn a workflow initialization that generalizes across these subtasks: the inner loop (lines 5â€“12 in Algorithm 1) adapts the workflow using LLM-generated feedback for each subtask, while the outer loop (lines 3â€“14 in Algorithm 1) aggregates these refinements into a shared initialization. At test time, we apply lightweight adaptation on unseen subtasks based on their semantic descriptions (lines 16â€“19 in Algorithm 1). By explicitly optimizing workflows at the subtask level, AdaptFlow enables structural adaptation to diverse problem types, addressing challenge (C1). Furthermore, the hierarchical innerâ€“outer update scheme ensures stable convergence in the discrete code space, effectively resolving challenge (C2). The full algorithm is provided in Algorithm 1, and its procedural flow is visualized in Figure 2.Refer to captionFigure 2: Illustration of the AdaptFlow framework, consisting of three stages. (1) Task Clustering: training tasks are grouped into semantically coherent subtasks. (2) Bi-Level Workflow Optimization: a bi-level optimization process is appliedâ€”inner loop explores workflow variants using LLM-generated feedback; outer loop aggregates updates into a generalizable initialization. (3) Test-Time Adaptation: the learned workflow is adapted to unseen tasks based on subtask-level descriptions generated from input questions. The detailed mechanism of inner and outer updates is shown in Figure 1.4.2 Task ClusteringMany tasks exhibit high internal diversity, making it difficult to optimize a single workflow across all instances. To address this, we first partition the training set ğ’¯train into m semantically coherent subtasks ğ’¯1,â€¦,ğ’¯m using K-Means MacQueen (1967) clustering over instruction embeddings. The embeddings are obtained from the all-MiniLM-L6-v2 model Reimers and Gurevych (2019). This decomposition enables subtask-specific workflow optimization and promotes more stable and effective learning.4.3 Bi-Level Workflow OptimizationInner Loop (Exploration)For each subtask ğ’¯t, the workflow ğ’²tâ€² is iteratively refined using LLM-generated textual feedback. At each step, we evaluate the current utility ğ’¥â€‹(ğ’²tâ€²,ğ’¯t) and apply the symbolic update:	ğ’²tâ€²â†ğ’°1â€‹(ğ’²tâ€²,âˆ‡~â€‹ğ’¥â€‹(ğ’²tâ€²,ğ’¯t)).		(8)To ensure stable and meaningful exploration, we define a binary continuation signal Î´tâˆˆ0,1 as:	Î´t=ğ•€â€‹[ğ’¥â€‹(ğ’²t,ğ’¯t)âˆ’ğ’¥â€‹(ğ’²tâ€²,ğ’¯t)>Ïµ],		(9)where ğ’²t denotes the best workflow found so far. Here, ğ’¥ evaluates a workflowâ€™s performance on task ğ’¯t via textual assessment, serving as a form of task-level textual loss. Based on this evaluation, the LLM generates a textual gradient âˆ‡~â€‹ğ’¥ that reflects potential improvements or corrections. The update operator ğ’°1 applies this feedback to revise the workflow ğ’²tâ€² in the code space. The inner loop continues only if Î´t=1, indicating that the update yields a non-trivial gain. This continuation signal acts as a local convergence criterion, mitigating instability from long-context accumulation and ensuring effective symbolic refinement. The prompt design for ğ’°1 is detailed in Section A.3.1.Outer Loop (Exploitation)After inner-loop optimization across all subtasks, we aggregate the resulting feedback to update the global workflow. Each âˆ‡~â€‹ğ’¥â€‹(ğ’²t,ğ’¯t) denotes a textual gradientâ€”natural language feedback from the LLM that suggests workflow improvements based on subtask performance. The aggregation function G merges these gradients into a unified signal, which is then applied via the update operator ğ’°2:	ğ’²â†ğ’°2â€‹(ğ’²,Gâ€‹({âˆ‡~â€‹ğ’¥â€‹(ğ’²t,ğ’¯t)}t=1m)).		(10)This meta-level update integrates subtask-specific insights into a generalizable workflow by aggregating the textual gradients from the best-performing workflows of each subtask and applying them to revise the global workflow.To further improve robustness, we apply a reflection step after the update. The updated workflow is re-executed on each subtask to identify remaining failure cases. The agent then generates refinement suggestions, which are used to perform a secondary symbolic update. This reflection-enhanced outer loop helps address blind spots and improve generalization.4.4 Test-Time AdaptationTo evaluate generalization, we apply the learned initialization ğ’² to a set of unseen test tasks ğ’¯â€‹test. Following the same procedure as in training, we partition ğ’¯â€‹test into n subtasks ğ’¯1â€²,â€¦,ğ’¯nâ€² using instruction-level clustering.For each subtask ğ’¯tâ€², we randomly sample a subset ğ’¯~tâ€²âŠ‚ğ’¯tâ€² and prompt a language model to generate a high-level description â„±â€‹(ğ’¯~tâ€²) based solely on the input questions from the sampled tasksâ€”without access to answers or solutions. This representation captures the subtaskâ€™s semantic intent and guides adaptation.We then apply the update operator ğ’°3 to specialize the global workflow based on this subtask description:	ğ’²â†ğ’°3â€‹(ğ’²,â„±â€‹(ğ’¯~tâ€²)).		(11)Here, ğ’°3 performs a fast adaptation of the workflow by leveraging the semantic intent of the subtask, which is derived from input prompts. It uses natural language cues to specialize the global workflow for the target subtask. The prompt design for ğ’°3 is detailed in Section A.3.4. The resulting adapted workflow ğ’² is then evaluated on the full subtask ğ’¯tâ€², enabling effective generalization to previously unseen task distributions. A concrete example of this process is illustrated in Section 6.5.5 Experiment SetupDatasetsWe evaluate our method on eight public datasets across three domains: question answering, code generation, and mathematical reasoning. For HumanEval Chen et al. (2021) and MBPP Austin et al. (2021), we use the full datasets. Following AFLOW Zhang et al. (2024b), we sample 1,319 examples from the GSM8K test split Cobbe et al. (2021). For MATH Hendrycks et al. (2021), we follow Hong et al. (2024a) and select level-5 problems from four categories: Combinatorics and Probability, Number Theory, Pre-algebra, and Pre-calculus. We also include two advanced math benchmarks: AIME OpenAI (2023) and OlympiadBench Zhu et al. (2024). For DROP Dua et al. (2019) and HotpotQA Yang et al. (2018), we follow prior work Shinn et al. (2023); Zhang et al. (2024b); Wang et al. (2025) and randomly sample 1,000 instances each. All datasets are split into validation and test sets with a 1:4 ratio. See Table 6 for full statistics.BaselinesWe compare our method against two categories of baselines: manually designed workflows and automatically optimized workflows for LLMs. Manual Workflows include widely used prompting strategies and agent-based methods: Vanilla prompting, Chain-of-Thought (CoT) Wei et al. (2022), Reflexion Shinn et al. (2023), LLM Debate Du et al. (2023), Step-back Abstraction Zhou et al. (2022), Quality-Diversity (QD) Wang et al. (2023), and Dynamic Role Assignment Qian et al. (2023). These approaches are constructed using fixed templates or heuristics without task-specific adaptation. Automatically Optimized Workflows are derived through workflow optimization or search. We include ADAS Hu et al. (2024) and AFLOW Zhang et al. (2024b), which learn or search for agentic workflow structures in a data-driven manner to improve LLM performance across tasks.Implementation DetailsWe use a decoupled architecture separating optimization and execution. GPT-4.1 OpenAI (2024a) serves as the optimizer, while executors include DeepSeekV2.5 DeepSeek (2024), GPT-4o-mini OpenAI (2024b), Claude-3.5-Sonnet Anthropic (2024), and GPT-4o OpenAI (2024c). All models are accessed via public APIs with a fixed temperature of 0.5. The outer loop runs for 3 iterations, and the inner loop allows up to 6 updates per subtask.MetricsWe adopt task-specific evaluation metrics tailored to each dataset category. For mathematics benchmarks, including GSM8K, MATH, AIME, and OlympiadBench, we use the Solve Rateâ€”the proportion of correctly solved problemsâ€”as the primary metric. For code generation tasks (HumanEval and MBPP), we report pass@1, following the evaluation protocol of Chen et al. Chen et al. (2021), which measures the correctness of the top-1 generated solution. For question-answering datasets such as HotpotQA and DROP, we adopt the F1 Score to evaluate the overlap between predicted and ground-truth answers.6 Results and Analysis6.1 Main ResultsMethod QA Coding MATH Average HotpotQA DROP HumanEval MBPP GSM8K MATH AIME Olympiad Vanilla 70.7 79.6 87.0 71.8 92.7 48.2 12.4 25.0 60.9 COT 69.0 78.8 90.8 72.5 91.3 49.9 10.1 26.4 61.1 Reflexion 68.3 79.5 86.3 72.4 92.4 49.3 10.5 25.9 60.6 LLM Debate 68.5 79.3 90.8 73.3 93.8 52.7 13.7 29.8 62.7 Step-back Abstraction 67.9 79.4 87.8 71.9 90.0 47.9 4.8 19.3 58.6 Quality Diversity 69.3 79.7 88.5 72.5 92.3 50.5 9.4 28.8 61.4 Dynamic Assignment 67.9 76.8 89.3 71.5 89.2 50.7 12.7 27.6 60.7 ADAS 64.5 76.6 82.4 53.4 90.8 35.4 10.4 21.2 54.3 AFlow 73.5 80.6 94.7 83.4 93.5 56.2 17.4 28.5 65.6 Ours 73.8 82.4 94.7 84.0 94.6 61.5 22.6 34.4 68.5Table 1: Performance comparison across three domains: question answering, code generation, and mathematics. Best results are shown in bold, and second-best results are underlined. In our method, GPT-4.1 is used for workflow refinement, while GPT-4o-mini-0718 is responsible for workflow execution.As shown in Table 1, our method delivers consistently strong performance across three distinct domainsâ€”question answering, code generation, and mathematicsâ€”achieving the highest overall average score of 68.5. This suggests that our unified framework generalizes well to tasks with varying structures and reasoning demands. In particular, the substantial gains on mathematics benchmarks demonstrate the frameworkâ€™s strength in handling complex symbolic and multi-step reasoning.These results highlight the advantage of learning workflows in a task-adaptive and optimization-aware manner. Compared to existing baselines, including both manually designed strategies and automatically optimized methods, our approach achieves more balanced improvements across domains, underscoring its robustness and scalability. The consistent lead over ADAS Hu et al. (2024) and Aflow Li et al. (2024b), which operate in a similar code-based search space, further supports the effectiveness of meta-level adaptation in building generalizable agentic workflows.6.2 Ablation StudyAblation on ReflectionTo evaluate the impact of the reflection module in the outer loop, we conduct an ablation study on the MATH dataset. We use GPT-4.1 for workflow updates and GPT-4o-mini-0718 for workflow execution. In the ablated setting, denoted as w/o reflection, we remove the reflection step where the model samples and revises failed cases after the initial outer-loop update. As shown in Table 2, incorporating reflection consistently leads to better performance across iterations, with a final accuracy of 61.5 compared to 60.2 without reflection. This highlights the importance of targeted self-correction in enhancing workflow robustness and adaptability.Outer Loop Iteration 	1 	2 	3w/o reflection 	56.7 	58.2 	60.2ours 	57.2 	58.6 	61.5Table 2: Performance comparison across iterations on the MATH dataset. w/o reflection denotes the setting without the reflection component, while ours includes it.Ablation on Test-Time AdaptationSubtask 	w/o adaptation 	oursPreA 	73.1 	76.4PreC 	20.8 	21.4C&P 	61.9 	63.1NT 	68.3 	73.9Overall 	58.0 	61.5Table 3: Ablation results on math subtasks with and without test-time adaptation. w/o adaptation disables test-time adaptation. Subtask abbreviations: PreC = Precalculus, PreA = Prealgebra, NT = Number Theory, C&P = Counting & Probability.To assess the effectiveness of our test-time adaptation strategy, we conduct an ablation study on four mathematical reasoning subtasks: Prealgebra, Precalculus, Counting & Probability, and Number Theory. As shown in Table 3, removing the adaptation module results in a consistent drop in performance across all subtasks. Notably, the largest improvement is observed in Number Theory, where accuracy increases from 68.3 to 73.9, suggesting that adaptation plays a crucial role in handling complex symbolic reasoning. The overall average accuracy improves by 3.5 points, confirming that test-time refinement enhances the generalization of the global workflow to previously unseen problems.6.3 Convergence AnalysisWe analyze the convergence behavior of both inner and outer loops on the MATH dataset, as shown in Figure 3. The inner loop exhibits noticeable fluctuations due to the accumulation of long-context dependencies and the large workflow search space, a challenge also observed in ADAS Hu et al. (2024). Despite this, our constrained update mechanism helps maintain reasonable performance at each step. In contrast, the outer loop shows steady improvement, as it only aggregates the best-performing workflows from each subtask, leading to more stable and reliable updates at the meta level. These results demonstrate that our method effectively ensures convergence throughout the optimization process, addressing the core challenge of C2.Refer to captionFigure 3: Convergence behavior of the inner and outer optimization loops on the MATH dataset. The inner loop (solid lines) shows fluctuations in solve rate across iterations for each subtask, with a maximum of 6 iterations per subtask, while the outer loop (dashed line) steadily improves overall performance by aggregating the best workflows per subtask.6.4 Model Agnostic AnalysisModel Method Vanilla COT Reflexion LLM debate Step-back Abstraction Quality Diversity Role Assignment Ours GPT-4o-mini 48.2 49.9 49.3 52.7 47.9 50.5 50.7 61.5 GPT-4o 53.8 53.7 54.2 55.1 53.3 56.6 53.3 63.6 claude-3-5-sonnet 20.4 22.6 22.6 23.8 20.7 21.4 20.1 27.8 DeepSeek-V2.5 52.6 52.0 53.3 54.1 52.8 55.1 53.5 61.1Table 4: Model-agnostic performance comparison across various workflow optimization methods on the MATH dataset. Ours consistently achieves the highest accuracy across all LLM backbones.To assess generality, we evaluate our method on the MATH dataset using four LLMs: GPT-4o-mini, GPT-4o, Claude-3.5-Sonnet, and DeepSeek-V2.5. As shown in Table 4, our method consistently achieves the best performance, demonstrating strong robustness and generalization.While absolute performance varies across LLMs, our method consistently outperforms all baselines. The lower accuracy of Claude-3.5-Sonnet may stem from its weaker handling of structured outputs like JSON, which are central to our answer extraction pipeline. Nonetheless, our approach remains effective across model families without requiring model-specific customization.6.5 Case StudyModule All PreC PreA NT C&P DA âœ“ âœ“ âœ“ âœ“ âœ“ AE âœ“ âœ“ âœ“ âœ“ âœ“ CS âœ“ âœ“ âœ“ âœ“ âœ“ VF âœ“ âœ“ âœ— âœ— âœ— CL âœ“ âœ— âœ— âœ— âœ— SY âœ“ âœ“ âœ“ âœ“ âœ“ VT âœ— âœ— âœ— âœ“ âœ— AD âœ— âœ— âœ“ âœ— âœ—Table 5: Module usage across subtasks on the MATH dataset. Each column represents a workflow configuration: All denotes the final workflow obtained after the third round of outer-loop optimization, while the others reflect the best inner-loop workflows before aggregation. Subtask abbreviations: PreC = Precalculus, PreA = Prealgebra, NT = Number Theory, C&P = Counting & Probability. Module abbreviations: DA = Diverse Agents, AE = Answer Extraction, CS = Consensus, VF = Verifier, CL = Clarifier, SY = Synthesis, VT = Value Tracker, AD = Approximation Detector. âœ“ indicates module is used; âœ— indicates not used.We present a case study on the MATH dataset by comparing workflows before and after the third outer-loop iteration. Specifically, we select the best-performing workflows for each subtask prior to the final aggregation, and denote the post-aggregation unified workflow as All. This case study illustrates how the outer loop consolidates subtask-specific refinements into a generalizable workflow (Table 5). The All column represents the workflow obtained after the third outer-loop update, while the other columns correspond to the best inner-loop workflows before this update.Shared Front-End.All workflows include three core modules: DA (Diverse Agents), AE (Answer Extraction), and CS (Consensus). These ensure solution diversity, consistent answer formats, and stable outputs, forming a robust foundation applicable across domains.Task-Specific Modules.Additional modules are selectively introduced based on subtask characteristics. For example, AD (Approximation Detector) in Prealgebra handles rounding mismatches, while VT (Value Tracker) in Number Theory tracks intermediate values in multi-step reasoning.This modular design supports both generalization and specialization, enabling high performance across diverse mathematical tasks.7 ConclusionWe introduced AdaptFlow, a bi-level meta-optimization framework that learns adaptable agentic workflows via LLM-guided symbolic feedback. Across eight benchmarks, AdaptFlow outperforms both manual and automated baselines, with components like reflection and test-time adaptation enhancing robustness. Overall, it offers a scalable, model-agnostic solution for automating workflow design.LimitationsWhile AdaptFlow achieves strong generalization, it has two primary limitations. First, the quality of symbolic updates depends on LLM-generated textual feedback, which can be vague or insufficiently detailed for complex failure cases. More structured or fine-grained feedback could improve update precision. Second, the optimization process requires repeated LLM queries, leading to non-trivial computational costs. Reducing query overhead through more efficient adaptation strategies is an important direction for future work.References    Achiam et al. (2023)Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.Anthropic (2024)Anthropic. 2024. Claude 3.5 sonnet. Available at https://www.anthropic.com/index/claude-3-5.Austin et al. (2021)Jacob Austin, Augustus Odena, Maxwell Nye, and 1 others. 2021. Program synthesis with large language models. In NeurIPS.Bai et al. (2022)Yuntao Bai, Saurav Kadavath, Sandipan Kundu, and et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.Chen et al. (2023)Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, BÃ¶rje F Karlsson, Jie Fu, and Yemin Shi. 2023. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288.Chen et al. (2021)Mark Chen, Jerry Tworek, Heewoo Jun, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.Cobbe et al. (2021)Karl Cobbe and 1 others. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2104.03235.Creswell and Shanahan (2023)Antonia Creswell and Murray Shanahan. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2305.05642.DeepSeek (2024)DeepSeek. 2024. Deepseek-v2.5. Available at https://deepseek.com.Ding et al. (2024)Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng Zhao, and Yasha Wang. 2024. 3ds: Decomposed difficulty data selectionâ€™s case study on llm medical domain adaptation. arXiv preprint arXiv:2410.10901.Du et al. (2023)Yixin Du and 1 others. 2023. The devil is in the debate: On the utility of argumentative dialogue agents for reasoning. arXiv preprint arXiv:2305.14325.Dua et al. (2019)Dheeru Dua and 1 others. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.Fernando et al. (2023)Bas Fernando, Azalia Mirhoseini, Andrew Dai, and Quoc Le. 2023. Promptbreeder: Towards the automatic evolution of prompts. arXiv preprint arXiv:2309.00680.Finn et al. (2017)Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126â€“1135. PMLR.Guo et al. (2025)Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.Guo et al. (2023)Jiahao Guo, Zifan Wang, Sheng Zha, Xiaodong Wang, Xin Jin, and Dacheng Tao. 2023. Evoprompt: Language model guided genetic prompt optimization. arXiv preprint arXiv:2309.07932.Hendrycks et al. (2021)Dan Hendrycks and 1 others. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.Hong et al. (2024a)Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, and 1 others. 2024a. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679.Hong et al. (2024b)Yujia Hong, Junyang Wu, Fan Zhang, and Shuo Zhang. 2024b. Adaptive agents with code and memory for solving math word problems. arXiv preprint arXiv:2403.01290.Hong et al. (2024c)Yujia Hong, Junyang Wu, Fan Zhang, and Shuo Zhang. 2024c. Sweagent: Code generation via structured workflow execution with llms. arXiv preprint arXiv:2403.01290.Hu et al. (2024)Shengran Hu, Cong Lu, and Jeff Clune. 2024. Automated design of agentic systems. arXiv preprint arXiv:2408.08435.Jiang et al. (2025)Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan Wang, Haote Yang, and 1 others. 2025. Evaluating large language model with knowledge oriented language specific simple question answering. arXiv preprint arXiv:2505.16591.Khattab et al. (2023)Omar Khattab, Bhanukiran Vinzamuri Akula, and 1 others. 2023. Dspy: Expressive, modular prompting for language models. arXiv preprint arXiv:2310.01348.Li et al. (2024a)Tao Li, Jiacheng Liu, Yichi Zhang, and 1 others. 2024a. Autoda: Towards data analysis automation with large language models. arXiv preprint arXiv:2403.18270.Li et al. (2024b)Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. 2024b. Autoflow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821.Liu et al. (2023)Zhen Liu, Wentao Wu, Yuke Zhu, and Zhiwei Steven Ling. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2303.13455.Liu et al. (2024)Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. 2024. A dynamic llm-powered agent network for task-oriented agent collaboration. In First Conference on Language Modeling.MacQueen (1967)J. MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics, pages 281â€“297. University of California Press.Mei et al. (2024)Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. 2024. Slang: New concept comprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, page 12558â€“12575. Association for Computational Linguistics.Mei et al. (2025)Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi Cheng. 2025. a1: Steep test-time scaling law via environment augmented generation. Preprint, arXiv:2504.14597.Nijkamp et al. (2023)Erik Nijkamp, Ziyang Tu, Zexue Lin, and et al. 2023. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309.Nori et al. (2023)Harsha Nori, Michael R. King, Scott M. McKinney, and 1 others. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375.OpenAI (2023)OpenAI. 2023. Aime benchmark for mathematical reasoning. https://openai.com/research. Accessed 2024.OpenAI (2024a)OpenAI. 2024a. Gpt-4.1 overview. Available at https://openai.com/index/gpt-4-1/.OpenAI (2024b)OpenAI. 2024b. Gpt-4o-mini-0718. Available via OpenAI API.OpenAI (2024c)OpenAI. 2024c. Introducing gpt-4o. Available at https://openai.com/index/hello-gpt-4o/.Qian et al. (2023)Yujia Qian and 1 others. 2023. Role-play prompting for multi-agent collaboration with llms. arXiv preprint arXiv:2305.14325.Qin et al. (2023)Chenyan Qin, Kang Liu, Yaqing Zhang, and 1 others. 2023. Toolllm: Facilitating large language models to master 160+ tools. arXiv preprint arXiv:2307.16789.Rajpurkar et al. (2016)Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. EMNLP.Reimers and Gurevych (2019)Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 3982â€“3992. Association for Computational Linguistics.Ridnik et al. (2024)Tomer Ridnik, Yuval Shalev, Achiya Noy, and 1 others. 2024. Coding agents: Interactive code generation via llm planning and execution. arXiv preprint arXiv:2402.03345.Saad-Falcon et al. (2024)Javier Saad-Falcon, Ren Liu, Anthony Chan, and Allen Lin. 2024. Hyperparameter optimization in agentic llm pipelines. arXiv preprint arXiv:2401.04903.Schick et al. (2023)Timo Schick, Ananya Dwivedi-Yu, Hinrich SchÃ¼tze, and Peter Prettenhofer. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.Shinn et al. (2023)Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634â€“8652.Song et al. (2024)Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, and Chi Wang. 2024. Adaptive in-conversation team building for language model agents. arXiv preprint arXiv:2405.19425.Wang et al. (2023)Yichi Wang and 1 others. 2023. Large language models as optimizers for quality-diversity search. arXiv preprint arXiv:2303.05832.Wang et al. (2025)Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, and Bryon Aragam. 2025. Scoreflow: Mastering llm agent workflows via score-based preference optimization. arXiv preprint arXiv:2502.04306.Wei et al. (2022)Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824â€“24837.Xie et al. (2024)Jinyuan Xie, Yang Yang, Ziyang Zhang, and 1 others. 2024. Autonova: Generating llm pipelines for data science via prompt evolution. arXiv preprint arXiv:2402.04002.Xu et al. (2024)Yicheng Xu, Wei Zhang, Tao Li, and Shuo Zhang. 2024. Towards generalizable agents for mathematical reasoning. arXiv preprint arXiv:2402.09399.Yang et al. (2018)Zhilin Yang and 1 others. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.Yao et al. (2023)Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:11809â€“11822.Ye et al. (2024)Qinyuan Ye, Xiao Liu, Yichong Zhou, and Shuo Zhang. 2024. Llm-dp: Towards autonomous data processing agents. arXiv preprint arXiv:2403.05923.Zhang et al. (2024a)Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. 2024a. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782.Zhang et al. (2024b)Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, and 1 others. 2024b. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762.Zhang et al. (2020)Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. 2020. Dialogpt: Large-scale generative pre-training for conversational response generation. ACL.Zhao et al. (2024)Yilun Zhao, Yuan Yang, Zecheng Hu, and 1 others. 2024. Agentcoder: Integrating planning and execution for code generation agents. arXiv preprint arXiv:2403.14260.Zhong et al. (2024)Licheng Zhong, Yiding Li, Yichong Zhou, and Shuo Zhang. 2024. Mathagent: Math reasoning with retrieval-augmented code agents. arXiv preprint arXiv:2402.03620.Zhou et al. (2022)Xuezhi Zhou and 1 others. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.Zhou et al. (2024a)Yichong Zhou, Bowen Zhang, Yujia Hong, and Shuo Zhang. 2024a. Reasoning agents: Llm-as-policy for compositional task solving. arXiv preprint arXiv:2404.01865.Zhou et al. (2024b)Yifan Zhou, Yecheng Li, Runzhe Xu, and 1 others. 2024b. Llm+p: Empowering large language models with planning for complex tasks. arXiv preprint arXiv:2403.03031.Zhu et al. (2025a)Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, and Conghui He. 2025a. Grait: Gradient-driven refusal-aware instruction tuning for effective hallucination mitigation. arXiv preprint arXiv:2502.05911.Zhu et al. (2025b)Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, and Conghui He. 2025b. Utilize the flow before stepping into the same river twice: Certainty represented knowledge flow for refusal-aware instruction tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 26157â€“26165.Zhu et al. (2024)Zeyu Zhu and 1 others. 2024. Olympiadbench: A benchmark for mathematical reasoning at the olympiad level. arXiv preprint arXiv:2402.00000.Zhuge et al. (2023)    Yujia Zhuge, Qinyuan Ye, Xiao Liu, Shujie Wang, and Furu Wei. 2023. Gptswarm: Multi-agent collaboration via llm-based swarm intelligence. arXiv preprint arXiv:2311.16688.Appendix A AppendixA.1 Dataset DetailsOur experiments span eight public benchmarks across three major domains: question answering, code generation, and mathematical reasoning. Table 6 summarizes the dataset statistics, including the number of validation/test instances and the number of subtasks for each dataset. Each subtask represents a semantically or structurally coherent group of problems, enabling more focused workflow specialization during meta-optimization.For question answering, we use subsets of HotpotQA and DROP, each containing 1,000 examples in total, with a 1:4 split for validation and testing. The examples are clustered into six subtasks based on instruction similarity. Similarly, in the coding domain, HumanEval and MBPP are divided into three and four subtasks, respectively, reflecting different code generation patterns.In the mathematics domain, the datasets exhibit more diverse task structures. For GSM8K and AIME, we apply instruction-level clustering to derive six distinct subtasks per dataset, capturing variations in reasoning complexity and problem format.Notably, two datasetsâ€”MATH and OlympiadBenchâ€”come with predefined topic categories, and thus do not undergo clustering. The MATH dataset contains high school-level math problems and is partitioned into four canonical categories: Prealgebra, Precalculus, Number Theory, and Counting & Probability, following the protocol introduced by Hendrycks et al. (2021). These categories capture distinct types of mathematical reasoning, from basic arithmetic to combinatorial logic.Likewise, OlympiadBench is sourced from competitive mathematics exams and is naturally divided into four topics: Algebra, Combinatorics, Geometry, and Number Theory, as defined in the original benchmark by Zhu et al. (2024). These topics correspond to challenging mathematical reasoning tasks requiring manipulation, multi-step derivation, and rigorous abstraction.Overall, our dataset setup provides a rich and heterogeneous landscape for evaluating workflow generalization, supporting both cluster-derived and taxonomy-preserving subtask definitions across domains.	QA 	Coding 	MATHHotpotQA 	DROP 	HumanEval 	MBPP 	GSM8K 	MATH 	AIME 	OlympiadBenchValidation Size 	200 	200 	33 	86 	264 	119 	91 	51Val. Subtasks 	6 	6 	3 	4 	6 	4 	6 	4Test Size 	800 	800 	131 	341 	1055 	486 	373 	212Test Subtasks 	6 	6 	3 	4 	6 	4 	6 	4Table 6: Dataset statistics for each domain and subtask. Validation/test sizes represent the number of instances used for evaluation, and subtask numbers denote the total distinct subtasks grouped under each benchmark.A.2 Analogy ExplanationFigure 2 visualizes the analogy between neural network optimization and workflow optimization, which forms the conceptual foundation for our method. Here, we detail the core correspondences both at the structure level (parameters, updates, gradients) and at the algorithmic level (meta-learning procedure).Structure-Level Analogy.In traditional supervised learning, model training involves continuous optimization of parameters Î¸ using gradients âˆ‡Î¸L derived from a differentiable loss. In contrast, our workflow optimization operates in a discrete, space, where the workflow ğ’² is updated through textual feedback generated by LLMs. The following table presents the one-to-one mapping:Neural Network Optimization 	Workflow Optimization (AdaptFlow)Model parameters Î¸ 	Workflow structure ğ’²Loss function Lâ€‹(fÎ¸â€‹(x),y) 	Utility function Jâ€‹(ğ’²,T)Gradient âˆ‡Î¸L 	Textual gradient âˆ‡~â€‹J (LLM feedback)Gradient descent update Î¸â†Î¸âˆ’Î·â€‹âˆ‡Î¸L 	Symbolic update ğ’²â€²â†ğ’°1â€‹(ğ’²,âˆ‡~â€‹J)Batch of examples {(xi,yi)} 	Batch of tasks or subtask data ğ’¯tTable 7: Structure-level analogy between differentiable model optimization and discrete workflow optimization.Meta-Learning Analogy: MAML vs. AdaptFlow.At the algorithmic level, AdaptFlow is inspired by Model-Agnostic Meta-Learning (MAML), but adapted to the setting. While MAML learns a parameter initialization Î¸ that can rapidly adapt via gradient updates, AdaptFlow learns a generalizable workflow ğ’² that adapts via LLM-generated updates. The table below compares the two approaches step-by-step:MAML (Finn et al., 2017) 	AdaptFlow (Ours)Model initialization Î¸ 	Workflow initialization ğ’²Task-specific adaptation via Î¸â€²â†Î¸âˆ’Î±â€‹âˆ‡Î¸LT 	Subtask-specific refinement via ğ’²â€²â†ğ’°1â€‹(ğ’²,âˆ‡~â€‹J)Compute outer gradient from Î¸â€² 	Aggregate textual feedback from refined workflows {âˆ‡~â€‹Jt}Outer update: Î¸â†Î¸âˆ’Î²â€‹âˆ‡Î¸â€‹âˆ‘LTiâ€‹(Î¸iâ€²) 	Meta update: ğ’²â†ğ’°2â€‹(ğ’²,Gâ€‹({âˆ‡~â€‹Jt}))Adaptation via differentiable gradient 	Adaptation via textual feedbackFew-shot generalization to new tasks 	Test-time adaptation via ğ’²âˆ—â†ğ’°3â€‹(ğ’²,â„±â€‹(Ttâ€²))Table 8: Algorithm-level comparison between MAML and AdaptFlow.Together, these analogies highlight how AdaptFlow generalizes the principles of meta-learning to the domain of agentic workflow optimization in spaces.A.3 Prompt TemplatesA.3.1 Inner Loop Workflow Optimization Prompt\UseRawInputEncoding# OverviewYou are an expert machine learning researcher testing various agentic systems. Your objective is to design building blocks such as prompts and control flows within these systems to solve complex tasks. Your aim is to design an optimal agent performing well on the MATH dataset, which evaluates mathematical problem-solving abilities across various mathematical domains including algebra, counting and probability, geometry, intermediate algebra, number theory, prealgebra and precalculus.## An example question from MATH:**instruction (Not Given)**: Solve the following problem and provide a detailed solution. Present the final answer using the \boxed{} format.**question**: question**solution (Not Given)**: solution# Discovered architecture archiveHere is the archive of the discovered architectures:[ARCHIVE]The fitness value is defined as the accuracy on a validation question set. Your goal is to maximize this fitness. You should use your own judgment to decide whether to optimize on the latest architecture, as its performance may not necessarily be better.# Output Instruction and Example:The first key should be ("thought"), and it should capture your thought process for designing the next function. In the "thought" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps.The second key ("name") corresponds to the name of your next agent architecture.Finally, the last key ("code") corresponds to the exact â€œforward()â€ function in Python code that you would like to try. You must write a COMPLETE CODE in "code": Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets.Here is an example of the output format for the next agent architecture:[EXAMPLE]You must use the exact function interface used above. You need to specify the instruction, input information, and the required output fields for various LLM agents to do their specific part of the architecture. Also, it could be helpful to set the LLMâ€™s role and temperature to further control the LLMâ€™s response. Note that the LLMAgentBase() will automatically parse the output and return a list of â€œInfosâ€. You can get the content by Infos.content. DO NOT FORGET the taskInfo input to LLM if you think it is needed, otherwise LLM will not know about the task.# Your taskYou are deeply familiar with LLM prompting techniques and LLM agent works from the literature. Your goal is to maximize "fitness" by proposing interestingly new agents.Observe the discovered architectures carefully and think about what insights, lessons, or stepping stones can be learned from them.Please focus on the architecture with the optimal fitness, and based on that, propose what you believe is the most likely next agent architecture. Note that each optimization step can involve adding one or two new modules to the current best solution, or proposing an entirely novel solution. However, itâ€™s important to ensure that each change remains relatively simple and not overly complex.A.3.2 Outer Loop Workflow Optimization Prompt# OverviewYou are an expert machine learning researcher testing various agentic systems. Your objective is to design building blocks such as prompts and control flows within these systems to solve complex tasks. Your aim is to design an optimal agent performing well on the MATH dataset, which evaluates mathematical problem-solving abilities across various mathematical domains including algebra, counting and probability, geometry, intermediate algebra, number theory, prealgebra and precalculus.## An example question from MATH:**instruction (Not Given)**: Solve the following problem and provide a detailed solution. Present the final answer using the \\boxed{} format.**question**: question**solution (Not Given)**: solutionNote: We divide the overall MATH task into seven distinct subtasks. Below is the performance of the Discovered Architecture Archive on each of these seven subtasks.Discovered Architecture ArchiveThe following presents the archive of the discovered architectures on seven subtasks as well as the full MATH task:[ARCHIVE_LIST]The fitness value is defined as the accuracy on a validation question set. Your goal is to identify an architecture that either maximizes fitness across the seven subtasks or can quickly evolve toward that goal. Note that you should not limit yourself to only the most recently generated architecturesâ€”your objective is to maximize this fitness.# Output Instruction and Example:The first key should be ("thought"), and it should capture your thought process for designing the next function. In the "thought" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps.The second key ("name") corresponds to the name of your next agent architecture.Finally, the last key ("code") corresponds to the exact â€œforward()â€ function in Python code that you would like to try. You must write a COMPLETE CODE in "code": Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets.Here is an example of the output format for the next agent architecture:[EXAMPLE]You must use the exact function interface used above. You need to specify the instruction, input information, and the required output fields for various LLM agents to do their specific part of the architecture.Also, it could be helpful to set the LLMâ€™s role and temperature to further control the LLMâ€™s response. Note that the LLMAgentBase() will automatically parse the output and return a list of â€œInfosâ€. You can get the content by Infos.content.DO NOT FORGET the taskInfo input to LLM if you think it is needed, otherwise LLM will not know about the task.## WRONG Implementation examples:Here are some mistakes you may make:1. This is WRONG: â€˜â€˜â€˜feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)feedback_info = verifier_agent([taskInfo, Info(â€™feedbackâ€™, â€™Critic Agentâ€™, thinking, 0)], verification_instruction)â€˜â€˜â€˜It is wrong to use "Info(â€™feedbackâ€™, â€™Critic Agentâ€™, thinking, 0)". The returned "feedback" from LLMAgentBase is already Info.# Your taskYou are well-versed in LLM prompting techniques and agent-based frameworks from the literature. You are tasked with designing a new agent architecture based on the best-performing solutions from each subtask of the MATH benchmark. The goal is for this new architecture to satisfy at least one of the following criteria:It effectively integrates key modules and features from the optimal solutions of individual subtasks, resulting in a generalizable and adaptable architecture that performs well across all subtasks;Alternatively, the architecture should exhibit strong adaptability and rapid update capabilities, allowing it to quickly evolve and converge toward the optimal solution for each specific subtask.However, you should ensure that the newly generated frameworks is not significantly more complex than the original one, and you may also remove some redundant LLM invocation code.A.3.3 Reflection PromptWe noticed that the current agent is prone to making mistakes when handling the following cases:[CASE_LIST]Please analyze the reasons for these mistakes and propose improvements.Your response should be organized as follows:"reflection": Provide your thoughts on the mistakes in the implementation, and suggest improvements."thought": Revise your previous proposal or propose a new architecture if necessary, using the same format as the example response."name": Provide a name for the revised or new architecture. (Donâ€™t put words like "new" or "improved" in the name.)"code": Provide the corrected code or an improved implementation. Make sure you actually implement your fix and improvement in this code.A.3.4 Test-Time Adaptation Workflow Optimization Prompt# OverviewYou are an expert machine learning researcher testing various agentic systems. Your objective is to design building blocks such as prompts and control flows within these systems to solve complex tasks. Your goal is to design an optimal agent that performs well on the MATH dataset. You may analyze the characteristics of these problems and then design an agent capable of effectively solving them.[TASK_DSC]Note: Your goal is to design an improved agent based on the previous agent, tailored to the characteristics of the current task. We aim to rapidly enhance the performance of the current agent.# Output Instruction and Example:The first key should be ("thought"), and it should capture your thought process for designing the next function. In the "thought" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps.The second key ("name") corresponds to the name of your next agent architecture.Finally, the last key ("code") corresponds to the exact â€œforward()â€ function in Python code that you would like to try. You must write a COMPLETE CODE in "code": Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets.Here is an example of the output format for the next agent architecture:[EXAMPLE]You must use the exact function interface used above. You need to specify the instruction, input information, and the required output fields for various LLM agents to do their specific part of the architecture.Also, it could be helpful to set the LLMâ€™s role and temperature to further control the LLMâ€™s response. Note that the LLMAgentBase() will automatically parse the output and return a list of â€œInfosâ€. You can get the content by Infos.content.DO NOT FORGET the taskInfo input to LLM if you think it is needed, otherwise LLM will not know about the task.# Your taskYou are well-versed in LLM prompting techniques and agent-based frameworks from the literature. You are tasked with designing a new agent architecture based on the previous agent to solve the current task.A.4 Workflow CaseTo provide a concrete illustration of our systemâ€™s output, we present the workflow code generated in the final outer-loop iteration on the MATH dataset. This example reflects the culmination of iterative refinement across subtasks and highlights the integration of shared and task-specific modules.def forward(self, taskInfo):import refrom collections import Counterdef extract(text):for p in [râ€™\\boxed{([^}]*)}â€™, râ€™\(([^)]+)\)â€™, râ€™\\frac{[^}]*}{[^}]*}â€™, râ€™(\d+)\s*$â€™]:m = re.search(p, text)if m: return m.group(0).strip()roles = [â€™Math Professorâ€™, â€™Grade School Teacherâ€™, â€™Math Enthusiastâ€™, â€™Math Olympiad Studentâ€™, â€™Helpful Assistantâ€™]agents = [LLMAgentBase([â€™thinkingâ€™, â€™solutionâ€™], fâ€™A{i}â€™, role=r, temperature=0.7 + 0.1*i) for i, r in enumerate(roles)]sols = [a([taskInfo], "Please think step by step and solve.", i) for i, a in enumerate(agents)]ext_agent = LLMAgentBase([â€™extracted_answerâ€™], â€™Extractorâ€™, role=â€™Answer Extractorâ€™, temperature=0.1)answers, amap = [], {}for i, (t, s) in enumerate(sols):ans = ext_agent([taskInfo, s], "Extract ONLY final boxed answer.", i)[0].content.strip() or extract(s.content)if ans: answers.append(ans); amap.setdefault(ans, (t, s))top = Counter(answers).most_common()if top:top_answers = [a for a, c in top if c == top[0][1]]if len(top_answers) == 1:_, sol = amap[top_answers[0]]else:inputs = [taskInfo] + sum((list(amap[a]) for a in top_answers), []) + [Info(â€™extracted_answerâ€™, â€™â€™, a, -1) for a in top_answers]sol = LLMAgentBase([â€™thinkingâ€™, â€™solutionâ€™], â€™Final Deciderâ€™, temperature=0.1)(inputs, "Choose best answer.")[1]else:inputs = [taskInfo] + sum(([t, s] for t, s in sols), [])sol = LLMAgentBase([â€™thinkingâ€™, â€™solutionâ€™], â€™Fallback Deciderâ€™, temperature=0.1)(inputs, "Choose among all.")[1]verifier = LLMAgentBase([â€™feedbackâ€™, â€™correctâ€™], â€™Verifierâ€™, role=â€™Checkerâ€™, temperature=0.1)clarifier = LLMAgentBase([â€™clarificationâ€™], â€™Clarifierâ€™, role=â€™Solverâ€™, temperature=0.4)synthesizer = LLMAgentBase([â€™thinkingâ€™, â€™solutionâ€™], â€™Synthâ€™, temperature=0.3)for i in range(2):ext = ext_agent([taskInfo, sol], "Extract ONLY final boxed answer.", 100+i)[0]fb, ok = verifier([taskInfo, sol, ext], "Check correctness.", i)if ok.content == â€™Trueâ€™: return solclar, = clarifier([taskInfo, sol, fb], "Respond to critique.", i)fb2, ok2 = verifier([taskInfo, sol, ext, clar], "Recheck solution.", 100+i)if ok2.content == â€™Trueâ€™: return solsyn_inputs = [taskInfo, sol, fb2, clar] + sum(sols, []) + [Info(â€™extracted_answerâ€™, â€™â€™, a, -1) for a in answers if a]sol = synthesizer(syn_inputs, "Revise or synthesize.")[1]return sol"
},{
  "research_proposal": "The paper proposes TeamMedAgents: a multi-agent teamwork framework for medical decision-making, where LLM-based agents collaborate via structured roles (leadership, mutual monitoring, shared mental models, closed-loop communication, team orientation, mutual trust) to enhance accuracy across both textual and visual medical QA tasks.",
  "benchmark": ["MedQA", "PubMedQA", "MMLU-Pro", "MedMCQA", "DDXPlus", "MedBullets", "PathVQA", "PmcVQA"],
  "performance_metrics": [13.7, 13.5, 5.7, 5.4, 9.6, 13.8, 8.4, 7.4],
  "paper_link": "https://arxiv.org/html/2508.08115v1",
  "full_text": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured TeamworkPranav Pushkar Mishra, Mohammad Arvan, Mohan ZalakeAbstractWe present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models (LLMs). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.â€™s â€Big Fiveâ€ model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop communication, and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the taskâ€™s requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains.IntroductionLarge language models (LLMs) have shown potential for improving medical decision-making. Optimal strategies for leveraging these models in complex clinical scenarios remain an active area of investigation (Kim et al. 2024). Although single-agent guidelines have demonstrated proficiency in medical knowledge tasks, the inherent complexity of clinical reasoning, characterized by uncertainty, multiple causes, and the need for diverse expertise, suggests that collaborative multi-agent frameworks may offer superior performance in complex diagnostic scenarios (Kim et al. 2024).Recent work by (Kim et al. 2024) introduced MDAgents, a pioneering framework that adapts collaboration structures based on medical task complexity. This approach demonstrated substantial improvements over baseline methods, achieving up to 11.8% accuracy gains across diverse medical benchmarks. However, while MDAgents successfully implemented adaptive collaboration structures, it lacked the systematic integration of evidence-based teamwork principles that have proven effective in clinical practice.Organizational psychology research has extensively studied teamwork effectiveness, culminating in Salas, Sims, and Burkeâ€™s â€Big Fiveâ€ model of teamwork (Salas, Sims, and Burke 2005). This framework identifies five core teamwork components: team leadership, mutual performance monitoring, team orientation, backup behavior, and adaptability, along with three coordinating mechanisms: shared mental models, closed-loop communication, and mutual trust. These components have been validated across numerous domains, including healthcare, where effective teamwork directly correlates with patient outcomes and diagnostic accuracy (King, Battles, and Baker 2008).Despite the well-documented role of teamwork in improving clinical outcomes (King, Battles, and Baker 2008), existing AI frameworks for medical decision-making have not systematically incorporated structured teamwork mechanisms. Integrating evidence-based teamwork components into multi-agent medical AI systems offers an opportunity to enhance performance by drawing on decades of research in organizational psychology. Bridging this gap requires translating human teamwork constructs into prompt-based LLM behaviors, implementing modular coordination strategies, and measuring teamwork effectiveness within LLM-mediated interactions.Our Approach: We present TeamMedAgents, a comprehensive framework that addresses these gaps by implementing and evaluating six teamwork components into LLM-based medical decision making. Our modular design enables independent configuration of teamwork mechanisms and systematic evaluation across diverse clinical scenarios. Our paper makes the following key contributions to agentic collaboration research:    1.    Systematic Teamwork Component Implementation: We implement six modular teamwork components: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop communication, and mutual trust. We demonstrate configurable prompt-based implementations of teamwork components that integrate with existing LLM-based medical reasoning processes. Our implementation enables adaptation to other multi-agent domains beyond medicine, providing a foundation for incorporating teamwork components in various collaborative AI applications.    2.    Comprehensive Ablation Studies: We present findings of systematic ablation studies across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Health, PubMedQA, DDXPlus, Medbullets, Path-VQA and PMC-VQA) with 50 questions per configuration across 3 independent runs to identify optimal teamwork configurations for different medical task types. Ablation studies demonstrate that individual components contribute measurably to overall performance, with synergistic effects observed when multiple components are combined. Substantial improvements were observed over baseline measures, achieving superior performance across 7 out of 8 evaluated medical benchmarks, encompassing both text-only and multimodal evaluation scenarios. Dataset-specific optimal teamwork configurations were identified, with leadership, shared mental models, and mutual trust forming the most effective combination across medical reasoning tasks.Refer to captionFigure 1: TeamMedAgents framework architecture integrates six teamwork components within a three-round collaborative decision-making process. The recruiter agent dynamically assembles specialized medical experts with task-specific weights, selectively activates teamwork mechanisms based on question complexity, and coordinates multi-round problem solving through independent assessment, structured discussion, and weighted decision aggregation.Related WorkOur work builds upon three research domains that directly inform our approach to integrating evidence-based teamwork components into medical multi-agent systems.Multi-Agent Collaboration Mechanisms in LLM SystemsRecent advances in multi-agent LLM systems have established foundational frameworks for coordinated collaboration, demonstrating consistent performance improvements over single-agent approaches across complex reasoning tasks. MetaGPT introduces Standardized Operating Procedures (SOPs) enabling role-based collaboration where specialized agents communicate through structured documents, achieving impressive accuracy on HumanEval benchmarks (Hong et al. 2024). TeamMedAgents draws inspiration from this assembly line paradigm, extending the â€Code = SOP(Team)â€ philosophy to medical team workflows where coordinated specialist expertise follows established clinical protocols.CAMEL provides theoretical foundations for autonomous multi-agent cooperation through inception prompting and role-playing frameworks (Li et al. 2023), while ChatDev demonstrates practical implementation through virtual company structures and â€communicative dehallucinationâ€ mechanisms (Qian et al. 2023). Our approach adapts these role-based collaboration concepts to medical contexts where specialist agents maintain distinct clinical expertise while coordinating patient care decisions. Multi-agent collaboration achieves substantial performance gains over single-agent approaches on complex reasoning datasets (Du et al. 2023).Sophisticated coordination mechanisms have emerged as essential components for effective collaboration. The Multi-Agent Collaboration Mechanisms framework characterizes collaboration across five dimensions: actors, collaboration types, organizational structures, coordination strategies, and communication protocols (Chen, Wang, and Zhang 2025). TeamMedAgents extends this framework by systematically integrating organizational psychology principles within these coordination dimensions. AutoGen establishes flexible role assignment through conversable agents (Wu et al. 2023), providing architectural foundations that parallel our modular teamwork component design.Enterprise applications demonstrate 90% end-to-end goal success rates through structured communication protocols (Rajbhandari et al. 2024). However, current systems face limitations in reasoning and joint planning, capabilities essential for medical decision-making where agents must understand patient perspectives and anticipate colleague responses (Agashe, Zhang, and Koppel 2023). Despite these advances, current methods often fall short in dynamic coordination, shared situational awareness, and assisting failing peers, capabilities fundamental to effective teamwork. These gaps underscore the need for frameworks grounded in human teamwork principles, such as leadership to orchestrate tasks, mutual monitoring to catch mistakes, and shared mental models to align agents around common goals. Our paper incorporates these teamwork components in multi-agent collaboration.Multi-Agent Systems in Medical ApplicationsMedical applications require complex diagnostic reasoning and thus can benefit from multi-agent collaboration roleplaying as specialist agents. A recent notable demonstration is by the MDAgents framework, which introduced an adaptive decision-making system mirroring real-world medical processes through dynamic LLM agent collaboration (Kim et al. 2024). MDAgents implements a four-stage process: Question Analysis â†’ Agent Recruitment â†’ Collaborative Analysis â†’ Decision Synthesis, with dynamic resource allocation ranging from single-agent configurations to multi-agent teams of 2-5 specialists.Other recent work has explored various approaches to medical multi-agent systems. KG4Diagnosis presents a hierarchical framework combining LLMs with knowledge graph construction through general practitioner and specialist agents (Zhang, Liu, and Chen 2024). MedSentry analyzes vulnerability mechanisms across different multi-agent topologies (Chen et al. 2025), while MedAide proposes an omni medical multi-agent collaboration framework performing query rewriting and intent recognition (Wang, Li, and Zhou 2024). Multi-agent systems have also been applied to patient monitoring through deep reinforcement learning approaches for coordinated emergency responses (Chen, Kumar, and Patel 2023).TeamMedAgents directly extends MDAgents architecture by systematically integrating evidence-based teamwork components. TeamMedAgents systematically enhances each stage through selective activation of our six evidence-based teamwork components, demonstrating how organizational psychology teamwork components can be algorithmically integrated into existing multi-agent architectures.Teamwork Models in Organizational Psychology and Computational ImplementationThe theoretical foundation for TeamMedAgents is Salas, Sims, and Burkeâ€™s â€Big Fiveâ€ teamwork model, which identifies five core behavioral components: Team Leadership, Mutual Performance Monitoring, Backup Behavior, Adaptability, and Team Orientation, along with three coordinating mechanisms: Shared Mental Models, Closed-Loop Communication, and Mutual Trust (Salas, Sims, and Burke 2005). This framework provides the validated psychological principles that we systematically translate into LLM agent collaboration.Empirical validation of the Big Five model spans diverse domains. Police operations studies confirmed six out of ten proposed direct effects and four out of seven indirect pathways (Verhage, Herrington, and Koedijk 2022). Healthcare applications demonstrated that Big Five Teamwork Awareness Sessions reduced rationing of nursing care and increased patient-centered care scores (Hassan, El-Sayed, and Ahmed 2025).Prior work has explored computational implementations of teamwork principles in multi-agent systems using distributed leadership, real-time performance monitoring, and trust-based information filtering (Stone and Veloso 2000; Foerster et al. 2018). TeamMedAgents advances this trajectory by operationalizing these principles through structured agent prompts, measurable behavioral tracking, and dynamic trust networks that influence information sharing between medical specialists. Unlike prior frameworks that use ad-hoc coordination, TeamMedAgents offers a systematic methodology for integrating validated teamwork components, demonstrating the value of theory-driven agent collaboration in complex decision-making domains.TeamMedAgents: Teamwork Enhanced Medical Decision-Making FrameworkOur framework operates through a four-stage process with dynamic agent recruitment, building on the prior success of dynamic recruitment (Kim et al. 2024):    1.    Multi-Domain Agent Allocation: The recruiter agent analyzes question domain requirements and dynamically allocates n specialized agents with diverse medical expertise.    2.    Adaptive Teamwork Component Selection: The recruiter agent systematically selects and activates specific teamwork mechanisms from the six-component framework based on collaborative coordination requirements, domain interdependencies, and predicted coordination benefits for optimal multi-agent performance.    3.    Multi-Round Collaborative Reasoning: Agents engage in structured three-round collaboration with directed communication. A three-round collaboration enables a balance between depth of reasoning and response latency.    4.    Weighted Decision Aggregation: Final decisions emerge through weighted voting mechanisms that reflect agent hierarchy and expertise relevance.The core innovation lies in the systematic integration of six modular teamwork mechanisms that enhance coordination effectiveness without disrupting the underlying medical reasoning process. Each component functions as an independent behavioral enhancement that can be selectively activated based on empirical optimization results and task requirements. Figure 1 illustrates the complete TeamMedAgents architecture with integrated teamwork components.Modular Teamwork Component ImplementationOur framework integrates six teamwork components that augment agent behavior through specialized coordination mechanisms. Each component operates as an independent enhancement layer, enabling systematic activation based on empirical optimization results.Team LeadershipThe team leadership component designates a leader agent responsible for coordination and synthesis (Zaccaro, Rittman, and Marks 2001). Implementation operates through two mechanisms: coordination phase leadership between Rounds 1-2 using domain-specific structured prompts that implement systematic problem decomposition templates, and synthesis phase integration following Round 3 with enhanced weighting (1.5Ã— multiplier) in decision aggregation. Structured prompts guide leaders through standardized medical reasoning frameworks, including differential diagnosis prioritization and evidence evaluation hierarchies.Mutual Performance MonitoringThis component implements systematic peer review through automated issue detection during Round 2 discussions (Marks, Mathieu, and Zaccaro 2001a). Agents analyze peer responses for diagnostic completeness, logical consistency, and medical errors using domain-specific evaluation criteria. Detected issues generate constructive feedback categorized by clinical severity (critical, moderate, minor) with resolution tracking across subsequent rounds to measure monitoring effectiveness through quantitative issue resolution rates.Team OrientationTeam orientation prioritizes collective diagnostic accuracy over individual position advocacy (Eby and Dobbins 1997). Implementation emphasizes solution-focused collaboration where agents orient toward optimal patient outcomes rather than defensive argumentation. Goal alignment mechanisms utilize specialized prompt engineering, emphasizing constructive contribution building and transparent information sharing, measured through linguistic analysis quantifying solution-focused versus competitive communication patterns.Shared Mental ModelsShared mental models ensure consistent understanding across the workflow (Stout et al. 1999). Implementation constructs formalized task models representing medical case objectives and evaluation criteria, plus team models defining expertise areas and interaction patterns. These frameworks enable coordinated reasoning and predictive capability for anticipating teammate contributions during directed communications.Closed-Loop CommunicationThis mechanism implements structured three-step communication protocols during Round 2 discussions (Cannon-Bowers et al. 1995): targeted information transmission, explicit acknowledgment and understanding confirmation, and verification with misunderstanding resolution. This protocol reduces transmission errors for complex medical concepts requiring precise communication between specialists.Mutual TrustMutual trust creates dynamic trust networks influencing information sharing depth (Webber 2002). Implementation initializes trust levels at 0.8 between agent pairs, updating based on observed behaviors including mistake admission, feedback acceptance, and information sharing quality. Trust-influenced communication modulates sharing depth during Round 2, with higher trust enabling more comprehensive knowledge exchange and greater feedback receptivity.While Salas et al.â€™s original â€Big Fiveâ€ teamwork framework includes additional components such as backup behavior and adaptability, these were not implemented in the current iteration of TeamMedAgents due to their limited computational analogs in static question-answering settings. Backup behavior, which involves dynamically stepping in to assist teammates under workload or error conditions, does not apply to our context, as each agent has a constant workload. Similarly, adaptability, which entails team-level adjustments to evolving environments, does not apply to our context as the environment and contextual settings remain constant.Technical ImplementationSystem Architecture: The framework employs a modular Python 3.12.6 architecture with isolated task configurations to prevent cross-contamination during parallel question processing. Each teamwork component is implemented as an independent class with standardized interfaces, enabling flexible composition and systematic evaluation. Experiments are executed across multiple Azure OpenAI foundry deployments utilizing the AzureChatOpenAI library from LangChain. Complete dependency specifications and environment configuration details are provided in supplementary materials to ensure experimental reproducibility.Model Configuration: All experimental evaluations utilize GPT-4o (Azure OpenAI API, model version gpt-4o-2024-05-13) as the underlying language model for consistency across datasets and configurations, supporting both text-only and multimodal (text+image) evaluation scenarios. Note that MDAgents baseline results marked with asterisks (*) in our results represent reproduced experiments using our GPT-4o setup, while the original MDAgents framework was evaluated using GPT-4, a larger and more computationally expensive model. The system supports multiple concurrent API deployments with round-robin question assignment for efficient parallel processing while maintaining deployment-specific performance tracking and rate limit management.Teamwork Component Parameters: Teamwork mechanisms operate through structured prompt templates with specific parameter configurations: trust mechanisms initialize at neutral calibration values with dynamic adjustment based on agent performance consistency, leadership coordination employs weighted voting aggregation with enhanced authority for designated coordinators, and communication protocols implement structured three-round exchanges with explicit verification steps for closed-loop communication components.Reproducibility and Logging: Experimental reproducibility is ensured through systematic random seed control (111, 222, 333) governing both question sampling and inter-agent communication sequences. Agent interactions are recorded through structured metadata channels, capturing teamwork behaviors, communication patterns, and decision evolution throughout the collaborative workflow. This comprehensive logging infrastructure enables detailed analysis of teamwork effectiveness and component-specific contributions to overall performance, facilitating reproducible evaluation and systematic ablation studies. Component activation follows deterministic selection logic based on task characteristics, with adaptive configuration determined through rule-based mapping between reasoning modalities and optimal teamwork combinations.Experimental EvaluationWe evaluate TeamMedAgents through a comprehensive performance assessment across eight established medical benchmarks, followed by systematic ablation studies to isolate individual teamwork component contributions. Our evaluation methodology implements standardized metrics for medical AI assessment (Hicks et al. 2022) and follows established ablation study protocols for AI systems (Newell 1975) that systematically remove components to quantify their individual contributions to system performance.The experimental design employs a two-stage evaluation framework: first, baseline comparison between single-agent approaches (no teamwork components) and our dynamic allocation framework where both agent count and teamwork components are variably assigned by the recruiter agent based on question-specific requirements, followed by systematic ablation studies where individual teamwork components are isolated and agent configurations range from 2-5 agents to quantify performance contributions across different team composition scales. This approach enables both practical validation of diagnostic accuracy and theoretical understanding of teamwork component interactions in collaborative medical reasoning.Configuration 	MedQA 	PubMedQA 	MMLU-Pro 	MedMCQA 	DDXPlus 	MedBullets 	PathVQA 	PmcVQA	Text 	Text 	Text 	Text 	Text 	Text 	Text+Image 	Text+ImageSingle-Agent Baseline 	75.0 Â± 1.3 	61.5 Â± 2.2 	75.3 Â± 6.0 	77.0 Â± 0.0 	70.3 Â± 2.0 	67.0 Â± 1.4 	57.9 Â± 1.6 	49.0 Â± 3.7MDAgents (Standard) 	88.7 Â± 4.0 	75.0 Â± 1.0 	80.7 Â± 2.0* 	80.8 Â± 1.1* 	77.9 Â± 2.1 	80.8 Â± 1.7 	65.3 Â± 3.9 	56.4 Â± 4.5Leadership 	89.0 Â± 5.0 	71.6 Â± 1.1 	79.3 Â± 3.3 	83.6 Â± 2.3 	76.0 Â± 2.83 	72.3 Â± 2.70 	69.3 Â± 0.9 	45.3 Â± 3.8Closed-loop 	89.0 Â± 3.0 	73.3 Â± 2.7 	80.0 Â± 4.0 	82.8 Â± 2.0 	76.0 Â± 8.49 	72.0 Â± 1.70 	70.0 Â± 4.9 	53.3 Â± 5.2Mutual Monitoring 	89.0 Â± 5.0 	73.3 Â± 2.8 	83.0 Â± 1.5 	80.8 Â± 2.0 	81.0 Â± 5.66 	74.0 Â± 2.40 	73.3 Â± 2.5 	54.0 Â± 1.6Shared Mental Model 	86.3 Â± 3.6 	72.6 Â± 4.4 	84.0 Â± 2.0 	83.6 Â± 2.7 	79.0 Â± 2.83 	68.6 Â± 1.40 	76.0 Â± 1.6 	56.7 Â± 2.5Team Orientation 	88.3 Â± 2.1 	71.0 Â± 2.2 	81.0 Â± 3.0 	84.8 Â± 2.7 	76.5 Â± 2.12 	72.0 Â± 2.50 	68.0 Â± 2.8 	56.7 Â± 3.4Mutual Trust 	90.0 Â± 4.0 	72.6 Â± 1.8 	82.0 Â± 2.0 	80.4 Â± 2.9 	81.5 Â± 2.12 	70.6 Â± 2.30 	72.0 Â± 1.6 	52.7 Â± 3.4All Features 	91.3 Â± 3.0 	69.3 Â± 1.7 	82.0 Â± 2.7 	82.4 Â± 1.5 	77.5 Â± 2.12 	72.4 Â± 2.60 	74.7 Â± 2.5 	43.3 Â± 3.8TeamMedAgents 	92.6 Â± 2.7 	76.6 Â± 3.4 	82.0 Â± 4.0 	82.4 Â± 2.0 	78.0 Â± 5.29 	72.0 Â± 2.00 	74.67 Â± 4.1 	56.67 Â± 4.1Table 1: Performance across medical benchmarks. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). MDAgents(Standard) uses original GPT-4 results from (Kim et al. 2024), except * which indicates our GPT-4o reproduction. All other configurations use GPT-4o.Experimental SetupDatasets and Benchmarks: We evaluate on eight diverse medical reasoning benchmarks spanning textual and visual modalities to comprehensively assess clinical decision-making capabilities: MedQA (Jin et al. 2020) for USMLE-style clinical reasoning requiring diagnostic synthesis; PubMedQA (Jin et al. 2019) for evidence-based reasoning over biomedical literature; MMLU-Pro Medical (Wang et al. 2024) for complex multi-step medical inference with 10-choice questions; MedMCQA (Pal, Umapathi, and Sankarasubbu 2022) for comprehensive medical knowledge assessment across 21 medical subjects; DDXPlus (Tchango et al. 2022) for differential diagnosis reasoning; MedBullets (Chen et al. 2024) for clinical case analysis; Path-VQA (He et al. 2020) for pathology image question answering; and PMC-VQA (Zhang et al. 2023) for medical visual question answering across diverse imaging modalities.Configuration Space: We systematically evaluate ten distinct teamwork configurations including individual components (Leadership, Closed-Loop Communication, Mutual Monitoring, Shared Mental Model, Team Orientation, Mutual Trust), comprehensive integration (All Features), and empirically-optimized combinations (Special Set). Our baseline configurations comprise: (1) Single-Agent: Medical Generalist without multi-agent recruitment; (2) MDAgents (Standard): The established multi-agent framework serving as our primary collaborative baseline (Kim et al. 2024).Evaluation Protocol: Each configuration was evaluated on 50 randomly sampled questions per dataset, with results averaged across 3 independent runs using different random seeds (111, 222, 333) to ensure statistical reliability and reproducibility. This sample size follows established practices in medical AI evaluation  (Kim et al. 2024). The primary evaluation metric is accuracy via weighted voting aggregation.Results and AnalysisOverall Performance: As evident in Table 1, TeamMedAgents demonstrates consistent performance improvements across all eight evaluated medical datasets. Compared to the established MDAgents framework (Kim et al. 2024), TeamMedAgents achieves improvements on seven of eight datasets, with notable gains in visual reasoning tasks (Path-VQA: +9.37%, PMC-VQA: +0.27%). The performance variation is typically â‰¤ 4.0% across multiple independent runs, indicating stable performance enhancement across diverse medical reasoning modalities.Performance analysis reveals distinct optimization patterns: text-based clinical reasoning tasks benefit most from leadership and trust mechanisms, knowledge assessment tasks benefit from shared mental models, while visual reasoning tasks benefit from monitoring-enhanced configurations. This systematic variation suggests that task-specific teamwork configurations outperform universal approach of enabling all the teamwork components.Component-Specific Effectiveness: Individual teamwork mechanisms exhibit distinct optimization patterns, revealing task-specific sensitivities to collaborative structures. Shared Mental Model achieves strong performance on knowledge-intensive tasks (MMLU-Pro: 84.0%, MedMCQA: 83.6%), indicating that conceptual alignment between agents enhances factual medical reasoning. This finding aligns with organizational psychology research demonstrating that shared understanding improves team performance in knowledge-based domains (Cannon-Bowers et al. 1995).Mutual Trust excels in clinical decision-making scenarios (MedQA: 90.0%), reflecting the critical role of confidence calibration in diagnostic reasoning where agents must weigh uncertain evidence and competing hypotheses. This mechanism enables appropriate modulation of information sharing depth based on demonstrated competence, preventing over-reliance on potentially erroneous reasoning paths (Webber 2002).Leadership demonstrates consistent improvements across textual reasoning tasks (MedQA: 89.0%, MedMCQA: 83.6%), highlighting the value of coordination and synthesis in multi-agent medical reasoning. However, vision-based tasks show more modest gains, suggesting domain-specific optimization requirements for visual medical reasoning.Synergistic Effects and Adaptive Configuration: Our results reveal that comprehensive teamwork integration (All Features: 91.3% MedQA, 69.3% PubMedQA) does not universally optimize performance, challenging assumptions that additional collaboration mechanisms inherently improve outcomes. The adaptive TeamMedAgents configuration, which selectively combines teamwork components based on task characteristics, consistently outperforms both individual components and comprehensive integration (92.6% MedQA, 76.6% PubMedQA).This finding establishes a fundamental principle in multi-agent collaboration: effective teamwork requires appropriate mechanism selection, as individual components may enhance or deter system performance depending on task-specific coordination requirements. Vision-based tasks (Path-VQA: 74.67%, PMC-VQA: 56.67%) particularly benefit from this selective approach, showing substantial improvements over baseline vision capabilities (Marks, Mathieu, and Zaccaro 2001b).Task-Specific Optimization PatternsTable 2: Optimal teamwork configurations by medical reasoning modality, derived from systematic ablation analysis.Dataset 	Reasoning Modality 	Optimal ConfigurationMedQA 	Clinical Diagnosis 	Leadership + Trust + OrientationPubMedQA 	Evidence Synthesis 	Leadership + Closed-Loop + TrustMMLU-Pro 	Complex Inference 	Shared Mental ModelMedMCQA 	Knowledge Assessment 	Shared Mental ModelDDXPlus 	Differential Diagnosis 	Monitoring + TrustMedBullets 	Clinical Case Analysis 	MonitoringPathVQA 	Pathology Visual Analysis 	Monitoring + Shared Mental Model + Closed-LoopPMCVQA 	Medical Visual Reasoning 	Shared Mental Model + Closed-Loop + OrientationOur ablation analysis reveals systematic patterns in optimal teamwork configurations across diverse medical reasoning modalities (Table 2). Clinical reasoning tasks (MedQA) achieve peak performance through comprehensive coordination mechanisms emphasizing leadership, trust-based information sharing, and collaborative orientation. This configuration mirrors real-world clinical teams where designated leaders coordinate multi-disciplinary expertise while trust mechanisms ensure reliable information exchange under diagnostic uncertainty (Kim et al. 2024).Evidence synthesis tasks (PubMedQA) optimize with leadership coordination, structured communication protocols, and trust-modulated collaboration. The prominence of Closed-Loop Communication in this configuration reflects the critical importance of precise information verification when synthesizing biomedical literature, where misinterpretation of research findings can propagate through the reasoning chain (Bandow 2001).Knowledge assessment tasks (MMLU-Pro, MedMCQA) consistently achieve peak performance with Shared Mental Model mechanisms, emphasizing conceptual alignment over communication intensity. This pattern indicates that factual medical reasoning primarily benefits from synchronized understanding of task objectives and evaluation criteria rather than complex coordination protocols (Cannon-Bowers et al. 1995).Differential diagnosis tasks (DDXPlus) demonstrate optimal performance through Monitoring and Trust mechanisms, reflecting the systematic elimination process inherent in diagnostic reasoning where agents must scrutinize peer assessments and calibrate confidence in competing hypotheses (Tchango et al. 2022). Clinical case analysis (MedBullets) benefits primarily from Mutual Monitoring, emphasizing the step-by-step verification process essential for complex clinical reasoning scenarios (Chen et al. 2024).Visual medical reasoning tasks exhibit distinct optimization patterns reflecting their multimodal complexity. Pathology visual analysis (Path-VQA) achieves optimal performance through combining Monitoring, Shared Mental Models, and Closed-Loop Communication, indicating the need for both systematic peer review and precise information verification when interpreting histopathological images (He et al. 2020). Medical visual reasoning (PMC-VQA) optimizes with Shared Mental Models, Closed-Loop Communication, and Team Orientation, reflecting the collaborative alignment required for comprehensive medical image understanding across diverse modalities (Zhang et al. 2023).DiscussionOur systematic evaluation of evidence-based teamwork components within multi-agent medical reasoning systems yields several critical insights into the design of multi-agent systems. Across eight diverse medical benchmarks, our findings show that selective activation of teamwork mechanisms, rather than indiscriminate integration, produces superior performance. This challenges the assumption that more coordination always leads to better collaboration and highlights the need for task-aligned teamwork configurations in high-stakes domains.Theoretical Implications and Mechanistic InsightsThe effectiveness of Shared Mental Models in knowledge-based tasks aligns with cognitive load theory that synchronizing conceptual understanding among agents promotes a shared understanding, enabling consistency in complex information processing (Stout et al. 1999). Visual reasoning tasks, by contrast, benefit more from Mutual Monitoring mechanisms. These enable systematic peer review, a crucial step when interpreting multimodal data like pathology images. This result echoes findings in clinical radiology and pathology, where structured review protocols enhance diagnostic accuracy.Trust-based mechanisms demonstrate particular effectiveness in uncertainty-heavy tasks such as clinical diagnosis and differential diagnosis, where agents must dynamically adjust confidence in teammate contributions based on demonstrated competence. This adaptive collaboration mirrors the flexibility required in real clinical teams facing novel or ambiguous cases (Kozlowski et al. 1999). Importantly, our findings reveal interference effects when all teamwork mechanisms are activated simultaneously. Rather than enhancing performance, full integration can introduce unnecessary coordination overhead that degrades effectiveness. This emphasizes the importance of principled mechanism selection based on task characteristics, rather than assuming that more coordination is always better (Driskell and Salas 1992).Future work: Despite the modular flexibility of our approach, several open challenges remain. First, automated configuration selection remains a non-trivial problem. While our ablation studies identify optimal teamwork patterns post hoc, developing real-time, task-aware adaptation strategies is essential for broader applicability. Second, translating human teamwork behaviors into computational analogs still faces limitations. Components like adaptive backup behavior or situational role reassignment are difficult to operationalize in static, single-shot question-answering settings, despite their proven effectiveness in real-world clinical teams. Finally, the principles we establish here may generalize beyond medicine, offering a foundation for teamwork-driven AI systems in domains such as finance, disaster response, and autonomous systems, where effective collaboration under uncertainty is equally critical.ConclusionWe present TeamMedAgents, a multi-agent framework that systematically integrates evidence-based teamwork mechanisms into medical decision-making. Through comprehensive ablation studies across eight medical benchmarks, our results demonstrate that modular teamwork integration yields substantial improvements over single-agent baselines (up to 17.6%) and established multi-agent frameworks from prior work (Kim et al. 2024). A key finding reveals that optimal performance emerges through task-specific mechanism selection. Clinical reasoning benefits from leadership-trust combinations, evidence synthesis requires structured communication protocols, while knowledge assessment optimizes with shared mental models. Indiscriminate integration of all teamwork components introduces coordination overhead, underscoring the need for principled configuration selection. These results provide empirical guidance for selective teamwork deployment in collaborative AI systems.References    Agashe, Zhang, and Koppel (2023)Agashe, S.; Zhang, Y.; and Koppel, J. 2023. LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models. arXiv preprint arXiv:2310.03903.Bandow (2001)Bandow, D. 2001. Time to create sound teamwork. The Journal for Quality and Participation, 24(2): 41.Cannon-Bowers et al. (1995)Cannon-Bowers, J. A.; Tannenbaum, S. I.; Salas, E.; and Volpe, C. E. 1995. Team performance and training in complex environments: Recent findings from applied research. In Current Directions in Psychological Science, 83â€“87.Chen et al. (2024)Chen, H.; Fang, Z.; Singla, Y.; and Dredze, M. 2024. Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions. arXiv preprint arXiv:2402.18060.Chen et al. (2025)Chen, K.; Zhen, T.; Wang, H.; Liu, K.; Li, X.; Huo, J.; Yang, T.; Xu, J.; Dong, W.; and Gao, Y. 2025. MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems. arXiv preprint. ArXiv preprint.Chen, Kumar, and Patel (2023)Chen, X.; Kumar, R.; and Patel, N. 2023. Adaptive Multi-Agent Deep Reinforcement Learning for Timely Healthcare Interventions. IEEE Transactions on Biomedical Engineering, 70(9): 2445â€“2456.Chen, Wang, and Zhang (2025)Chen, Y.; Wang, L.; and Zhang, H. 2025. Multi-Agent Collaboration Mechanisms: A Survey of LLMs. arXiv preprint arXiv:2501.06322.Driskell and Salas (1992)Driskell, J. E.; and Salas, E. 1992. Collective behavior and team performance. Human Factors, 34(3): 277â€“288.Du et al. (2023)Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch, I. 2023. Improving Factuality and Reasoning in Language Models through Multiagent Debate. arXiv preprint arXiv:2305.14325.Eby and Dobbins (1997)Eby, L. T.; and Dobbins, G. H. 1997. Collectivistic orientation in teams: An individual and group-level analysis. Journal of Organizational Behavior, 18(3): 275â€“295.Foerster et al. (2018)Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and Whiteson, S. 2018. Counterfactual multi-agent policy gradients. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).Hassan, El-Sayed, and Ahmed (2025)Hassan, S. A.; El-Sayed, W. M.; and Ahmed, F. Z. 2025. From awareness to action: investigating the impact of big-five teamwork model awareness on rationing of nursing care and patient-centered care. BMC Nursing, 24(1): 1â€“15.He et al. (2020)He, X.; Zhang, Y.; Mou, L.; Xing, E.; and Xie, P. 2020. PathVQA: 30000+ Questions for Medical Visual Question Answering. arXiv preprint arXiv:2003.10286.Hicks et al. (2022)Hicks, S. A.; StrÃ¼mke, I.; Thambawita, V.; Hammou, M.; Riegler, M. A.; Halvorsen, P.; and Parasa, S. 2022. On evaluation metrics for medical applications of artificial intelligence. Scientific Reports, 12(1): 5979.Hong et al. (2024)Hong, S.; Zhuge, M.; Chen, J.; Zheng, X.; Cheng, Y.; Wang, J.; Zhang, C.; Wang, Z.; Yau, S. K. S.; Lin, Z.; Zhou, L.; Ran, C.; Xiao, L.; Wu, C.; and Schmidhuber, J. 2024. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In The Twelfth International Conference on Learning Representations.Jin et al. (2020)Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and Szolovits, P. 2020. What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv preprint arXiv:2009.13081.Jin et al. (2019)Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; and Lu, X. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2567â€“2577. Association for Computational Linguistics.Kim et al. (2024)Kim, Y.; Park, C.; Jeong, H.; Chan, Y. S.; Xu, X.; McDuff, D.; Lee, H.; Ghassemi, M.; Breazeal, C.; and Park, H. W. 2024. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making. arXiv preprint arXiv:2404.15155.King, Battles, and Baker (2008)King, H. B.; Battles, J.; and Baker, D. P. 2008. TeamSTEPPSâ„¢: Team Strategies and Tools to Enhance Performance and Patient Safety. In Henriksen, K.; Battles, J. B.; and Keyes, M. A., eds., Advances in Patient Safety: New Directions and Alternative Approaches (Vol. 3: Performance and Tools). Rockville, MD: Agency for Healthcare Research and Quality (US).Kozlowski et al. (1999)Kozlowski, S. W.; Gully, S. M.; Nason, E. R.; and Smith, E. M. 1999. Developing adaptive teams: A theory of compilation and performance across levels and time. In The Changing Nature of Performance: Implications for Staffing, Motivation, and Development, 240â€“292. Jossey-Bass.Li et al. (2023)Li, G.; Hammoud, H. A. A. K.; Itani, H.; Khizbullin, D.; and Ghanem, B. 2023. CAMEL: Communicative Agents for â€Mindâ€ Exploration of Large Language Model Society. arXiv preprint arXiv:2303.17760.Marks, Mathieu, and Zaccaro (2001a)Marks, M. A.; Mathieu, J. E.; and Zaccaro, S. J. 2001a. A temporally based framework and taxonomy of team processes. Academy of Management Review, 26(3): 356â€“376.Marks, Mathieu, and Zaccaro (2001b)Marks, M. A.; Mathieu, J. E.; and Zaccaro, S. J. 2001b. A Temporally Based Framework and Taxonomy of Team Processes. Academy of Management Review, 26(3): 356â€“376.Newell (1975)Newell, A. 1975. A tutorial on speech understanding systems. In Speech Recognition, 3â€“54. Academic Press.Pal, Umapathi, and Sankarasubbu (2022)Pal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, 248â€“260. PMLR.Qian et al. (2023)Qian, C.; Cong, X.; Yang, C.; Chen, W.; Su, Y.; Xu, J.; Liu, Z.; and Sun, M. 2023. ChatDev: Communicative Agents for Software Development. arXiv preprint arXiv:2307.07924.Rajbhandari et al. (2024)Rajbhandari, S.; Dong, Y.; Kumar, A.; Ramesh, M.; and Jha, M. 2024. Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications. arXiv preprint arXiv:2412.05449.Salas, Sims, and Burke (2005)Salas, E.; Sims, D. E.; and Burke, C. S. 2005. Is there a â€Big Fiveâ€ in teamwork? Small Group Research, 36(5): 555â€“599.Stone and Veloso (2000)Stone, P.; and Veloso, M. 2000. Multiagent Systems: A Survey from a Machine Learning Perspective. Autonomous Robots, 8(3): 345â€“383.Stout et al. (1999)Stout, R. J.; Cannon-Bowers, J. A.; Salas, E.; and Milanovich, D. M. 1999. Planning, shared mental models, and coordinated performance: An empirical link is established. Human Factors, 41(1): 61â€“71.Tchango et al. (2022)Tchango, A. F.; Goel, R.; Wen, Z.; Martel, J.; and Ghosn, J. 2022. DDXPlus: A New Dataset For Automatic Medical Diagnosis. Advances in Neural Information Processing Systems, 35: 23304â€“23318.Verhage, Herrington, and Koedijk (2022)Verhage, A.; Herrington, V.; and Koedijk, R. 2022. Police Dyads Within an Operational Simulation: an Empirical Test of the Research Propositions Made in the â€Big Fiveâ€ Teamwork Approach. Criminal Justice Policy Review, 33(8): 892â€“919.Wang et al. (2024)Wang, Y.; Ma, X.; Zhang, G.; Ni, Y.; Chandra, A.; Guo, S.; Ren, W.; Arulraj, A.; He, X.; Jiang, Z.; et al. 2024. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. arXiv preprint arXiv:2406.01574.Wang, Li, and Zhou (2024)Wang, Z.; Li, J.; and Zhou, Q. 2024. MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration. arXiv preprint arXiv:2410.12532.Webber (2002)Webber, S. S. 2002. Trust, satisfaction, and equity in small groups: whatâ€™s important to members? Small Group Research, 33(1): 85â€“106.Wu et al. (2023)Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Zhang, S.; Zhu, E.; Li, B.; Jiang, L.; Zhang, X.; and Wang, C. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv preprint arXiv:2308.08155.Zaccaro, Rittman, and Marks (2001)Zaccaro, S. J.; Rittman, A. L.; and Marks, M. A. 2001. Team leadership. The Leadership Quarterly, 12(4): 451â€“483.Zhang, Liu, and Chen (2024)Zhang, W.; Liu, M.; and Chen, Y. 2024. KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph Enhancement for Medical Diagnosis. arXiv preprint arXiv:2412.16833.Zhang et al. (2023)    Zhang, X.; Wu, C.; Zhao, Z.; Lin, W.; Zhang, Y.; Wang, Y.; and Xie, W. 2023. PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering. arXiv preprint arXiv:2305.10415.AppendixExtended Ablation Study ResultsThis appendix presents comprehensive ablation analysis across teamwork configurations and team sizes, extending our main experimental evaluation with detailed performance breakdowns across agent configurations ranging from 2-4 agents per team. These results provide granular insight into the interaction effects between teamwork components and team composition, demonstrating how optimal configurations vary systematically with both task characteristics and collaborative team structure. The extended evaluation validates our theoretical framework by revealing consistent patterns in teamwork component effectiveness across diverse medical reasoning modalities.Text-Based Medical Reasoning TasksTable 3: MedQA performance across team configurations and sizes. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). Bold values indicate best performance per team size.Configuration n=2 n=3 n=4 Single-Agent Baseline 88.0Â±2.0 88.0Â±0.0 86.0Â±4.0 MDAgents (Standard) 87.3Â±2.6 91.0Â±1.0 89.0Â±5.0 Leadership 87.6Â±2.2 88.0Â±6.0 90.3Â±5.0 Closed-Loop Communication 88.7Â±5.4 89.0Â±3.0 88.0Â±4.0 Mutual Monitoring 86.9Â±2.9 89.0Â±5.0 89.0Â±3.0 Shared Mental Model 86.3Â±3.6 89.0Â±3.0 85.0Â±1.0 Team Orientation 88.3Â±0.3 89.0Â±5.0 89.0Â±3.0 Mutual Trust 90.0Â±3.3 89.0Â±6.0 90.0Â±4.0 All Features 89.0Â±1.0 89.0Â±4.0 89.0Â±1.0 Special Set 91.3Â±2.6 91.6Â±4.0 91.3Â±3.0Table 4: PubMedQA performance across team configurations and sizes. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). Bold values indicate best performance per team size.Configuration n=2 n=3 n=4 Single-Agent Baseline 71.0Â±1.0 61.0Â±11.0 56.7Â±8.7 MDAgents (Standard) 75.0Â±3.0 56.0Â±15.0 58.3Â±8.3 Leadership 68.0Â±4.0 64.0Â±2.0 67.3Â±13.3 Closed-Loop Communication 76.0Â±6.0 64.0Â±6.0 64.7Â±12.7 Mutual Monitoring 72.0Â±0.0 61.0Â±13.0 69.3Â±8.7 Shared Mental Model 72.0Â±6.0 61.0Â±8.0 62.7Â±7.3 Team Orientation 73.0Â±5.0 64.7Â±6.7 62.0Â±11.0 Mutual Trust 73.0Â±1.0 63.3Â±4.7 64.7Â±3.8 All Features 70.0Â±2.0 63.3Â±2.7 66.0Â±4.0 Special Set 69.0Â±1.0 70.0Â±6.0 68.0Â±7.0Table 5: MMLU-Pro Medical performance across team configurations and sizes. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). Bold values indicate best performance per team size.Configuration n=2 n=3 n=4 Single-Agent Baseline 73.3Â±4.0 75.3Â±6.0 84.0Â±1.0 MDAgents (Standard) 80.7Â±5.3 80.7Â±2.0 83.0Â±1.0 Leadership 77.3Â±4.0 79.3Â±3.3 80.0Â±1.0 Closed-Loop Communication 76.0Â±4.0 79.3Â±2.0 80.0Â±4.0 Mutual Monitoring 77.3Â±3.3 79.3Â±4.7 83.0Â±1.0 Shared Mental Model 78.0Â±4.0 78.7Â±4.0 84.0Â±2.0 Team Orientation 76.0Â±4.0 79.3Â±2.0 81.0Â±3.0 Mutual Trust 77.3Â±3.3 79.7Â±2.7 82.0Â±2.0 All Features 78.0Â±1.3 82.0Â±2.7 82.0Â±0.0 Special Set 80.0Â±4.0 81.3Â±3.3 82.0Â±4.0Visual Medical Reasoning TasksTable 6: PathVQA dataset performance across team configurations and sizes. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). Bold values indicate best performance per team size.Configuration n=2 n=3 n=4 Single-Agent Baseline 57.9Â±1.6 57.9Â±1.6 57.9Â±1.6 MDAgents (Standard) 65.3Â±3.9 65.3Â±3.9 65.3Â±3.9 Leadership 72.0Â±5.9 72.0Â±2.8 69.3Â±0.9 Closed-Loop Communication 71.3Â±1.9 73.3Â±0.9 70.0Â±4.9 Mutual Monitoring 73.3Â±2.5 71.3Â±0.9 74.0Â±2.8 Shared Mental Model 69.3Â±4.7 76.0Â±1.6 69.3Â±2.5 Team Orientation 68.0Â±2.8 74.0Â±2.8 69.3Â±0.9 Mutual Trust 72.7Â±0.9 72.0Â±1.6 69.3Â±1.9 All Features 74.7Â±2.5 72.0Â±4.3 71.3Â±2.5 Special Set 74.0Â±2.0 74.67Â±4.1 72.0Â±3.0Table 7: PMC-VQA dataset performance across team configurations and sizes. Results show accuracy (%) averaged over 3 runs with standard error indicators (Â±). Bold values indicate best performance per team size.Configuration n=2 n=3 n=4 Single-Agent Baseline 49.0Â±3.7 49.0Â±3.7 49.0Â±3.7 MDAgents (Standard) 56.4Â±4.5 56.4Â±4.5 56.4Â±4.5 Leadership 45.3Â±3.8 38.0Â±3.3 42.0Â±3.3 Closed-Loop Communication 53.3Â±5.2 52.0Â±0.0 52.7Â±6.2 Mutual Monitoring 49.3Â±0.9 54.0Â±1.6 50.7Â±3.4 Shared Mental Model 50.0Â±1.6 52.7Â±6.2 56.7Â±2.5 Team Orientation 56.7Â±5.2 49.3Â±5.2 54.0Â±4.3 Mutual Trust 49.3Â±2.5 50.7Â±3.8 52.7Â±3.4 All Features 43.3Â±6.2 43.3Â±3.8 42.0Â±1.6 Special Set 56.0Â±4.0 56.67Â±4.1 55.3Â±3.5"
}, {
  "research_proposal": "The paper proposes Memp, a task-agnostic procedural memory framework for LLM-based agents that stores both low-level action trajectories and high-level procedural scripts from past experiences, retrieves relevant memory via vector similarity, and dynamically updates it through success-based appending, validation, and reflection to improve reasoning and task efficiency.",
  "benchmark": ["TravelPlanner CS", "TravelPlanner HC", "ALFWorld Dev", "ALFWorld Test"],
  "performance_metrics": [ 8.01, -3.12, 45.0, 35.72 ],
  "paper_link": "https://arxiv.org/html/2508.06433v1#Sx5",
  "full_text": "Mâ€‹eâ€‹mp: Exploring Agent Procedural MemoryRunnan Fangâ™ â™¡âˆ—, Yuan Liangâ™ , Xiaobin Wangâ™¡, Jialong Wuâ™¡,Shuofei Qiaoâ™ â™¡, Pengjun Xieâ™¡, Fei Huangâ™¡, Huajun Chenâ™ , Ningyu Zhangâ™ â€ƒ Equal Core Contributors.â€ƒ Corresponding Author.AbstractLarge Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Mâ€‹eâ€‹mp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.IntroductionAs large language models (LLMs) grow ever more powerful, LLM-based agents augmented by their own reasoning and external tools are taking on increasingly sophisticated works (Zhao et al. 2023; Wang et al. 2024a; Xi et al. 2025; Qiao et al. 2023). No longer mere assistants, these agents now trawl the web for elusive insights and weave them into comprehensive, publication ready reports, like Deep Research (OpenAI 2025; x.ai 2025) and WebDancer (Wu et al. 2025a). Moreover, they can handle complex data analyses (Lan et al. 2025; Ifargan et al. 2025; Ou et al. 2025), navigate multi-step GUI workflows (Luo et al. 2025; Qin et al. 2025), and sustain long-horizon, tool-rich interactions (Yao et al. 2025; Barres et al. 2025; Chen et al. 2025; Fang et al. 2025) with precision. Yet executing such intricate, long-horizon tasks demands dozens of steps and protracted runtimes. Along the way, unpredictable external eventsâ€”network glitches, UI changes, shifting data schemasâ€”can derail the entire process. Restarting from scratch every time is a punishing ordeal for present-day agents. Beneath their surface diversity, many complex tasks share deep structural commonalities and a similar environment. Instead of starting fresh each time, an agent should extract its experience from past successes. By turning earlier trajectories into reusable templates like patterns of reasoning, tool sequences, and recovery tactics, it can progress step by step, learning from every failure and success, until even the most convoluted missions become routine.Refer to captionFigure 1: With procedural memory, agents can improve both the success rate (accuracy â†‘) and execution efficiency (steps â†“) when solving similar tasks.The capacity to distill, chronicle, and re-apply lessons from oneâ€™s own experiential trajectory is the bedrock of human learning and the pivotal gateway through which an agent ascends toward self-directed refinement (Liu et al. 2025a; Sumers et al. 2023; Li et al. 2023). Procedural memory (Gupta and Cohen 2002; Cohen and Squire 1980) silently compiles habitual skills into executable subroutines, enabling unconscious, fluent action. While contemporary agents built on LLMs can compose short action plans or call external tools, their procedural knowledge is either hand-crafted, stored as brittle prompt templates, or implicitly entangled in model parameters that are expensive to update. Existing memory-augmented frameworks such as LangGraph (Mavroudis 2024), AutoGPT (Yang, Yue, and He 2023), or agent cognitive architectures like Memory Bank (Zhong et al. 2024a) and Soar (Laird 2022) provide coarse abstractions (buffers, rule chunks, production systems) but leave the optimization of procedural memory life-cycle operations about how skills are built, indexed, patched, and eventually pruned, largely unexamined. Consequently, there is no principled way to quantify how efficiently an agent evolves its procedural repertoire or to guarantee that new experiences improve rather than erode performance.To close this gap, we present Mâ€‹eâ€‹mp, a task-agnostic framework that treats procedural memory as a first-class optimization object. The core exploration of Mâ€‹eâ€‹mp lies in how different strategies for memory construction, retrieval, and updating affect overall performance. During the construction phase, we follow the majority of traditional memory architectures and agent-based memory designs by leveraging either the full historical trajectory or explicit guidelines to guide the process. In the retrieval phase, we experiment with various key-building strategiesâ€”such as query-vector matching and keyword-vector matchingâ€”to investigate how procedural memory can be constructed more precisely. Unlike prior memory mechanisms or learning from experience, Mâ€‹eâ€‹mp introduces diverse procedural-memory update strategies: In the realm of agents, memory updating is crucial for agents to adapt to dynamic environments. By incorporating diverse strategies like ordinary addition, validation filtering, reflection, and dynamic discarding, agents can efficiently manage their knowledge base. This ensures they stay updated with new information, discard outdated data, and optimize memory resources. Such strategies enhance learning efficiency, improve decision-making quality, and boost adaptability, allowing agents to perform optimally in various tasks and scenarios.We instantiate Mâ€‹eâ€‹mp on top of strong LLMs (GPT-4o and Claude, Qwen) and evaluate on two diverse domains: long-horizon housework ALFWorld (Shridhar et al. 2021) and long-term information seeking task TravelPlanner (Xie et al. 2024). On two benchmark datasets that rigorously evaluate agent capabilities, we demonstrate that constructing and retrieving procedural memory during training empowers an agent to distill and reuse its prior experience. When this memory is exploited at test time, the agentâ€™s task accuracy rises, and compared with tackling each instance in isolation, it eliminates most fruitless exploration on unfamiliar tasks, yielding substantial reductions in both step count and token consumption. Further, by equipping the agent with a set of memory-update mechanisms, we allow it to build and refine its procedural memory while acting in the test environment. This endows the agent with a continual, almost linear mastery of the task. Extensive ablations reveal that procedural memory also scales gracefully and transfers effectively to new, related tasks.Related WorksMemory in Language Agents.Memory is a foundational component in language agents, enabling them to retain and utilize past information across multiple timescales, including short-term, episodic, and long-term memory, to enhance their performance and adaptability (Zhou et al. 2023, 2024; Zhang et al. 2024; Liu et al. 2025a). These systems aim to mimic aspects of human memory to improve coherence, personalization, and learning capabilities  (Chhikara et al. 2025; Wu et al. 2025b). Current approaches include end-to-end memory systems (Yu et al. 2025; Zhou et al. 2025), external memory systems (Chhikara et al. 2025; Zhong et al. 2024b), and hierarchical memory structures (Hu et al. 2024a; Xu et al. 2025). These methods involve encoding and storing information in various formats, using retrieval mechanisms like vector embeddings and semantic search, and implementing memory updating and forgetting strategies to maintain relevance and efficiency. Despite its importance, memory in multi-turn agent interactions remains underexplored, and enabling agents to effectively learn and utilize memory across trajectories poses a significant challenge. Procedural memory is a type of long-term memory that involves the retention of procedures and skills, such as typing or riding a bike, which are performed automatically without conscious thought. The agent utilizes procedural memory to internalize and automate repetitive tasks, decision-making processes, and interaction patterns, leading to more efficient and context-aware responses over time. Although there have been several works, such as Voyager (Wang et al. 2023), AWM (Wang et al. 2024b), and AutoManual (Chen et al. 2024), that utilize procedural memory to enhance agentsâ€™ capabilities on similar tasks, there still lacks a systematic analysis on how to construct, retrieve, and update such procedural memory. Therefore, our work mainly focuses on exploring how to build an effective procedural memory system for agents performing cross-trajectory tasks.Learning from Experience.LLM-based Agent learning from experience involves intelligence continuously improving their decision-making capabilities through interaction with environments and utilization of past experiences (Tan et al. 2025; Tang et al. 2025; Zhou et al. 2025; Qiao et al. 2025). This approach is crucial for developing adaptive and intelligent agents capable of handling dynamic real-world scenarios, as it allows them to optimize behaviors, reduce manual programming needs, and enhance performance across various tasks (Zheng et al. ; Liu et al. 2025c; Wang et al. 2025). Agents typically employ mechanisms such as reinforcement learning (Lu et al. 2025; Dong et al. 2025), experience replay (Feng et al. 2025; Liu et al. 2025b), imitation learning (Sun et al. 2024; Yang et al. 2024b), memory management (Hou, Tamoto, and Miyashita 2024; Hu et al. 2024b), and multi-agent learning to achieve this. However, current methods face limitations including low sample efficiency, poor generalization across tasks, catastrophic forgetting when learning new information, and there are very few features for memory update. Additionally, collecting high-quality training data can be challenging and may introduce biases. Addressing these limitations is essential for advancing the capabilities of LLM-based agents and ensuring their effective application in real-world contexts.PreliminaryRefer to captionFigure 2: The procedural memory framework consists of Build, Retrieve, and Update, which respectively involve encoding stored procedural memory, forming new procedural memories, and modifying existing ones in light of new experiences.When an agent influences its external environment by invoking external tools or executing prescribed actions, and iteratively refines its behavior over multiple rounds to accomplish a complex multi-step objective, this paradigm can be modeled as a Markov Decision Process (MDP). (Puterman 1990) Under this view, at each discrete time step t, the agent, situated in state stâˆˆS, chooses an action atâˆˆA, according to its policy Ï€â€‹(at|st), where A is the action space of the task. The environment then transitions to a new state st+1âˆˆS and emits an observation Ot. Consequently, the entire interaction trajectory may be compactly expressed as:	Ï„=(s0,a0,o1,s1,a1,o2,â€¦,sT),		(1)where Ï„ is the complete exploration trajectory of this task. Moreover, a reward function R will evaluate the taskâ€™s completion r within this environment eâ€‹nâ€‹v by assigning a score based on the final state sT or the entire trajectory Ï„.	r=Râ€‹(eâ€‹nâ€‹v,sT,Ï„)âˆˆ[0,1]		(2)Although approaches resembling Markov Decision Processes inevitably contain erroneous actions and exploratory attempts, the contextual information they generate becomes valuable for decision-making as the modelâ€™s reasoning and reflective capabilities improve. Nevertheless, this benefit comes at a high test-time costâ€”both in time and in token consumption. When facing an entirely new and complex environment, many actions (or tokens) are spent simply understanding the environment and the task itself. This leads to redundancy when similar tasks are executed within the same environment: the agent has already acquired partial procedural knowledge about the environment or task during earlier episodes, yet fails to transfer that knowledge effectively to subsequent tasks.By shifting from parallel task completion to sequential task completion, the agent can learn and distill experience from earlier tasks, thereby reducing repetitive exploration. Inspired by human procedural memory, we propose to equip the agent with a procedural memory module. This module transforms the conventional policy Ï€â€‹(at|st) into Ï€mpâ€‹(at|st), where mp is the agentâ€™s learned procedural memory.Agent Procedural MemoryProcedural memory is the type of long-term memory responsible for knowing how to perform tasks and skills, such as typing or riding a bike. By mastering this type of procedural memory, humans avoid the need to relearn the process each time. For an agent, that is, for a task trajectory Ï„ and its reward r, a memory mp is constructed by a builder B, thereby achieving the acquisition of memory, namely	Mâ€‹eâ€‹m=âˆ‘t=1Tmpt,wâ€‹hâ€‹eâ€‹râ€‹eâ€‹mpt=Bâ€‹(Ï„t,rt)		(3)where Mâ€‹eâ€‹m is the procedural memory library acquired by the agent over the T tasks. After constructing the procedural memory library, when facing a new task tnâ€‹eâ€‹w, we need a good procedural memory retriever to recall a memory that fits tnâ€‹eâ€‹w. Generally speaking, we would choose the task tâˆˆT that is most similar to tnâ€‹eâ€‹w, because similar experiences are more helpful for the agent to complete the new task.	mrâ€‹eâ€‹tâ€‹râ€‹iâ€‹eâ€‹vâ€‹eâ€‹d=argâ¡maxmpiâˆˆMâ€‹eâ€‹mâ¡Sâ€‹(tnâ€‹eâ€‹w,ti)		(4)As we use cosine similarity for the vector embedding model Ï• of the task in the experiment, the retrieval process becomes:	mrâ€‹eâ€‹tâ€‹râ€‹iâ€‹eâ€‹vâ€‹eâ€‹d=argâ¡maxmpiâˆˆMâ€‹eâ€‹mâ¡Ï•â€‹(tnâ€‹eâ€‹w)â‹…Ï•â€‹(ti)â€–Ï•â€‹(tnâ€‹eâ€‹w)â€–â€‹â€–Ï•â€‹(ti)â€–.		(5)Moreover, as the number of completed tasks continuously increases, simply augmenting the agentâ€™s procedural memory is inconsistent with common sense. A well-designed procedural memory system should have a reasonable update mechanismâ€”that is, it should dynamically perform addition, deletion, modification, and retrieval based on the task execution context.Let Mâ€‹(t) denote the agentâ€™s procedural memory at time t, and Ï„t represent the set of tasks completed up to time t. Then, the update mechanism can be modeled as a function U that takes the current procedural memory and task execution feedback to produce the updated memory:	Mâ€‹(t+1)=Uâ€‹(Mâ€‹(t),Eâ€‹(t),Ï„t),		(6)where Eâ€‹(t) encapsulates the execution feedback (e.g., success, failure, performance metrics). A more sophisticated implementation of U could be represented as:	U=Aâ€‹dâ€‹dâ€‹(Mnâ€‹eâ€‹w)âŠ–Râ€‹eâ€‹mâ€‹oâ€‹vâ€‹eâ€‹(Moâ€‹bâ€‹sâ€‹o)âŠ•Uâ€‹pâ€‹dâ€‹aâ€‹tâ€‹eâ€‹(Meâ€‹xâ€‹iâ€‹sâ€‹t),		(7)where Mnâ€‹eâ€‹w represents new procedural memory to be added; Moâ€‹bâ€‹sâ€‹o indicates procedural memory to be removed, Meâ€‹xâ€‹iâ€‹sâ€‹tâ€‹iâ€‹nâ€‹g are tasks to be updated based on execution feedback Eâ€‹(t). This comprehensive formula captures the essential add, delete, and modify operations within the update mechanism.ExperimentModel 	Granularity 	TravelPlanner 	ALFWorld#CS â†‘ 	#HC â†‘ 	Steps â†“ 	Dev â†‘ 	Test â†‘ 	Steps â†“GPT-4o 	No Memory 	71.93 	12.88 	17.84 	39.28 	42.14 	23.76Script 	72.08 	5.50 	15.79 	66.67 	56.43 	18.52Trajectory 	76.02 	8.25 	14.64 	67.17 	74.29 	16.49Proceduralization 	79.94 	9.76 	14.62 	87.14 	77.86 	15.01Claude-3.5-sonnet 	No Memory 	63.49 	33.06 	18.84 	39.20 	34.97 	24.12Script 	62.08 	29.61 	19.21 	56.13 	53.59 	19.38Trajectory 	65.76 	29.61 	17.72 	69.28 	71.78 	15.97Proceduralization 	65.46 	30.14 	15.29 	82.50 	74.72 	15.79Qwen2.5-72b 	No Memory 	56.57 	7.34 	18.32 	44.91 	41.25 	21.38Script 	58.59 	7.34 	18.53 	66.24 	61.88 	17.13Trajectory 	63.41 	12.66 	18.12 	64.49 	69.57 	16.40Proceduralization 	63.82 	14.19 	17.94 	85.71 	77.19 	15.32Table 1: Results on Build Policy. #CS, #HC denote Commensense and Hard Constraint, respectively. â†‘ indicates the higher values are better, and â†“ denotes the lower values are better. The best results among all methods with similar settings are bolded, and the second-best results are underlined.In this section, we will introduce the Procedural Memory framework in detail (Figure 2), covering the storage, retrieval, and update modules of memory, as well as analyzing which strategies perform better within each module.Experimental SettingsDatasets.For our experiments, we adapt TravelPlanner (Xie et al. 2024) and ALFWorld (Shridhar et al. 2021) benchmarks. TravelPlanner is a benchmark designed to evaluate agentsâ€™ ability to use tools and perform complex planning under intricate constraints. In contrast, ALFWorld comprises household tasks. In each interaction round, the agent outputs an action, and the environment responds with textual feedback describing the resulting state. This process repeats for multiple turns until the task is completed or the maximum number of rounds is reached. ALFWorld includes test split to evaluate the agentâ€™s generalization ability.Backbones.In our experiments, we benchmarked our procedural memory on three base models. Specifically, we adopt the two proprietary frontier models that have consistently dominated public leaderboards: OpenAIâ€™s GPT-4o (OpenAI 2022) and Anthropicâ€™s Claude (Anthropic 2022), and complement them with the open-sourced Qwen2.5-72B-Instruct (Yang et al. 2024a). The first two provide state-of-the-art closed-source performance, while the third allows us to verify that our findings generalize beyond proprietary systems and remain valid in the open-source regime.Evaluation.For ALFWorld dataset, task completion is evaluated by the execution environment. After a task is completed or the maximum number of execution steps is reached, the environment provides a reward of 0 or 1 to indicate whether the task has been successfully completed. For TravelPlanner, we conduct experiments on the test set in a two-stage mode. After multiple rounds of interaction to obtain the travel trajectory and the final planner, GPT-4o converts the travel plan into a specified JSON format. The converted plan is then compared with the gold standard to obtain scores for both Common Sense and Hard Constraint.Memory Storage & RetrievalProcedural knowledge is typically stored in two main formats: (1) trajectories are kept verbatim, round by round, in memory, or (2) high-level abstractions are extracted from these trajectories and then stored. Once a similar procedural memory is retrieved, it is appended to the task as part of the context, serving as prior knowledge to assist the model in completing the task.Inspired by this, we designed the following experimental conditions:    â€¢    No Memory: The model tackles the assigned task in a ReAct fashion without any external memory.    â€¢    Trajectory: We first filter the gold trajectories from the training set and store them. At inference time, the system retrieves the top-k trajectories whose query vectors are most similar to the current taskâ€™s vector, supplying them as procedural memories before execution.    â€¢    Script: The model analyzes and summarizes the gold trajectories from the training set, distilling them into abstract procedural knowledge that is provided as a prompt before each task.    â€¢    Proceduralization: This condition combines the full retrieved trajectories with the high-level script generated by the model, integrating both concrete examples and abstract guidance as the procedural memory.As shown in Table 1, all memory construction methods outperform the no-memory baseline, achieving higher scores on both datasets while also reducing the number of steps required. This indicates that procedural memory built during training is beneficial for directly applying tasks during testing. Furthermore, we observe that the approach of abstracting trajectories into scripts during training yields relatively better performance on the ALFWorld test set compared to the dev set. Conversely, trajectories that utilize complete execution traces as procedural memory achieve higher scores on the dev set, suggesting that scripts are more capable of generalizing to different test tasks, while trajectories are better suited for scenarios involving tasks similar to those already completed. By combining procedure knowledge from both methods of employing abstracted guidelines along with concrete execution trajectories, we attain the optimal performance.After converting a set of completed trajectories into procedural memory, the next critical challenge is to retrieve the most accurate and relevant procedural knowledge when a new task arrives. We have designed several different key construction methods for memory storage to facilitate subsequent vector-based matching and retrieval:    â€¢    Random Sample: Does not utilize keys for vector retrieval; instead, randomly extracts a few memories from procedural memory.    â€¢    Query: Employ query description as the key for storage, leveraging the semantic similarity of queries for retrieval.    â€¢    AveFact: We apply a large model to extract keywords from the taskâ€™s query, then computes the average similarity across matched keywords for retrieval.During the retrieval process, we evaluate the similarity by calculating the cosine similarity between their corresponding vectors. Our experiments show that these different retrieval strategies produce varying results. Specifically, compared to random sampling, employing the query based and AveFact methods for precise retrieval significantly improves performance. The query-based approach benefits from capturing semantic contexts, enabling more accurate matches. The AveFact method, by extracting key features and averaging their similarities, effectively focuses on core task elements, leading to better retrieval efficacy. Overall, our findings suggest that incorporating semantic understanding and key feature extraction in retrieval strategies substantially enhances memory access accuracy and the effectiveness of downstream task performance.Model 	Policy 	#CS â†‘ 	#HC â†‘ 	Steps â†“GPT-4o 	No Memory 	71.93 	12.88 	17.84Random Sample 	74.59 	6.72 	15.12Key=Query 	73.38 	8.95 	15.44Key=AveFact 	76.02 	8.25 	14.64Claude-3.5-sonnet 	No Memory 	63.49 	33.06 	18.84Random Sample 	63.99 	29.91 	17.93Key=Query 	64.93 	28.56 	17.60Key=AveFact 	65.76 	29.61 	17.72Qwen2.5-72b 	No Memory 	56.57 	7.34 	18.32Random Sample 	59.76 	8.43 	18.31Key=Query 	61.71 	11.97 	18.54Key=AveFact 	63.41 	12.66 	18.12Table 2: Results on Retrieve Policy on TravelPlanner.Refer to captionFigure 3: Reward gain and steps reduction vs. trajectory group index with procedural memory.Memory UpdateWhile many prior efforts have focused on developing reusable procedural knowledge, enabling models to learn from prior experiences rather than solving each test task in isolation, most existing memory update methods remain quite rudimentary. Typically, they simply append newly acquired memories to the existing storeâ€”a so-called â€mergeâ€ strategy. In this work, we explore several online memory-update mechanisms to identify which dynamic strategy delivers the best performance on our tasks. Beyond end-to-end evaluation metrics, we also analyze how both accuracy and efficiency evolve as the number of executed tasks increases, explicitly measuring the benefits conferred by our procedural memory.To facilitate systematic comparison, we designed several memory-update scenarios. In each, the agentâ€™s episodic memory is refreshed after every t test-set tasks. The specific update strategies are as follows:    â€¢    Vanilla Memory Update: After every t tasks, all trajectories from these tasks are consolidated into procedural memories and directly appended to the memory bank.    â€¢    Validation: After every t tasks, only the trajectories of successfully completed tasks are retained and converted into procedural memories for storage.    â€¢    Adjustment: When a retrieved procedural memory results in a failed execution, the erroneous trajectory is combined with the original memory and then revised in place, yielding an updated procedural memory.As depicted in Figure 3, we systematically divided the tasks within our testbed into several distinct groups, with each group comprising a diverse set of individual tasks. Upon the completion of tasks within each group, we employed the previously described strategies to construct, store, and update the procedural memory. The experimental results reveal a clear trend: as we sequentially progress through more groups and iteratively refresh the memory, all strategies contribute to improved performance on subsequent tasks. Specifically, this is reflected not only in higher overall scores but also in a reduction in the number of steps required to complete the tasks.A closer comparison of different strategies exposes significant disparities in their effectiveness. Notably, the reflexion-based update mechanism stands out as the most effective approach. By the time the final group of tasks is reached, this method delivers a substantial advantage: it surpasses the second-best strategy by an impressive margin of +0.7 points and achieves a reduction of 14 steps. These improvements underscore the value of continually updating the memory, particularly when the update is guided by an error-correction mechanism embedded in the reflexion process.Refer to captionFigure 4: (a) Transfer result of GPT-4oâ€™s procedural memory to Qwen2.5-14B-Instruct and its performance on TravelPlanner dataset.(b) The relationship between the quantity of procedural memory retrieved for GPT-4oâ€™s performance on the ALFWorld dataset.AnalysisProcedural Memory Boosts Accuracy and Cuts Trials.Figure 5 presents a case study demonstrating how Procedural Memory enhances both accuracy and efficiency. In the absence of Procedural Memory, facing a complex task that has not been performed before, there are usually two situations. In the first scenario (left), the model repeatedly attempts illegal or incorrect actions, causing the context to become increasingly complex and eventually exceeding the modelâ€™s understanding capacity. In the second scenario (middle), after multiple attempts, the model completes the task but at a cost significantly higher than the optimal path.In contrast, once Procedural Memory is available for similar tasks, the model spends less time on trial and error. For example, in the egg heating problem, Procedural Memory can indicate the approximate location of the egg, saving the aimless search. During the heating process, it provides clear guidance, ensuring that the heating actions are performed consecutively and correctly, thereby allowing the task to be completed in fewer steps.Refer to captionFigure 5: Compare trajectories with and without procedural memory, shortens the process by 9 steps and saves 685 tokens.Procedural memory exhibits transferability from strong models to weaker ones.For a procedural memory constructed from a strong model in an offline memory library, we aim to verify whether this form of procedural memory can be effectively transferred to other models, or even weaker models. This exploration underscores the significance of memory transfer, as it could potentially enhance the adaptability and efficiency of various models by leveraging the knowledge and experience encapsulated within the strong modelâ€™s memory structure. As shown in Figure 4 (b), procedural memory generated by GPT-4o was employed by Qwen2.5-14B. On the Travel Plan benchmark, the 14 billion parameter model raised its task completion rate by 5% and cut the average number of steps by 1.6. Similar gains, both in success rate and trajectory length, appeared on ALFWorld. These outcomes confirm that procedural knowledge from a stronger model can be distilled into a reusable memory bank and transferred to a smaller model with minimal overhead, giving that smaller model a clear boost in task solving ability. Moreover, by leveraging procedural memory transfer, we can rapidly migrate the experiential knowledge that one model has acquired to another, which is highly beneficial for agents as they adapt to new tasks with greater efficiency and robustness.Scaling Memory Retrieval Improves Agent Performance.While our main experiment has already demonstrated that procedural memory improves an agentâ€™s task accuracy and reduces the number of steps required, vector-based storage and retrieval confer an advantage over human procedural memory: they can be scaled both in total capacity and in the number of memories retrieved. To investigate whether an agentâ€™s performance continues to rise as the procedural-memory store and the number of retrieved memories increase, we designed a set of follow-up experiments. As showned in Figure 4 (b), as the number of retrieved procedural memories increases, the agentâ€™s performance also improves steadily, exhibiting an upward trend followed by a plateau. However, retrieving too many memories can lead to a decline in the agentâ€™s performance. This is because excessive retrieval can affect the context length and also introduce less accurate procedural memories, which can interfere with the overall effectiveness.Conclusion and Future WorkWe introduce Mâ€‹eâ€‹mp, a task-agnostic framework that elevates procedural memory to a core optimization target in LLM-based agents. By systematically studying strategies for memory construction, retrieval, and updating, Mâ€‹eâ€‹mp enables agents to distill, reuse, and refine their own past experiences across diverse, long-horizon tasks. Empirical results on housework automation and information-seeking benchmarks show that leveraging procedural memory significantly boosts task success rates and efficiency. Beyond improving individual episodes, Mâ€‹eâ€‹mp supports continual learning and robust generalization, marking a step toward self-improving, resilient agents.In our experiments, Mâ€‹eâ€‹mp has achieved promising results in both construction and retrieval. Moving forward, we plan to enhance this work in several ways. Firstly, we will develop more diverse retrieval strategies. The current approach involves constructing different keys for vector-based retrieval. However, traditional methods like BM25 could also be explored to retrieve precise memories more effectively. Secondly, in Mâ€‹eâ€‹mp, we currently rely on the standard reward signals provided by the benchmark. However, in real-world scenarios, many tasks do not have clear reward signals, making it difficult for the agent to determine whether a task has been completed successfully. In such cases, using a large language model (LLM) as a judge to assess task completion could be a viable solution. This would transform the agentâ€™s lifecycle into a continuous loop of executing tasks, self-assessing completion, building memories, and then proceeding to new tasks.References    Anthropic (2022)Anthropic. 2022. Claude 3.5 Sonnet System Card.Barres et al. (2025)Barres, V.; Dong, H.; Ray, S.; Si, X.; and Narasimhan, K. 2025. Ï„2-Bench: Evaluating Conversational Agents in a Dual-Control Environment.Chen et al. (2025)Chen, C.; Hao, X.; Liu, W.; Huang, X.; Zeng, X.; Yu, S.; Li, D.; Wang, S.; Gan, W.; Huang, Y.; et al. 2025. ACEBench: Who Wins the Match Point in Tool Usage?Chen et al. (2024)Chen, M.; Li, Y.; Yang, Y.; Yu, S.; Lin, B.; and He, X. 2024. AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning. arXiv:2405.16247.Chhikara et al. (2025)Chhikara, P.; Khant, D.; Aryan, S.; Singh, T.; and Yadav, D. 2025. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory.Cohen and Squire (1980)Cohen, N. J.; and Squire, L. R. 1980. Preserved learning and retention of pattern-analyzing skill in amnesia: Dissociation of knowing how and knowing that.Dong et al. (2025)Dong, G.; Chen, Y.; Li, X.; Jin, J.; Qian, H.; Zhu, Y.; Mao, H.; Zhou, G.; Dou, Z.; and Wen, J.-R. 2025. Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning.Fang et al. (2025)Fang, R.; Wang, X.; Liang, Y.; Qiao, S.; Wu, J.; Xi, Z.; Zhang, N.; Jiang, Y.; Xie, P.; Huang, F.; et al. 2025. SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement.Feng et al. (2025)Feng, E.; Zhou, W.; Liu, Z.; Chen, L.; Dong, Y.; Zhang, C.; Zhao, Y.; Du, D.; Hua, Z.; Xia, Y.; et al. 2025. Get Experience from Practice: LLM Agents with Record & Replay.Gupta and Cohen (2002)Gupta, P.; and Cohen, N. J. 2002. Theoretical and computational analysis of skill learning, repetition priming, and procedural memory.Hou, Tamoto, and Miyashita (2024)Hou, Y.; Tamoto, H.; and Miyashita, H. 2024. â€ my agent understands me betterâ€: Integrating dynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1â€“7.Hu et al. (2024a)Hu, M.; Chen, T.; Chen, Q.; Mu, Y.; Shao, W.; and Luo, P. 2024a. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model.Hu et al. (2024b)Hu, M.; Chen, T.; Chen, Q.; Mu, Y.; Shao, W.; and Luo, P. 2024b. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model.Ifargan et al. (2025)Ifargan, T.; Hafner, L.; Kern, M.; Alcalay, O.; and Kishony, R. 2025. Autonomous llm-driven researchâ€”from data to human-verifiable research papers.Laird (2022)Laird, J. E. 2022. Introduction to the soar cognitive architecture.Lan et al. (2025)Lan, W.; Tang, Z.; Liu, M.; Chen, Q.; Peng, W.; Chen, Y. P.; and Pan, Y. 2025. The large language models on biomedical data analysis: a survey.Li et al. (2023)Li, G.; Hammoud, H.; Itani, H.; Khizbullin, D.; and Ghanem, B. 2023. Camel: Communicative agents forâ€ mindâ€ exploration of large language model society.Liu et al. (2025a)Liu, B.; Li, X.; Zhang, J.; Wang, J.; He, T.; Hong, S.; Liu, H.; Zhang, S.; Song, K.; Zhu, K.; et al. 2025a. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems.Liu et al. (2025b)Liu, Y.; Si, C.; Narasimhan, K.; and Yao, S. 2025b. Contextual Experience Replay for Self-Improvement of Language Agents.Liu et al. (2025c)Liu, Z.; Chai, J.; Zhu, X.; Tang, S.; Ye, R.; Zhang, B.; Bai, L.; and Chen, S. 2025c. Ml-agent: Reinforcing llm agents for autonomous machine learning engineering.Lu et al. (2025)Lu, F.; Zhong, Z.; Liu, S.; Fu, C.-W.; and Jia, J. 2025. ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay.Luo et al. (2025)Luo, R.; Wang, L.; He, W.; and Xia, X. 2025. Gui-r1: A generalist r1-style vision-language action model for gui agents.Mavroudis (2024)Mavroudis, V. 2024. LangChain v0. 3.OpenAI (2022)OpenAI. 2022. GPT-4 System Card.OpenAI (2025)OpenAI. 2025. Deep Research System Card.Ou et al. (2025)Ou, Y.; Luo, Y.; Zheng, J.; Wei, L.; Qiao, S.; Zhang, J.; Zheng, D.; Chen, H.; and Zhang, N. 2025. AutoMind: Adaptive Knowledgeable Agent for Automated Data Science.Puterman (1990)Puterman, M. L. 1990. Markov decision processes.Qiao et al. (2025)Qiao, S.; Fang, R.; Qiu, Z.; Wang, X.; Zhang, N.; Jiang, Y.; Xie, P.; Huang, F.; and Chen, H. 2025. Benchmarking Agentic Workflow Generation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net.Qiao et al. (2023)Qiao, S.; Ou, Y.; Zhang, N.; Chen, X.; Yao, Y.; Deng, S.; Tan, C.; Huang, F.; and Chen, H. 2023. Reasoning with Language Model Prompting: A Survey. In Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 5368â€“5393. Association for Computational Linguistics.Qin et al. (2025)Qin, Y.; Ye, Y.; Fang, J.; Wang, H.; Liang, S.; Tian, S.; Zhang, J.; Li, J.; Li, Y.; Huang, S.; et al. 2025. Ui-tars: Pioneering automated gui interaction with native agents.Shridhar et al. (2021)Shridhar, M.; Yuan, X.; CÃ´tÃ©, M.; Bisk, Y.; Trischler, A.; and Hausknecht, M. J. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.Sumers et al. (2023)Sumers, T.; Yao, S.; Narasimhan, K.; and Griffiths, T. 2023. Cognitive architectures for language agents.Sun et al. (2024)Sun, J.; Zhang, Q.; Duan, Y.; Jiang, X.; Cheng, C.; and Xu, R. 2024. Prompt, plan, perform: Llm-based humanoid control via quantized imitation learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), 16236â€“16242. IEEE.Tan et al. (2025)Tan, X.; Li, B.; Qiu, X.; Qu, C.; Chu, W.; Xu, Y.; and Qi, Y. 2025. Meta-Agent-Workflow: Streamlining Tool Usage in LLMs through Workflow Construction, Retrieval, and Refinement. In Companion Proceedings of the ACM on Web Conference 2025, WWW â€™25, 458â€“467. New York, NY, USA: Association for Computing Machinery. ISBN 9798400713316.Tang et al. (2025)Tang, X.; Qin, T.; Peng, T.; Zhou, Z.; Shao, D.; Du, T.; Wei, X.; Xia, P.; Wu, F.; Zhu, H.; Zhang, G.; Liu, J.; Wang, X.; Hong, S.; Wu, C.; Cheng, H.; Wang, C.; and Zhou, W. 2025. Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving. arXiv:2507.06229.Wang et al. (2023)Wang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023. Voyager: An open-ended embodied agent with large language models.Wang et al. (2024a)Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024a. A survey on large language model based autonomous agents.Wang et al. (2025)Wang, Z.; Xu, H.; Wang, J.; Zhang, X.; Yan, M.; Zhang, J.; Huang, F.; and Ji, H. 2025. Mobile-agent-e: Self-evolving mobile assistant for complex tasks.Wang et al. (2024b)Wang, Z. Z.; Mao, J.; Fried, D.; and Neubig, G. 2024b. Agent workflow memory.Wu et al. (2025a)Wu, J.; Li, B.; Fang, R.; Yin, W.; Zhang, L.; Tao, Z.; Zhang, D.; Xi, Z.; Fu, G.; Jiang, Y.; et al. 2025a. WebDancer: Towards Autonomous Information Seeking Agency.Wu et al. (2025b)Wu, J.; Yin, W.; Jiang, Y.; Wang, Z.; Xi, Z.; Fang, R.; Zhang, L.; He, Y.; Zhou, D.; Xie, P.; and Huang, F. 2025b. WebWalker: Benchmarking LLMs in Web Traversal. In Che, W.; Nabende, J.; Shutova, E.; and Pilehvar, M. T., eds., Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 10290â€“10305. Vienna, Austria: Association for Computational Linguistics. ISBN 979-8-89176-251-0.x.ai (2025)x.ai. 2025. Grok 3 Beta â€” The Age of Reasoning Agents.Xi et al. (2025)Xi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.; Zhang, M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2025. The rise and potential of large language model based agents: A survey.Xie et al. (2024)Xie, J.; Zhang, K.; Chen, J.; Zhu, T.; Lou, R.; Tian, Y.; Xiao, Y.; and Su, Y. 2024. TravelPlanner: A Benchmark for Real-World Planning with Language Agents. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net.Xu et al. (2025)Xu, W.; Liang, Z.; Mei, K.; Gao, H.; Tan, J.; and Zhang, Y. 2025. A-mem: Agentic memory for llm agents.Yang et al. (2024a)Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; et al. 2024a. Qwen2. 5 technical report.Yang, Yue, and He (2023)Yang, H.; Yue, S.; and He, Y. 2023. Auto-gpt for online decision making: Benchmarks and additional opinions.Yang et al. (2024b)Yang, Y.; Zhou, T.; Li, K.; Tao, D.; Li, L.; Shen, L.; He, X.; Jiang, J.; and Shi, Y. 2024b. Embodied multi-modal agent trained by an llm from a parallel textworld. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 26275â€“26285.Yao et al. (2025)Yao, S.; Shinn, N.; Razavi, P.; and Narasimhan, K. R. 2025. Ï„-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains. In The Thirteenth International Conference on Learning Representations.Yu et al. (2025)Yu, H.; Chen, T.; Feng, J.; Chen, J.; Dai, W.; Yu, Q.; Zhang, Y.-Q.; Ma, W.-Y.; Liu, J.; Wang, M.; and Zhou, H. 2025. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent. arXiv:2507.02259.Zhang et al. (2024)Zhang, Z.; Bo, X.; Ma, C.; Li, R.; Chen, X.; Dai, Q.; Zhu, J.; Dong, Z.; and Wen, J.-R. 2024. A survey on the memory mechanism of large language model based agents.Zhao et al. (2023)Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language models.(53)Zheng, L.; Wang, R.; Wang, X.; and An, B. ???? Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. In The Twelfth International Conference on Learning Representations.Zhong et al. (2024a)Zhong, W.; Guo, L.; Gao, Q.; Ye, H.; and Wang, Y. 2024a. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 19724â€“19731.Zhong et al. (2024b)Zhong, W.; Guo, L.; Gao, Q.; Ye, H.; and Wang, Y. 2024b. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 19724â€“19731.Zhou et al. (2023)Zhou, W.; Jiang, Y. E.; Li, L.; Wu, J.; Wang, T.; Qiu, S.; Zhang, J.; Chen, J.; Wu, R.; Wang, S.; Zhu, S.; Chen, J.; Zhang, W.; Tang, X.; Zhang, N.; Chen, H.; Cui, P.; and Sachan, M. 2023. Agents: An Open-source Framework for Autonomous Language Agents. arXiv:2309.07870.Zhou et al. (2024)Zhou, W.; Ou, Y.; Ding, S.; Li, L.; Wu, J.; Wang, T.; Chen, J.; Wang, S.; Xu, X.; Zhang, N.; Chen, H.; and Jiang, Y. E. 2024. Symbolic Learning Enables Self-Evolving Agents. arXiv:2406.18532.Zhou et al. (2025)Zhou, Z.; Qu, A.; Wu, Z.; Kim, S.; Prakash, A.; Rus, D.; Zhao, J.; Low, B. K. H.; and Liang, P. P. 2025. MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents. arXiv:2506.15841."
},{
  "research_proposal": "The paper introduces a World State Model that maintains an explicit representation of the software environment's state, enabling agents to reason over both local and external state inputs (LS and ES). It evaluates the impact of integrating contextual disambiguation (CD) into this model to improve multi-application task performance.",
  "benchmark": ["AgentRewardBench", "OS-World-Full", "Prof/Office"],
  "performance_metrics": [3.5, 25.5, 24.5],
  "paper_link": "https://arxiv.org/html/2508.04700v2#S4",
  "full_text": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from ExperienceZeyi Sun1,2 â€ƒZiyu Liu1,2 â€ƒYuhang Zang2 â€ƒYuhang Cao2 â€ƒXiaoyi Dong2,3 â€ƒTong Wu3 â€ƒDahua Lin2,3 â€ƒJiaqi Wang21Shanghai Jiao Tong University â€ƒ2Shanghai Artificial Intelligence Laboratory3The Chinese University of Hong Kongszy2023@sjtu.edu.cn, wangjiaqi@pjlab.org.cnhttps://github.com/SunzeY/SEAgentAbstractRepurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agentâ€™s policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.1 Introductionâ€œA new generation of agents will acquire superhuman capabilities by learning predominantly from experience.â€ silver2025welcomeâ€” David Silver, Richard S. SuttonWith the rapid development of large vision-language models (LVLMs) touvron2023llama ; grattafiori2024llama ; bai2025qwen2 ; wang2024qwen2 ; gpt4 ; claude-3-7 ; team2023gemini , computer use agents (CUAs) claude ; operator ; qin2025uitars ; lin2024showui ; wu2024atlas have not only emerged but also demonstrated increasing practical utility. By leveraging the powerful perception and reasoning capabilities of LVLMs, these agents can interpret screenshots as visual inputs and operate computers via keyboard and mouse actions. Despite their promising capabilities, current CUAs qi2024webrl ; putta2024agent ; deng2023mind2web ; he2024webvoyager ; bai2024digirl ; lu2024gui primarily depend on costly human-curated datasets deng2023mind2web ; chen2024guicourse ; wu2024atlas ; kapoor2024omniact ; li2024effects , which are typically derived from demonstrations lu2024gui ; zhang2023you ; gur2023real ; rawles2023androidinthewild ; zhang2024agentohana or video tutorials in the wild xu2024agenttrek . However, new software continuously emerges and existing software may regularly be updated, often in the absence of annotated human data. It is both necessary and timely to enter an era that emphasizes learning from experience silver2025welcome in CUA domain. In this paper, we aim to enable CUAs to autonomously explore unfamiliar software environments and evolve into experts without relying on human supervision.Refer to captionFigure 1: SEAgent enables computer use agents self-evolving in novel environments by autonomously exploring and learning from their own experiences without human intervention. The specialist-to-generalist training strategy further enhances the development of a strong generalist agent.To address this challenge, we propose SEAgent, an agentic self-evolving framework in which Computer Use Agents (CUAs) are exposed to previously unfamiliar software environments and engage in autonomous exploration and experiential learning, as illustrated in Fig. 1. Enabling such self-evolution requires addressing two key challenges: (1) generating executable tasks within unfamiliar software environments, and (2) accurately assessing task success and precisely identifying the step at which failure occurs. To this end, we introduce a World State Model for environmental state captioning and step-wise trajectory assessment, together with a Curriculum Generator powered by a continuously updated software guidebook memory to generate increasingly diverse and challenging tasks, thereby establishing a curriculum learning paradigm. The agentâ€™s policy is optimized through experiential learning from both failures and successes, combining adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones.Given the critical role of reward accuracy, we conduct extensive evaluations and observe that existing reward models of computer use tasks fall short in terms of judgment precision and reward density. Leveraging the enhanced long-context processing capabilities of advanced LVLMs, we input the agentâ€™s full trajectory of states into the reward model and fine-tune a reward model, World State Model, using Qwen2.5-VL bai2025qwen2 , substantially narrowing the gap with commercial models such as GPT-4o gpt4 with +7.5% improvement in precision compared to baseline model in evaluating CUAsâ€™ trajectories on AgentRewardBench lu2025agentrewardbench , enable World State Model to provide high quality step level reward signals in self-evolving agentic system.Moreover, SEAgent enables agents to evolve into either single-software specialists or multi-software generalists. To overcome the limitation that directly training a generalist underperforms compared to specialists, inspired by zhang2024buildingspecializedgeneralistai , we introduce a novel specialist-to-generalist training strategy, which even surpasses the performance of individual specialists on their respective software applications.We perform extensive experiments of SEAgent built on UI-TARS qin2025uitars and evaluated on five professional software applications from OSWorld xie2024osworld . SEAgent with the specialist-to-generalist strategy significantly improves the UI-TARS qin2025uitars from 11.3% to 34.5%. Furthermore, SEAgent with the specialist-to-generalist strategy (34.5%) outperforms both specialist RL (32.2%) and generalist RL (30.6%) by a substantial margin, demonstrating the effectiveness of the specialist-to-generalist paradigm. In general, SEAgent offers a promising approach for developing more powerful and versatile computer-use agents without human involvement.2 Related WorkAgent for Computer Use.With the recent advances in LLMs and LVLMs touvron2023llama ; grattafiori2024llama ; liu2023visual ; bai2025qwen2 ; wang2024qwen2 , which enable human-level perception and reasoning capabilities, the development of agents for computer use has garnered significant attention hu2024agents ; hong2024cogagent ; cheng2024seeclick ; nguyen2024gui ; lin2024showui . These agents either rely solely on structured text inputs qi2024webrl ; nakano2021webgpt ; putta2024agent ; lai2024autowebglm ; ma2023laser or, in a more human-like manner, use multi-modal inputs such as screenshots combined with textual conditions hong2023cogagent ; lin2024showui ; wu2024atlas ; operator . Although they have been extensively studied and show strong performance on in-domain benchmarks lu2024gui ; zheng2024gpt ; liu2024visualwebbench ; li2025screenspot ; cheng2024seeclick , computer use agents still lag significantly behind human-level performance in simulated environments xie2024osworld ; rawles2024androidworld ; koh2024visualwebarena ; zhou2023webarena . This gap highlights the challenges posed by the multi-dimensional demands on LVLMs, including grounding, decision-making, and reasoning. Some approaches address this by decomposing tasks into specialized expert models gou2024navigating ; wan2024omniparser and employing agent collaboration agashe2024agent ; agashe2025agent ; liu2023bolaa ; zhang2025appagent through prompt engineering yan2023gpt ; he2024webvoyager ; zhang2024android ; wang2023voyager ; wu2024copilot . However, improvements from these training-free methods remain limited without fine-tuning. In this work, we explore the next phase of computer use agents, where a pretrained agent is fine-tuned to learn from its own experience, enabling self-evolution on novel, specialized software without human annotations.Reinforcement Learning for LLMs/LVLMs.Previous scalable training efforts for LLMs and LVLMs touvron2023llama ; grattafiori2024llama ; liu2023visual ; bai2025qwen2 ; wang2024qwen2 ; xing2025scalecap ; sun2024bootstrap3d ; sun2024xpromptuniversalincontextimage ; ding2025mm have primarily relied on supervised fine-tuning (SFT) liu2023visual ; wei2022chain . Analogous to imitation learning or behavior cloning in reinforcement learning (RL), SFT trains models to produce desired outputs based on labeled data, making it heavily dependent on high-quality human-curated procedures. Recently, DeepSeek-R1 guo2025deepseek demonstrated strong reasoning capabilities via Group Relative Policy Optimization (GRPO) shao2024deepseekmath using verifiable rewards. Earlier works ouyang2022training ; ziegler2019fine ; rafailov2023direct have also employed RL for single-turn optimization from human feedback. However, in agentic scenarios such as computer usageâ€”where feedback is sparse with reward signals often results from multi-step interactionsâ€”it becomes crucial to introduce stable, step-level reward signals. Prior RL approaches for agents bai2024digirl ; qi2024webrl ; zhou2024archer ; zhai2024fine ; carta2023grounding have fine-tuned their own critic models for advantage estimation schulman2015high , either using output reward models (ORMs) trained on labeled data or adopting Direct Preference Optimization (DPO) rafailov2023direct based on interaction data putta2024agent ; qin2025uitars . In this work, we investigate various strategies for constructing high-performing reward models for CUAs and find that full-process-based analysis yields more accurate evaluations with fine-grained reward signals compared to training dedicated critic models for advantage estimation as done in bai2024digirl ; qi2024webrl or with filtered behavior cloning pan2024autonomous ; chen2020bail .3 MethodsProblem Formulation. The objective of SEAgent is to establish a training pipeline enabling the Computer Use Agent (CUA) to autonomously explore its environment (Sec. 3.1) and progressively self-evolve on novel software applications via reinforcement learning from experience (Sec. 3.2). Specifically, the SEAgent pipeline comprises three primary components: an Actor Model Ï€ performing exploratory actions to accomplish these tasks, and a World State Model â„³sâ€‹tâ€‹aâ€‹tâ€‹e describing the current environment state and evaluating the success or failure of executed actions, and a Curriculum Generator â„³tâ€‹aâ€‹sâ€‹k that continuously proposes more diverse and challenging exploration tasks:(1) Actor Model Ï€: The policy Ï€â€‹(a|st,I) defines the probability of taking action a at time step t, conditioned on the current state st and the overall task instruction I.(2) World State Model â„³sâ€‹tâ€‹aâ€‹tâ€‹e: This component is a fine-tuned Large Vision-Language Model (LVLM) responsible for providing detailed descriptions of environment states. It also evaluates each step of the trajectory executed by the Actor Model Ï€, producing trajectory judgement ğ’¥ which indicates whether the task has been successfully completed. Joint training with state change captioning ğ’ of the software GUI has been shown to enhance judgment accuracy, as shown in Table 1.(3) Curriculum Generator â„³tâ€‹aâ€‹sâ€‹k: This component utilizes a powerful Large Language Model (LLM) to automatically generate novel exploration tasks. It also maintains and updates a software guidebook U based on the state change captioning ğ’ and the trajectory judgement ğ’¥ provided by â„³sâ€‹tâ€‹aâ€‹tâ€‹e during interactions. The gradually enriched guidebook U enables â„³tâ€‹aâ€‹sâ€‹k to progressively generate increasingly diverse and challenging tasks in a curriculum learning fashion.SEAgent can be applied to enable the self-evolution of a computer-use agent, either as a specialist for a single software or as a generalist across multiple software. However, we observe that direct training for generalist agents is suboptimal. We introduce a specialist-to-generalist training strategy, which achieves even better overall performance than training multiple generalist agents, as discussed in Sec. 3.3.3.1 Autonomous Exploration with Self-evolving CurriculumRefer to captionFigure 2: SEAgent autonomous exploration and experiential learning pipeline. Guided by tasks generated by the Curriculum Generator, the Actor Model is updated according to step-level rewards from the World State Model through verifiable reward functions tailored for different action types.Autonomous exploration is essential for enabling the Computer Use Agent (CUA) to develop proficiency in novel software applications that are previously unseen or poorly understood. This process involves addressing two key challenges: (1) generating executable tasks within unfamiliar software environments, and (2) evaluating task completion success and pinpointing the specific step at which failure occurs. To tackle these challenges, we introduce two novel components: the World State Model â„³state and the Curriculum Generator â„³task. These components jointly support a self-evolving curriculum paradigm, which facilitates the autonomous generation of increasingly diverse and challenging tasks.The self-evolving curriculum paradigm pipeline is structured into P sequential phases. Before the first phase, a set of initial tasks targeting basic GUI operations is generated (details provided in Sup. C.1). In each phase, these tasks are executed and step-wise evaluated. The resulting judgments and descriptions of the exploration trajectories are fed into the Curriculum Generator â„³task, which updates a self-maintained software guidebook U. Leveraging this updated guidebook and the current capabilities of the CUA, the generator then produces more diverse and challenging tasks for subsequent phases. The following outlines each step of the process in detail:(1) Task initiation: The initial state of the unfamiliar software is provided, typically in the form of screenshots of its basic GUI interface. The World State Model â„³state performs dense captioning of the GUI elements, including button detection and OCR-based recognition. These detailed captions are passed to the Curriculum Generator â„³task, which generates an initial set of task instructions â„0={I0(1),I0(2),â‹¯} along with an initial software guidebook U0 for the software.(2) World state judgment: In the p-th phase of Auto Exploration, the Actor Model Ï€p executes tasks based on the instructions in â„p. Each execution is evaluated by the World State Model â„³state, which provides feedback ğ’¥p={Jp(1),Jp(2),â‹¯} for each step within the operation trajectory. In addition, it generates a detailed description of GUI state changes based on captured screenshots, denoted as ğ’p.(3) Task self-evolving: Based on the outcomes ğ’¥p and ğ’p, the Curriculum Generator â„³task produces a more challenging task set â„p+1 and expands the agentâ€™s knowledge boundary by updating the software guidebook to Up+1. The detailed prompting process is illustrated in Fig. 8. This iterative update can be formalized as:	Up+1,â„p+1=â„³taskâ€‹(Up,â„p,ğ’¥p,ğ’p)		(1)Here, Up+1 serves as a more comprehensive software guidebook memory, while â„p+1 represents a more challenging task set tailored to the current capabilities of the Actor Model Ï€p. Examples of â„p are provided in Fig. 4, where the Actor Model Ï€ demonstrates curriculum learning by handling increasingly complex tasks across different phases p. Illustrations of Up across various software applications are provided in Sup. J. Comparison with previous methods murty2025nnetnavunsupervisedlearningbrowser ; murty2024bagelbootstrappingagentsguiding ; sun2024genesis on task generation are detailed in Sup.C.2(4) Autonomous RL Training: Through iterative reinforcement learning, the Actor Model Ï€p is updated based on its execution of the instruction set â„p, guided by evaluation feedback ğ’¥p and a set of action-specific verifiable functions â„›verifer. The resulting policy Ï€p+1 is then used as the actor in the subsequent phase. Further details are provided in Sec. 3.2.3.2 Reinforcement Learning from ExperienceThe World State Model â„³sâ€‹tâ€‹aâ€‹tâ€‹e provides step-level reward signals for reinforcement learning. Unlike previous reward models for CUA qi2024webrl ; bai2024digirl ; putta2024agent ; pan2024autonomous ; lu2025agentrewardbench , our â„³sâ€‹tâ€‹aâ€‹tâ€‹e model takes the entire trajectory of states and actions, â„‹={(s0,a0),(s1,a1),â€¦}, as input. It classifies each action a as either aF or aT, where aF indicates an incorrect action leading to failure or redundant loops, and aT represents a correct action that contributes to successful progression without redundancy. The curated prompt used for judgment is depicted in Fig. 7. For historical states that result in aT, we encourage CUA to reinforce these actions through verifiable rewards defined by a set of functions â„›verifer={rdâ€‹iâ€‹sâ€‹t}. Conversely, for states leading to aF, we penalize them using negative KL divergence with adversarial imitation.Adversarial Imitation for Failure Action Punishment. To explicitly encourage the policy to diverge from failure-inducing behaviors, we employ a contrastive log-ratio loss based on a reference failure action aF. This objective encourages the policy to sample actions a that minimize alignment with the failure action aF:	â„’AIâ€‹(Ï€Î¸)=ğ”¼Î½â€‹[âˆ’logâ¡Ï€Î¸â€‹(aâˆ£s,I)Ï€refâ€‹(aFâˆ£s,I)]		(2)This formulation serves as an adversarial imitation signal. By maximizing divergence from this distribution, the agent is trained to explore alternative action distributions that deviate from those leading to failure, particularly in complex GUI interaction scenarios. Notably, this loss shares a similar form with DPO rafailov2023direct but only the negative part.Verifiable Rewards for Correct Action Encouragement. To more effectively guide the policy toward correct actions aT, we adopt Reinforcement Learning with Verifiable Rewards (RLVR) guo2025deepseek ; shao2024deepseekmath , which has recently shown success in enhancing language models on tasks with objectively verifiable answers, such as mathematics shao2024deepseekmath , and more recently, counting and grounding in the vision-language domain liu2025visual ; shen2025vlm ; meng2025mm . After labeling the correct step (s,aT) using the World State Model, we apply Group Relative Policy Optimization (GRPO), computing the relative advantage of each response based on its reward:	A(i)=r(i)âˆ’meanâ€‹({r(j)}j=1G)stdâ€‹({r(j)}j=1G),i=1,â‹¯,G.		(3)As we design distinct reward signals for different action types, we define the reward function between a predicted action a and the ground-truth action aT as:	r(i)=râ€‹(a(i),aT)=ğ•€â€‹(typeâ€‹(a(i))=typeâ€‹(aT))+rdistâ€‹(a(i),aT),		(4)where ğ•€â€‹(â‹…) is the indicator function that returns 1 if the predicted action and ground-truth action are of the same type, and 0 otherwise. The distance-based reward term rdistâ€‹(a(i),aT) is defined according to the specific action type: for click actions, it is computed based on the normalized L1 distance between the clicked coordinates; for drag and select actions, it is computed using the Intersection over Union (IoU) between the predicted and ground-truth bounding boxes; and for type actions, it is determined by the character-level BLEU score between the predicted and ground-truth text. All rdist rewards are normalized to the range [0,1] to ensure consistency across different action types. A comprehensive list of rdistâ€‹(a(i),aT) definitions for various action types is provided in Tab. 8. The final loss of GRPO is directly adopted from shao2024deepseekmath :	â„’GRPOâ€‹(Ï€Î¸) 	=âˆ’ğ”¼(s,I)âˆ¼ğ’Ÿ,{a(i)}i=1Gâˆ¼Ï€ref(â‹…âˆ£s,I) 		(5)	[1Gâˆ‘i=1G1|a(i)| 	âˆ‘t=1|a(i)|{min(rt(i)(Î¸)A(i),clip(rt(i)(Î¸),1âˆ’Ïµ,1+Ïµ)A(i))âˆ’Î²DKL(i,t)(Ï€Î¸âˆ¥Ï€ref)}], 		whereri,tâ€‹(Î¸)=Ï€Î¸â€‹(a(i)|s,I)Ï€Î¸refâ€‹(a(i)|s,I)â€‹ and â€‹DKLi,tâ€‹(Ï€Î¸,Ï€ref)=Ï€refâ€‹(a(i)|s,I)Ï€Î¸â€‹(a(i)|s,I)âˆ’1âˆ’logâ¡Ï€refâ€‹(a(i)|s,I)Ï€Î¸â€‹(a(i)|s,I). 	Similar to shao2024deepseekmath ; guo2025deepseek , advantage A is weighted on the whole reasoning token logits to encourage free form thinking for performing action and planning.The final training loss is defined as a weighted combination of positive and negative action samples, i.e., correct actions aT and incorrect actions aF: â„’â€‹(Ï€â€‹(Î¸))=â„’GRPO+Î³â€‹â„’AI. We set Î³=0.2 during training, and the rationale for this choice is discussed in the ablation study presented in Sup. F.This strategy is shown to be more effective in Sec. 4.2 compared to Generalized Advantage Estimation (GAE) schulman2015high -based RL methods qi2024webrl ; bai2024digirl , as the more powerful reward model â„³sâ€‹tâ€‹aâ€‹tâ€‹e provides accurate step-level reward signals by leveraging the entire episode trajectory â„‹ from a global perspective.3.3 From Specialist to Generalist.Achieving a generalist agent capable of operating across multiple software platforms is an ambitious and valuable goal. We first attempted to train such a generalist directly using the proposed SEAgent framework across all software environments. However, this approach led to suboptimal performance compared to specialized agents, as the actor struggled to learn effectively in the multi-software environment.We thus introduce a specialist to generalist strategy, as illustrated in Fig. 1. Specifically, we first train software-specialized agents via SEAgent on individual environments, allowing each to master a specific application. These specialists are then distilled into a single generalist model through supervised fine-tuning (SFT) on synthesized successful trajectories. Finally, the generalist is refined via SEAgent on multiple software. This generalist, now equipped with better reasoning, planning abilities, and software-specific commonsense, achieves significantly improved performance, outperforming both the SEAgent via direct general RL and the performance combination of multi-specialists as in Table 2.4 Experiments4.1 Benchmark of Reward Model for computer use agent.Providing CUA agents with reliable reward signals is crucial for enabling self-evolution in agentic systems, consisting of an actor (CUA) and a judge model, especially when interacting with unfamiliar software environments. Recent work, AgentRewardBench lu2025agentrewardbench , proposes to evaluate the precision of reward models by assessing the accuracy of judge predictions on web-based tasks using trajectories from diverse agents. Building upon AgentRewardBench lu2025agentrewardbench , we further extend the evaluation beyond web tasks to a broader set of PC software environments. Specifically, we evaluate on all 339 feasible tasks from OSWorld xie2024osworld , using rule-based criteria for determining success or failure. Trajectories are sampled from UI-TARS qin2025uitars and Gemini-2.5-Pro google2025gemini25preview , and rule-based evaluation is used as ground-truth supervision. We then compute the confusion matrix by comparing the predictions of different reward models against these labels.The judge strategy in AgentRewardBench lu2025agentrewardbench relies solely on the final state and the associated action history. However, it is more natural and reliable for a judge model to consider the entire trajectory when assessing task success, rather than focusing only on the final state. For example, consider the task of booking a flight to London. A final state message such as "Your flight ticket has been successfully booked." does not confirm whether the correct date and time were selected, which can lead to compromised judgment accuracy.However, we observe that current open-sourced LVLMs do not perform well under this more holistic evaluation strategy. As shown in Fig. 3, feeding additional historical screenshots into Qwen2.5-VL bai2025qwen2 significantly degrades its Average Precision (AP), diverging notably from GPT-4o hurst2024gpt on the same curated prompt detailed in Fig.6. We attribute this performance drop to the insufficient pretraining of Qwen2.5-VL on long sequences of high-resolution screenshots, which likely pushes it toward the limits of its 32K context length.Table 1: Precision and Negative Predictive Value (NPV) on AgentReardBench lu2025agentrewardbench and OSWorld xie2024osworld with last screenshot only (LS) or entire process screenshots (ES) as input. World State Model closes the gap with commercial model supporting full process high resolution screenshots as input. The co-training with screenshot change description (CD) improves judgment precision.Model 	Input 	AgentRewardBench 	OS-World-Full 	Prof/OfficePrecision 	NPV 	Precision 	NPV 	Precision 	NPVGPT-4o hurst2024gpt 	LS 	68.1 	92.3 	46.3 	88.2 	40.5 	81.0ES 	72.1 	92.2 	74.6 	95.2 	70.4 	85.3Qwen2.5-VL-72B bai2025qwen2 	LS 	64.5 	94.2 	41.5 	86.9 	31.7 	78.7ES 	26.2 	83.0 	26.8 	83.0 	25.6 	76.6Qwen2.5-VL-7B bai2025qwen2 	LS 	64.1 	90.3 	37.3 	85.2 	31.8 	79.0ES 	25.4 	83.8 	20.0 	81.7 	23.5 	76.0World State Model (w/o CD) 	ES 	69.1 	88.5 	71.1 	88.4 	65.0 	81.1World State Model (w/ CD) 	ES 	71.6 	91.2 	73.9 	90.5 	69.3 	82.0To address this issue, we propose a distilled model based on Qwen2.5-VL-7B, referred to as World State Model, which conducts step-by-step screenshot analysis to produce final judgments. The training process for World State Model is detailed in Sup. A.2, using a dataset of 0.86K GPT-4o hurst2024gpt generated evaluations on trajectories with dense GUI change descriptions, exclusively from Chrome within the OSWorld xie2024osworld environment. Despite being trained solely on Chrome data, World State Model exhibits strong generalization to both other professional software in OSWorld and the external AgentRewardBench lu2025agentrewardbench benchmark. This demonstrates that the model learns transferable judgment patterns rather than overfitting to the specifics of a single application, thanks to the diversity and quality of step-level annotations in the training data.Refer to captionFigure 3: The Average Precision on AgentRewardBench lu2025agentrewardbench , where GUI-Judge exhibits an improvement in AP as the number of input middle states increases, showing a similar trend to that of the closed sourced GPT-4o hurst2024gpt when compared with its base model.We evaluate World State Model and our full-process screenshot-conditioned strategy on AgentRewardBench lu2025agentrewardbench , as well as on agent trajectories from OSWorld xie2024osworld . As shown in Tab. 1 and further analyzed in Fig. 3, World State Model achieves state-of-the-art performance among open-sourced models, significantly narrowing the gap with GPT-4o hurst2024gpt . More importantly, it exhibits a similar performance trend to GPT-4o when conditioned on historical screenshots. Despite being trained on a relatively small dataset, World State Model is explicitly encouraged to capture the sequential dependencies among historical screenshots and to perform step-by-step reasoning for final judgment. Serving as our foundation reward model, World State Model provides reliable, step-level reward signals that support downstream policy learning. In line with our agentic system designâ€”which emphasizes the evolution of the actor agent with full open-sourced modelsâ€”we intentionally avoid relying on GPT-4o hurst2024gpt API calls for judgment during training and inference (also due to inefficiency). More details of World State Model is supplied in Sup.A.4.2 Self evolution of GUI AgentsTable 2: Success Rate (SR) on OSWorld xie2024osworld . SEAgent demonstrates strong performance after reinforcement learning from experience. In addition to evolving on separate software, a new General Model achieves better performance after another iteration of SEAgent. *Indicates specialist agents trained separately for each software with ensembled results. All results are averaged over three runs.Model 	VScode 	GIMP 	Impress 	VLC 	Writer 	OverallHuman Performance 	73.9 	73.1 	80.9 	70.6 	73.9 	74.5GPT-4o hurst2024gpt 	4.35 	3.85 	6.77 	16.1 	4.35 	7.08GPT-4V gpt4 	0.00 	7.69 	2.52 	18.3 	4.35 	6.59Gemini-Pro-1.5 team2023gemini 	0.00 	11.5 	13.2 	6.53 	8.71 	7.99Claude3.7 Sonnet anthropic2025claude37systemcard 	18.8 	24.4 	10.6 	27.5 	17.4 	19.7Gemini-Pro-2.5 google2025gemini25preview 	21.7 	26.9 	9.92 	25.5 	24.6 	21.7UI-TARS-7B-DPO lu2024gui 	13.0 	23.1 	4.26 	11.8 	4.35 	11.3UI-TARS-72B-DPO lu2024gui 	18.8 	25.6 	6.38 	15.7 	8.70 	15.0DigiRL bai2024digirl (Specialized RL)* 	21.7 	32.1 	12.8 	23.5 	18.8 	21.8WebRL qi2024webrl (Specialized RL)* 	27.5 	29.5 	10.6 	25.5 	15.9 	21.8SEAgent (Specialized RL)* 	37.7 	38.5 	22.0 	33.3 	29.0 	32.2DigiRL bai2024digirl (General RL) 	21.7 	35.9 	12.1 	19.6 	15.9 	21.0WebRL qi2024webrl (General RL) 	20.3 	32.5 	9.93 	21.6 	14.5 	19.6SEAgent (General RL) 	36.2 	39.7 	19.9 	31.4 	26.1 	30.6SEAgent (General SFT) 	30.4 	37.2 	18.4 	31.9 	20.3 	27.9SEAgent (Specialist-to-Generalist) 	40.5 	42.3 	22.7 	35.3 	31.8 	34.5Refer to captionFigure 4: Self-evolved task instructions and success rate (SR) curves across different software. Tasks are progressively upgraded by the Curriculum Generator without human intervention, based on the evolving capabilities of the Actor Model at different training phases.Models Before Self-Evolution.Our self-evolving system is initialized with three locally deployed models: UI-TARS-7B-DPO qin2025uitars as the Actor Model, World State Model as the step-level reward model, and Qwen2.5-72B yang2024qwen2 as the Curriculum Generator for task evolution with software guidebook memory. We conduct experiments on five professional and office-related software applications from OSWorld xie2024osworld . As shown in Tab. 2, the initial actor agent demonstrates limited performance on these software environments, achieving an average success rate of 11.3% only.Evolution Process Details.At beginning, we provide World State Model with the initial GUI state of the novel software. The Curriculum Generator then generates the first software guidebook and a set of basic tasks (illustrated in Fig.5). This yields an initial instruction set â„0, averaging 150.2 instructions, which are executed by the Actor Model. The resulting trajectories are evaluated by World State Model and parsed into an average of 1361.5 multi-turn conversation pairs (detailed statistics are in Sup.H). We then perform reinforcement fine-tuning (RFT) following the methodology described in Sec. 3.2. Training is conducted for 1k iterations on 8 NVIDIA A100 80GB GPUs, with G=8, a batch size of 16, and a learning rate of 2Ã—10âˆ’5, scheduled via cosine decay. This evolution process is repeated iteratively for three phases using the same training configuration.Specialist Evaluation.For a fair comparison with previous reinforcement learning methods bai2024digirl ; qi2024webrl , we adapt their training strategies to the UI-TARS qin2025uitars model. Specifically, we initialize the actor agent from UI-TARS-7B-DPO and, instead of providing step-level reward signals, We evaluate its executed trajectories with binary success or failure outcomes using World State Model. A separate critic model is also initialized from UI-TARS-7B-DPO, with additional random initialized MLP layers taking the LLMâ€™s hidden states as input to regress value predictions. This critic is trained to perform advantage estimation based on Generalized Advantage Estimation (GAE) schulman2015high . The loss functions follow the same configurations as in bai2024digirl ; qi2024webrl . Both the critic and the actor agent are trained iteratively using the same phased reinforcement fine-tuning (RFT) process, where the Curriculum Generator continually generates new curriculum-style tasks.As shown in Fig. 4 and Tab. 2, we train separate actor agents for five different software applications. Our approach, denoted as SEAgent (Specialist), achieves strong performance compared to previous reinforcement learning methods such as DigiRL bai2024digirl and WebRL qi2024webrl . We attribute this improvement to the use of World State Model, which provides fine-grained, step-level reward signals derived from a comprehensive understanding of the full history of states and actions. This contrasts with previous approaches that rely on separate critic modelsâ€”typically initialized from the actor itselfâ€”to estimate advantages from sparse, final success/failure signals. Furthermore, the curriculum of task instructions generated by the Curriculum Generator, as illustrated in Fig. 4, validates the effectiveness of our autonomous learning framework. These tasks progress from simple to complex based on the actorâ€™s evolving capabilities, enabling it to gradually specialize in each target software environment. Based on the observed evolution curves, we set the number of training phases to three, as performance gains saturate beyond that point.From Specialist to Generalist.After training five strong software specialists, we pursue generalization using the methodology described in Sec. 3.3. Specifically, we collect task instructions generated during each specialistâ€™s training phase and use them to prompt the respective specialists for execution. A total of 3.5K successful trajectories, along with their corresponding reasoning traces, are distilled into a new base model (UI-TARS-7B qin2025uitars ) via supervised fine-tuning (SFT). This distilled model is then further optimized through reinforcement learning (RL) across all five software environments.As shown in Tab. 2, the resulting generalist model surpasses the performance of the individual specialist ensemble, demonstrating the effectiveness of a specialization-first strategy for achieving generalization. By learning from a broad range of software tasks, the generalist improves its reasoning and decision-making capabilities, acquiring transferable commonsense knowledge across domains.Table 3: Ablation of different configurations and their corresponding VScode success rates on OSWorld xie2024osworld . Using World State Model as the reward model yields significant performance gains. We further compare different training strategies including supervised fine-tuning (behavior cloning), GRPO, and Adversarial Imitation (AI).Qwen2.5VL-72B 	World State Model 	SFT (BC) 	GRPO 	AI 	VScode SR					13.0âœ“ 		âœ“ 			10.1âœ“ 			âœ“ 		11.6	âœ“ 	âœ“ 			23.2	âœ“ 	âœ“ 		âœ“ 	30.4	âœ“ 		âœ“ 		34.8	âœ“ 		âœ“ 	âœ“ 	37.7Ablation Study of Specialist Training.In Tab. 3, we present an ablation study on the effectiveness of various components in our training pipeline, using the success rate on VSCode from OSWorld xie2024osworld as the evaluation metric. First, we ablate the use of the World State Model for reward signal generation. Its high precision in judging the success or failure of the actor agentâ€™s actionsâ€”compared to using a base modelâ€”is shown to be essential for effective self-evolution. In addition to reward quality, reinforcement fine-tuning (RFT) also proves critical. Compared to direct supervised fine-tuning (behavior cloning), RFT encourages more diverse and exploratory reasoning patterns under verifiable rewards, enabling more generalized task planning. Finally, incorporating adversarial imitation to penalize critical failure-inducing actions allows the CUA to learn from its mistakes, yielding additional performance gains. This highlights the importance of learning not only from successful behaviors but also from failure signals.5 ConclusionIn this work, we introduce SEAgent, an autonomous Computer Use Agent (CUA) exploration system that learns from its own experience on specific software. Powered by a robust World State Model that provides step-level reward signals, and a carefully designed reinforcement learning framework that encourages free-form reasoning through trial and error, the CUA is able to evolve into a specialist for individual software platforms. Furthermore, a specialist-to-generalist training strategy enables the development of a strong generalist agent capable of operating across multiple software environments. Given that computer software constitutes a highly regularized virtual world, we believe this work can inspire future research on agentic systems in both gaming and real world embodied environments.Limitations and future work. While promising, our work still has several unresolved limitations. Firstly, our self evolving agent system is bounded by GUI-Judge to provide reliable reward signal instead of real signal from environment. As its still challenging to learning from sparse reward signal in complex environment. Secondly, though we tested on relatively complex and novel software like libreoffice-tools and GIMP. The task is still relatively simple as it only takes a human expert less than 20 step to accomplish. How to adapt the system to achieve hours-long workflow in even more challenging software used by real human expert are thus interesting future directions.References    (1)Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024.(2)Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025.(3)Anthropic. Claude computer use. 2024.(4)Anthropic. Claude 3.7 sonnet system card. https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf, 2025.(5)Anthropic. Claudeâ€™s extended thinking. 2025.(6)Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:12461â€“12495, 2024.(7)Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.(8)Thomas Carta, ClÃ©ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 3676â€“3713. PMLR, 2023.(9)Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024.(10)Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:18353â€“18363, 2020.(11)Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024.(12)Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36:28091â€“28114, 2023.(13)Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Mm-ifengine: Towards multimodal instruction following. arXiv preprint arXiv:2504.07957, 2025.(14)Google DeepMind. Gemini 2.5 Pro Preview (03-25). https://deepmind.google/technologies/gemini, 2025.(15)Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024.(16)Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.(17)Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.(18)Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.(19)Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024.(20)Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281â€“14290, 2024.(21)Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for GUI agents. CoRR, abs/2312.08914, 2023.(22)Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: A survey on mllm-based agents for general computing devices use, 2024.(23)Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.(24)Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161â€“178. Springer, 2024.(25)Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.(26)Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: A large language model-based web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5295â€“5306, 2024.(27)Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025.(28)Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pages arXivâ€“2406, 2024.(29)Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024.(30)Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892â€“34916, 2023.(31)Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024.(32)Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023.(33)Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025.(34)Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024.(35)Xing Han LÃ¹, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina StaÅ„czak, Peter Shaw, Christopher J Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. arXiv preprint arXiv:2504.08942, 2025.(36)Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023.(37)Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025.(38)Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Bootstrapping agents by guiding exploration with language, 2024.(39)Shikhar Murty, Hao Zhu, Dzmitry Bahdanau, and Christopher D. Manning. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild, 2025.(40)Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.(41)Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: A survey. arXiv preprint arXiv:2412.13501, 2024.(42)OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.(43)OpenAI. Operator. 2025.(44)Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730â€“27744, 2022.(45)Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.(46)Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024.(47)Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024.(48)Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. UI-TARS: pioneering automated GUI interaction with native agents. CoRR, abs/2501.12326, 2025.(49)Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:53728â€“53741, 2023.(50)Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024.(51)Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:59708â€“59728, 2023.(52)John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.(53)Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.(54)Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025.(55)David Silver and Richard S Sutton. Welcome to the era of experience. Preprint of a chapter to appear in Designing an Intelligence, edited by George Konidaris, MIT Press (forthcoming), 2025.(56)Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024.(57)Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows. arXiv preprint arXiv:2505.19897, 2025.(58)Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. X-prompt: Towards universal in-context image generation in auto-regressive vision language foundation models, 2024.(59)Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Bootstrap3d: Improving 3d content creation with synthetic data. arXiv e-prints, pages arXivâ€“2406, 2024.(60)Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.(61)Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.(62)Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15641â€“15653, 2024.(63)Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.(64)Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language modelâ€™s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.(65)Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824â€“24837, 2022.(66)Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024.(67)Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024.(68)Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040â€“52094, 2024.(69)Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, et al. Scalecap: Inference-time scalable image captioning via dual-modality debiasing. arXiv preprint arXiv:2506.19848, 2025.(70)Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605, 2024.(71)An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.(72)An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.(73)Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935â€“110971, 2024.(74)Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 1â€“20, 2025.(75)Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang, Zuxin Liu, Liangwei Yang, et al. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024.(76)Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024.(77)Kaiyan Zhang, Biqing Qi, and Bowen Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion, 2024.(78)Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.(79)Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.(80)Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.(81)Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.(82)    Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.Appendix A World State ModelThe World State Model (WSM) is a central component of SEAgent, responsible for understanding visual state changes and evaluating the effectiveness of the agentâ€™s actions.A.1 Model Architecture and OperationThe WSM is built upon the Qwen2.5-VL-7B vision-language model. It operates in two distinct modes, each with a specific input-output structure to perform different tasks:    1.    Trajectory Judgment:        Input:        A sequence of screenshot images captured during an episode.        Output:        Short captions for each screenshot, the reasoning process for the judgment, and a structured judgment dictionary (containing fields such as Correctness, Redundant, and First Error Step, as detailed in Figure 7 of the supplementary material).    2.    State Change Description:        Input:        Two screenshot images, one from before and one after a single action was executed.        Output:        A detailed description of the visual differences between the two images.A.2 Fine-Tuning Dataset and ProcessTo equip the WSM with these capabilities, a specialized dataset was constructed for fine-tuning.Data ConstructionThe data construction process is as follows:    1.    Trajectory Sampling: A Computer Using Agent (CUA), powered by UI-TARS and Gemini-2.5-Pro, was used to sample trajectories from 43 feasible tasks in Google Chrome within the OSWorld benchmark. These trajectories were saved as screenshot sequences.    2.    GPT-4o Annotation: Using the prompts detailed in Figures 6 and 7 of the supplementary material, GPT-4o was employed to annotate the sampled trajectories, generating judgments and screenshot captions. Only samples where the judgment matched the ground truth from OSWorld evaluation protocols were retained, resulting in 860 high-quality annotated trajectories.    3.    Change Description Data: An additional 1,000 pairs of (before action, after action) screenshots were sampled. GPT-4o was used to generate detailed descriptions of the differences, creating a 1,000-sample Change Description (CD) dataset.Fine-Tuning ProcessThe fine-tuning was performed using the Llama-Factory framework on 8 NVIDIA A100 (80G) GPUs for 2,000 iterations. A learning rate of 2Ã—10âˆ’5 was used, and LoRA (rank=128) was employed for parameter-efficient fine-tuning. The 860 annotated trajectories serve as the core training data for teaching the model trajectory judgment, captioning, and reasoning. The 1,000-sample CD dataset acts as auxiliary data, specifically to encourage the model to focus on fine-grained visual differences, which enhances its overall state understanding. As shown in Table 1 of the main paper, incorporating CD data significantly boosts judgment performance. The two datasets were combined for training without any special re-weighting.A.3 Reward Generation from Trajectory AnalysisThe trajectory judgment capability of the WSM is the core source of the reward signal for reinforcement learning. After an agent executes a full trajectory â„‹={s0,a0,s1,a1,â€¦,sfinal}, the WSM analyzes it and outputs a structured judgment. Based on this output, actions within the trajectory are dynamically labeled as either positive actions (aT) or failure actions (aF):    â€¢    Fully Successful Trajectory: If Correctness is â€˜Trueâ€˜ and there are no Redundant steps, all actions a in the trajectory are labeled as aT.    â€¢    Successful but Inefficient Trajectory: If Correctness is â€˜Trueâ€˜ but Redundant steps begin at step k, all actions prior to step k are labeled as aT.    â€¢    Failed Trajectory: If Correctness is â€˜Falseâ€˜ and the First Error Step is e, all actions prior to step e are labeled as aT, while the erroneous action ae is labeled as aF.These dynamically labeled aT and aF actions constitute the reward signals for the RL pipeline. During training, the actor predicts an action at based on the history {a0,s0,â€¦,st} and uses these labels to calculate rewards.Appendix B Curriculum GeneratorThe Curriculum Generator is designed to dynamically produce tasks of increasing difficulty and diversity, guiding the agent through a systematic exploration of the softwareâ€™s capabilities.B.1 Task Generation MechanismThe workflow of the Curriculum Generator is detailed in the pseudocode in our supplementary material. Its core idea is to leverage the WSMâ€™s analysis of completed tasks to generate new ones. The process, illustrated by the "add a rectangle" example from Figure 5, involves three main steps:    1.    Analysis and Feedback: The agent successfully completes an initial task, "add a rectangle." The WSM analyzes the execution trajectory and extracts two key pieces of information: a task evaluation (Exam) and a list of observed state changes (CD_list).        CD_list: {"add a rectangle": ["The Edit bar is expandedâ€¦", "The cursor has changed into a crossâ€¦", "A blue box appears on the screen with side bars showing properties such as fill, line, color, width, transparency, and corner styleâ€¦"], â€¦}        Exam: [{"task": "add a rectangle", "status": "success"}, â€¦]     2.    Knowledge Integration and Task Generation: The CD_list and Exam are fed into the Curriculum Generator. It distills new knowledge, such as "properties of a rectangle," and integrates it into its internal Software guidebook. Based on this new knowledge, it generates more challenging tasks like "Add a green rectangle" or "Add a red rectangle with 50% transparency," which are then added to the task buffer.    3.    Iterative Learning: In the next RL phase, the agent samples from this updated, more challenging task buffer. The continuously enriched Software guidebook acts as the systemâ€™s long-term memory, driving the Curriculum Generator to propose increasingly sophisticated and unexplored tasks in subsequent rounds, thereby guiding the agent toward mastery.Appendix C Details of Curriculum Generator.C.1 Exemplar Case during Task Evolution.Refer to captionFigure 5: SEAgent autonomous exploration pipeline. The agent (policy model) and World State Model iteratively generate new task and perform RL to become a specialist in novel software.We provide an exemplar case of our task evolution pipeline in Fig. 5, demonstrated using LibreOffice Impress. Initially, the World State Model parses a screenshot of the Impress interface into detailed captions describing the layout and individual buttons. The Task Generator then produces an initial task set, â„0={I0(1),I0(2),â€¦}, and summarizes the initial software guidance memory U0. The initial agent executes tasks in â„0, such as â€œAdd a Rectangle,â€ while the World State Model evaluates these actions, providing judgments and detailed descriptions of resulting changes. As shown in the Auto-Exploration stage, this includes generating captions for newly appeared property panels and assessing execution success. The Task Generator incorporates feedback on execution success and newly revealed properties (e.g., transparency) to evolve new tasks, such as â€œDraw a green rectangle with 50% transparency.â€ This process iteratively improves through reinforcement learning, enabling continuous task evolution and agent self-improvement.C.2 Comparative Analysis of Instruction Generation Strategies.To validate the effectiveness of our Curriculum Generator, we conducted a comparative analysis against state-of-the-art instruction generation methods, namely those from NNetNav [39] and WebRL [47].Experimental SetupWe adapted the official code and prompts from these prior works from web environments to general software applications. To ensure a fair comparison of the curriculum quality, for each strategy, we employed two leading LLMs: the open-source Qwen2.5-72B [7] and the proprietary Gemini-2.5-Pro [14]. The tasks generated by each strategy were used to train an RL agent (using GRPO only), with reward signals uniformly provided by our fine-tuned WSM. The evaluation was performed on two applications: VSCode from OSWorld (a standard software) and Celestia from ScienceBoard [sun2025scienceboardevaluatingmultimodalautonomous] (a more challenging, out-of-domain scientific application). The primary metric was the task success rate.Results and DiscussionThe results are presented in Table 4.Table 4: Success rate (%) comparison of different task generation strategies on two software applications.Task Generation Strategy 	LLM 	VSCode 	CelestiaWebRL 	Qwen2.5-72B 	27.5 	0.00WebRL 	Gemini2.5-Pro-thinking 	36.2 	3.03NNetNav 	Qwen2.5-72B 	34.6 	0.00NNetNav 	Gemini2.5-Pro-thinking 	43.6 	5.05Curriculum Generator (Ours) 	Qwen2.5-72B 	37.7 	9.09Curriculum Generator (Ours) 	Gemini2.5-Pro-thinking 	42.3 	12.12As shown, the reverse instruction generation strategy from NNetNav [39] is highly effective on the in-domain application (VSCode), demonstrating high data generation efficiency by producing successful trajectories. However, a critical trade-off was observed: this approach tends to generate many similar tasks, limiting its ability to explore the full breadth of the softwareâ€™s functionalities. This limitation becomes more pronounced when the task generator is unfamiliar with the target software, as seen in the OOD Celestia environment.In contrast, our guidebook-based method, while having a lower initial data generation efficiency, excels at systematic exploration. It builds structured knowledge of the software from scratch, making it more robust for tackling novel applications. This is evidenced by its superior performance on the more challenging Celestia software.We conclude that these two strategies are complementary. Reverse instruction generation can efficiently exploit known functionalities, while our guidebook-based method can systematically explore new ones and help the task generator build a more comprehensive understanding of the target software. A hybrid approach combining both strategies is a promising direction for future work.Appendix D Test on TARS-1.5Our work focuses on enabling agents to adapt to out-of-domain (OOD) and novel software where human-labeled data is not available. To test this, we applied our SEAgent pipeline to the UI-TARS-1.5 [48] model on two distinct benchmarks. On OSWorld [68], we observed moderate performance gains. We hypothesize this is because UI-TARS-1.5â€™s training data already targeted OSWorld, making it a familiar, in-domain environment for the base model. However, on the ScienceBoard [57] benchmarkâ€”a suite of scientific applications that are truly novel to UI-TARS-1.5â€”our pipeline delivers significant and substantial improvements. This strongly validates our core claim: SEAgent is most impactful when performing self-evolution learning on truly OOD software. We excluded two of the six ScienceBoard applicationsâ€”Lean and TeXâ€”as they are primarily text- and code-based software for mathematics and typesetting, which are not suitable for evaluating a GUI-centric agent like UI-TARS.Table 5: Performance comparison on OSWorld and ScienceBoard benchmarks. Scores represent success rates (%).	OSWorld 	ScienceBoardModel 	LibreOfficeImpress	LibreOfficeWriter	GIMP 	ChamerX 	GrassGIS 	KAlgebra 	CelestiaUI-TARS-1.5-7B-DPO 	19.15 	33.04 	51.54 	12.41 	0.00 	11.61 	4.85UI-TARS-1.5-7B-DPO+SEAgent 	23.83 	35.65 	56.92 	23.45 	10.59 	21.29 	11.52Appendix E Sensitivity Analysis on Key HyperparametersWe conducted a sensitivity analysis on key hyperparameters to evaluate their impact on the SEAgent pipeline. For model sampling, we set the temperature t=0 for better reproducibility. We analyze two specific parameters: the number of generated tasks and the number of change descriptions. The results are presented in Table 6 and discussed below.Table 6: Sensitivity analysis for key hyperparameters in the SEAgent pipeline, evaluated on VSCode. The metric is Success Rate (%).# Tasks Generated 	VScode SR 	# Change Descriptions 	VScode SR30 	31.88 	30 	33.3350 	36.23 	50 	37.68100 	37.68 	100 	37.68200 	37.68 	200 	34.78Number of Generated TasksThis parameter controls the breadth of exploration in each learning cycle. As shown in our analysis, performance improves as more diverse tasks are generated, eventually plateauing around 100 tasks.Number of Change DescriptionsThis parameter controls how much new information the generator receives to update its "software guidebook." We found a clear trade-off: A sufficient number of descriptions (50â€“100) is essential for the generator to learn about new UI functionalities and create meaningful, unexplored tasks. However, providing too many descriptions (e.g., 200) creates an overly long context for the LLM, which degrades the quality of task generation and hurts final performance.Appendix F Ablation on the Loss Balance Factor.In Sec.3.2, we use Î³ to balance the ratio of two loss item: adversarial imitation that learn from error and GRPO that learn to achieve success. We ablate the choice of Î³ in Tab.7, according to which we set Î³=0.2 in main experiments.Î³ 	0.0 	0.1 	0.2 	0.3 	0.5 	0.8Success Rate (%) 	34.8 	36.2 	37.7 	31.9 	26.1 	23.1Table 7: VScode Success Rate on OSWorld [68] under different loss balance factor Î³ values.Appendix G Reward Function for Different Actions.Action Type 	Description 	Distance-based Rewardclick, left_single, right_single, hover 	Click or hover on a location 	Normalized L1 distance between predicted and ground-truth coordinatesleft_double, double_click 	Double click on a region 	Normalized L1 distance between clicked coordinatesdrag, select 	Drag from start box to end box 	Intersection over Union (IoU) between predicted and ground-truth boxestype 	Type textual input 	Character-level BLEU score between predicted and ground-truth texthotkey 	Press multiple keys at once 	Character-level BLEU score between predicted and ground-truth key combinationspress 	Press a single key 	Character-level BLEU score between predicted and ground-truth keyscroll 	Scroll in a certain direction 	Character-level BLEU score between predicted and ground-truth directionmove_mouse 	Move mouse to a specific location 	Normalized L1 distance between predicted and ground-truth coordinateshighlight 	Highlight a rectangular UI region 	IoU between predicted and ground-truth regioncopy, paste 	Clipboard operations 	BLEU score between copied/pasted contentwait 	Explicit wait command 	Fixed reward + 1finished, finish_task 	Finish current task/trajectory 	Fixed reward + 1Table 8: Reward computation for each action type in GUI agentAppendix H Data Statistics during Iterative Reinforcement Learning.	Phase0 	Phase1 	Phase2 	Phase3VSCode 	112/39 	282/83 	161/34 	98/55GIMP 	104/51 	309/90 	183/50 	95/52Impress 	102/44 	290/92 	185/61 	87/51VLC 	85/29 	114/41 	160/48 	53/27Writer 	123/62 	278/101 	201/69 	101/43Table 9: Number of episode (Success/Failure) across four phases for different software tools during self-evolution. Each episode contains 8.8 multi-turn conversions in average.Appendix I Detailed Prompt Templates.For evaluation on AgentRewardBench [35], we use their official template for final state screenshot only testing and modified prompt in Fig.6 for entire process (or sampled middle screenshots) testing.For evaluation on OSWorld Sampled trajectories, we use prompt in Fig.7 to prompt GPT-4o to provide step level judges, the sampled judges on Chrome in OSWorld [68] serves as training data of GUI-Judge. This template is also used in training GUI-Judge and at inference time in autonomous exploration stage.For navigator, we use prompt template in Fig.8, which takes previous software usage manual and the performance of actor agent evaluated by judge (Empty if in initial phase.) as well as detailed exploration caption as input and output the updated usage manual as well as new task for agent to execute.Refer to captionFigure 6: Prompt Template of GUI-Judge for web agent trajectories evaluations with history screenshots as input, its difference with default prompt of AgentRewardBench [35] is highlighted in bold.Refer to captionFigure 7: Prompt Template of GUI-Judge for OSWorld [68] trajectories, which prompts judge model to provide step level reward signal.Refer to captionFigure 8: Prompt Template for task buffer update, which generates new tasks in a curriculum manner and update software documents. The new tasks are used for actor to perform next phase of RL.Appendix J Self documented usage manual on different software during exploration.In Fig.9 Fig.11, Fig.10, Fig.12, we demonstrate the self-documented usage manuals of the navigator (Qwen2.5-72B [72]) in the exploration and learning system introduced in Sec.3.1.Appendix K Broader ImpactsPotential positive societal impacts: SEAgent introduces a self-evolving paradigm for Computer Use Agents (CUAs), enabling them to autonomously learn and adapt to previously unseen software without human supervision. This significantly reduces the need for extensive manual data annotation and domain-specific customization, allowing intelligent agents to assist users across a wide range of applicationsâ€”including productivity tools, multimedia editing, and educational software. By automating repetitive tasks and providing guidance in complex software environments, SEAgent holds promise for improving accessibility, enhancing digital literacy, and reducing cognitive workload in both professional and everyday settings.Potential negative societal impacts: The capability of SEAgent to autonomously explore and operate complex software also introduces risks of misuse. Malicious actors might repurpose SEAgent for unauthorized software automation, such as automating account creation, spamming interfaces, or conducting surveillance via GUI interactions. In addition, as the agent learns from its own experience, there exists a risk that the agent may inadvertently inherit or amplify software-specific biases, potentially leading to unfair or inappropriate behaviors in sensitive applications (e.g., finance, legal automation). Mitigation strategies include controlled release of models, behavior filters during deployment, and incorporating safeguards in the World State Model to detect and prevent unintended or adversarial behavior.Refer to captionFigure 9: Automatically generated usage manual during self exploration on VScode.Refer to captionFigure 10: Automatically generated usage manual during self exploration on GIMP.Refer to captionFigure 11: Automatically generated usage manual during self exploration on LibreOffice_Impress.Refer to captionFigure 12: Automatically generated usage manual during self exploration on LibreOffice_Writer.Algorithm 1 SEAgent Specialized Self-Evolution Training Loop1:Input: Initial policy Ï€0, World State Model â„³state, Curriculum Generator â„³task, Initial GUI state S02:1. Task Initialization3:ğ’0â†CaptionGUIâ€‹(S0) âŠ³ Parse initial GUI layout (menu bar, buttons, etc.)4:â„0,U0â†â„³taskâ€‹(âˆ…,âˆ…,âˆ…,ğ’0) âŠ³ Generate basic initial tasks and usage guide  5:for p=0 to Pâˆ’1 do âŠ³ 2. Self-Evolution Phase Loop6:â€ƒâ€‚2.1 Autonomous Exploration7:â€ƒâ€‚ğ’Ÿtrajâ†âˆ…8:â€ƒâ€‚for all Iâˆˆâ„p do9:â€ƒâ€ƒâ€ƒÏ„â†ExecuteInstructionâ€‹(Ï€p,I) âŠ³ Actor executes task in the virtual environment10:â€ƒâ€ƒâ€ƒ2.2 Effect Evaluation11:â€ƒâ€ƒâ€ƒğ’¥I,ğ’Iâ†â„³stateâ€‹(Ï„) âŠ³ Step-level trajectory judgment and new state captions12:â€ƒâ€ƒâ€ƒğ’Ÿtrajâ†ğ’Ÿtrajâˆª{(Ï„,ğ’¥I,ğ’I)} âŠ³ ğ’¥I: a sequence of per-step feedback labels (aT or aF)13:â€ƒâ€‚end for 14:â€ƒâ€‚2.3 Policy Update (RFT)15:â€ƒâ€‚Split ğ’Ÿtraj into:16:â€ƒâ€‚â€ƒğ’Ÿpos: steps labeled as positive aT17:â€ƒâ€‚â€ƒğ’Ÿneg: steps labeled as negative aF18:â€ƒâ€‚Compute GRPO loss on ğ’Ÿpos:19:â€ƒâ€‚â€ƒrâ€‹(a,aT)=ğ•€â€‹[typeâ€‹(a)=typeâ€‹(aT)]+rdistâ€‹(a,aT)20:â€ƒâ€‚Compute Adversarial Imitation loss on ğ’Ÿneg:21:â€ƒâ€‚â€ƒâ„’AI=âˆ’logâ¡Ï€Î¸â€‹(aâˆ£s,I)Ï€refâ€‹(aFâˆ£s,I)22:â€ƒâ€‚Total loss: â„’total=â„’GRPO+Î³â€‹â„’AI23:â€ƒâ€‚Ï€p+1â†Updateâ€‹(Ï€p,â„’total)  24:â€ƒâ€‚2.4 Task Update25:â€ƒâ€‚â„p+1,Up+1â†â„³taskâ€‹(Up,â„p,{ğ’¥I},{ğ’I}) âŠ³ Generate more complex tasks based on new software knowledge and performance feedback26:end for 27:Output: Specialized agent policy Ï€P after P stages of self-evolutionAppendix L SEAgent Self-Evolution AlgorithmAlgorithm 1 presents the core self-evolution training loop of SEAgent in a specialized software environment. The procedure is divided into four major stages:(1) Task Initialization. Given the initial GUI state of a target software application, the World State Model performs dense captioning to extract structural semantics (e.g., menu bar, buttons), which is used by the Curriculum Generator to create an initial set of executable tasks and an editable software guidebook.(2) Autonomous Exploration and Effect Evaluation. The agent explores each task via its current policy. The World State Model then performs step-level trajectory analysis, assigning each action a feedback labelâ€”either correct (aT) or incorrect (aF)â€”and generating GUI state change captions. This produces rich supervision signals for both policy learning and downstream task generation.(3) Policy Update via Reinforcement Fine-Tuning. Based on the labeled execution data, positive and negative action steps are separated. We apply Group Relative Policy Optimization (GRPO) to reinforce correct actions, and Adversarial Imitation (AI) to suppress failure-prone behaviors. The updated policy is used for the next exploration round.(4) Task Update. The Curriculum Generator leverages feedback signals (ğ’¥) and GUI state transitions (ğ’) to propose more diverse and challenging tasks, thereby expanding the task frontier in a curriculum fashion.This process repeats over multiple curriculum phases, ultimately yielding a specialized agent policy capable of mastering complex operations in the given software environment."
},{
  "research_proposal": "Fine-tune open-source 7B LLMs (LLaMA-3, Qwen, DeepSeek, Gemma) on the VERT dataset for automated HDL assertion generation using LoRA across all layers. Conduct hyperparameter sweeps for rank and alpha, and employ 4-bit quantization with Unsloth optimizations to maximize accuracy and efficiency.",
  "benchmark": ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "Accuracy"],
  "performance_metrics": [0.83, 0.87, 0.83, 0.86, 0.97],
  "paper_link": "https://arxiv.org/html/2508.07371v1",
  "full_text": "AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generationzhongyi@bupt.edu.cn \surHongchao Liu \surDi Zhao [ [ [AbstractAs the complexity of software systems continues to increase, the demand for automated testing and maintenance tools is growing exponentially. To meet this urgent need, we propose a new assertion generation method based on Hardware Description Language (HDL). This method combines a lightweight, parameter-adjustable large language model (LLM) with the Unsloth platform to automatically generate test cases, thereby significantly reducing training costs without sacrificing accuracy or generalization performance. Empirical evaluation shows that our method can efficiently generate assertions that strictly conform to the hardware logic. This framework provides a robust and flexible solution to modern software testing and maintenance challenges. https://github.com/liusu-orange/AutoAssert-1 and https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.keywords: Assertion generation, Large Language Model, Fine-tuning, Hardware description language, Automated testing1 IntroductionWith the rapid development of hardware systems, their complexity has grown exponentially, making manual testing and maintenance both time-consuming and error-prone. Traditional automated test generation methods in hardware verification [1, 2] often rely on rigid rule-based systems or manual scripting, lacking the flexibility to handle modern design complexities. For example, in the verification of multi-core processors, the combinatorial explosion of states makes exhaustive testing impossible, often leaving critical edge cases uncovered, only 60% to 70% of edge cases are covered. [3] Another notable case is the Intel Pentium FDIV bug, where a flaw in the floating-point division unit escaped detection due to incomplete test coverage, costing the company over $475 million in recalls.To address these issues, the progress of large language models (LLMs) [4, 5, 6] has shown great potential in automating complex tasks such as code generation and test case synthesis. However, fine-tuning these models for specific domain tasks (e.g., generating assertions from hardware description languages) faces several critical challenges in hardware verification:Firstly, the validation system for training large models mainly relies on specific domain datasets, such as annotated RTL code or SystemVerilog assertions [7, 8]. However, creating such datasets requires a significant amount of human effort and deep professional knowledge, which often leads to a scarcity of high-quality training samples.Secondly, general-purpose pre-trained language models often perform poorly when applied to hardware-related tasks because their understanding of hardware-specific grammar and semantics is limited. Although they perform well in natural language processing, they usually have difficulty simulating the inherent time constraints and concurrent characteristics of hardware systems, resulting in grammatically invalid or logically incorrect generated outputs.Thirdly, fine-tuning to apply large language models to the hardware domain requires a large amount of computing resources, such as distributed frameworks and high-performance GPUs. Due to the need to frequently update model parameters based on evolving architectures (such as RISC-V, ARMv9), this increases the overall training cost. For example, training a model with 7 billion parameters may require an 8-fold A100 GPU cluster to run for 48 hours, with a cost of approximately $5000.This paper presents a novel approach that applies the pre-trained large-scale language model to the assertion generation task in hardware testing by minimizing parameter updates [9]. Our method utilizes the Unsloth platform [10], significantly reducing the computational cost while maintaining high accuracy and generalization ability. In addition, we also adopt the lightweight Lora fine-tuning strategy [11, 12] to freeze the original parameter model and only update a small number of Lora parameters. This method not only reduces the cost of model adaptation but also provides a new idea for deploying large-scale language models in resource-constrained environments. The contributions of this article are as follows:    â€¢    We introduce a lightweight methodology that achieves competitive performance with minimal parameter updates, making it feasible for practical deployment.    â€¢    Our work specifically targets assertion generation for hardware testing, addressing a critical gap in automated verification tools.    â€¢    Through extensive experiments, we demonstrate that our approach outperforms traditional rule-based methods and reduces the computational burden compared to full-model fine-tuning.2 Related workThe development of assertion generation techniques in hardware verification has undergone an evolution process from traditional rule-based methods to modern data-driven methods. Early research mainly relied on formal languages and manual templates, such as the Attribute Description Language and SystemVerilog Assertions (SVA) [13][2], which provided standardized assertion syntax specifications. However, these methods required designers to have professional hardware knowledge and were difficult to adapt to new design requirements. Template-based tools like FoCs developed by IBM could automatically generate some assertions, but still had issues of insufficient flexibility and requiring a lot of manual intervention [1].With the development of machine learning technology, researchers began to explore data-driven assertion generation methods [14]. In supervised learning [15, 16], there were studies that used support vector machines to analyze simulation trajectories to predict assertion templates, while works like DeepAssert used LSTM networks to generate assertions from HDL code [17, 18]. However, these methods generally faced difficulties in obtaining labeled data. Reinforcement learning methods such as RLAssert modeled assertion generation as a reinforcement learning problem, although it reduced manual intervention, due to the need to design complex reward functions, it also had high computational costs.In recent years, large language models [4][11] have demonstrated strong capabilities in code generation tasks, providing new solutions for assertion generation. General large models like GPT-3 [19] can generate code snippets similar to assertions based on natural language prompts, but they have the problem of insufficient domain specificity and low prediction accuracy for specific hardware description languages. VeriGen [20] and other studies adapted pre-trained models to the SystemVerilog assertion generation task through full parameter fine-tuning, although the results were significant, the computational resource consumption was huge. Emerging efficient fine-tuning platform like Unsloth [10][21], although they could significantly reduce computational costs, have not yet been fully explored in the field of hardware verification.In summary, existing methods still have significant deficiencies in domain adaptability and computational efficiency, which provide an innovative space for this research. Our proposed lightweight supervised fine-tuning method [11][9] not only significantly reduces the parameter update volume but also improves the modelâ€™s professionalism through domain adaptation, ensuring performance while significantly reducing computational costs. Through systematic experiments, this method shows significant advantages over traditional rule-based methods and full parameter fine-tuning, providing an efficient and practical new approach for hardware assertion generation.3 Framework and Method3.1 Framework OverviewThe hardware assertion generation framework proposed in this study is based on the Unsloth efficient fine-tuning platform and adopts the LoRA paradigm to adapt large language models to specific domains [5][9]. You can see the architecture in Fig. 1.This framework maintains the parameter freezing of the base model while only performing low-rank adaptation on the key projection layers of the attention mechanism and the feedforward network, achieving the optimal balance between computational efficiency and model performance.Refer to captionFigure 1: This framework achieves the best balance between computational efficiency and model performance by keeping the parameters of the base model frozen while only making low-rank adaptive adjustments to the key projection layers in the attention mechanism and the feedforward network.3.2 LoRA Fine-Tuning PrincipleLoRA [11] (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that approximates full parameter updates through low-rank decomposition, as illustrated in Fig. 1. Given an original weight matrix Wâˆˆâ„dÃ—k in a pre-trained model, LoRA introduces two low-rank matrices: Aâˆˆâ„dÃ—r and Bâˆˆâ„rÃ—k, where the rank r satisfies râ‰ªminâ¡(d,k). The matrix A is randomly initialized, while B is initialized to zero. The parameter update Î”â€‹W is computed as shown in the equation (1) and equation (2).	Î”â€‹W=Bâ€‹A		(1)The fine-tuned weight matrix Wâ€² is then given by:	Wâ€²=W+Î”â€‹W=W+Bâ€‹A		(2)The rank r is typically much smaller than the original dimensions d and k (e.g., r=8 for d,kâ‰¥1024). This reduces the number of trainable parameters from dÃ—k to rÃ—(d+k), significantly lowering memory and computational costs.During backpropagation, gradients flow only through B and A, leaving the original weights W frozen. This prevents catastrophic forgetting of pre-trained knowledge while adapting to new tasks.The matrix A is initialized with random Gaussian noise scaled by 1/r, while B is zero-initialized to ensure Î”â€‹W=0 at the start of training. This stabilizes early fine-tuning.In our study, we apply LoRA to critical projection layers in the Transformer architecture, targeting the attention layers (q_proj, k_proj, v_proj, and o_proj) and feed-forward network layers (gate_proj, up_proj, and down_proj). This layer selection strategy is theoretically grounded in the observation that attention layers directly determine token interaction patterns crucial for semantic understanding, while feed-forward network projections govern nonlinear feature transformations essential for domain knowledge encoding. By setting the rank r=16, we reduce the trainable parameters in each target layer to less than 0.1% of the original parameters. For typical dimensions of d=4096 and k=4096, this yields a compression ratio of approximately 0.52%, demonstrating the remarkable efficiency of our approach while maintaining model performance.4 Dataset and Experiments4.1 Introduction of datasetThis study uses the VERT dataset [8] to evaluate and train the hardware assertion generation task. You can find this dataset at https://github.com/AnandMenon12/VERT. The dataset contains 20,000 pairs of carefully annotated Verilog/SystemVerilog code and assertions, and it is divided into various categories to ensure coverage of all scenarios in the hardware domain. Among them, 460 categories are derived from the Xiangshan processor.In the experiment, we selected 18,000 pairs of data from the dataset for fine-tuning the model as the training set, 1,000 pairs of data for adjusting the hyperparameters as the validation set, and another 1,000 pairs of data for the final performance evaluation as the test set. All assertions in the dataset strictly follow the SystemVerilog assertion (SVA) syntax standard and cover complete verification scenarios ranging from basic logic constraints to timing requirements.4.2 Experimental Design4.2.1 Pretrained Model ComparisonAs shown in Table 1, we evaluate four open-source LLMs in the context of hardware assertion generation. Among them, LLaMA-3-7B [22] achieves the best performance, primarily due to its English-centric pretraining.In contrast, Qwen-7B [23], with its multilingual orientation and Chinese-focused training corpus, exhibits weaker compatibility with HDL syntax despite its robust handling of extended contexts. DeepSeek-LLM-7B [24], designed for Chinese-English code blending, introduces unnecessary complexity when applied to monolingual hardware domains. Similarly, although Gemma-7B [25] features low-latency optimizations, this comes at the cost of reduced accuracy in generating temporal logic assertions. These findings suggest that for HDL-related tasks, linguistic precision and syntactic fidelity are more critical than multilingual capabilities or inference speed.In this comparison, we use five commonly adopted evaluation metrics for text generation tasks: BLEU, ROUGE-1, ROUGE-2, ROUGE-L, and Accuracy. Below are their definitions and corresponding formulas.BLEU (Bilingual Evaluation Understudy) measures the similarity between generated text and reference text. BLEU calculates the precision of multiple n-grams and includes a brevity penalty (BP) to penalize short outputs. The BLEU score is computed as shown in equation(3):	BLEU=BPâ‹…expâ¡(âˆ‘n=1Nwnâ€‹logâ¡pn)		(3)Here, pn is the precision of n-grams, wn is the weight (usually uniform),c is the length of the candidate sentence and r is the length of the reference sentence and BP is the brevity penalty defined in equation(4):	BP={1if â€‹c>re1âˆ’rcif â€‹câ‰¤r		(4)where c is the length of the candidate sentence and r is the length of the reference sentence.ROUGE-N evaluates the recall of n-grams between the generated and reference texts. ROUGE-1 computes unigram recall, while ROUGE-2 computes bigram recall. The ROUGE-N score is given in equation(5):	ROUGE-N=âˆ‘overlapnâˆ‘referencen		(5)ROUGE-L is based on the Longest Common Subsequence (LCS), capturing sentence-level structure similarity. The F-score version of ROUGE-L is defined in equation(6):	ROUGE-L=(1+Î²2)â‹…Râ‹…PR+Î²2â‹…P		(6)where the precision P and recall R are calculated as shown in equation(7):	P=LCSCandidate Length,R=LCSReference Length		(7)Accuracy reflects the proportion of correct predictions among all predictions. The accuracy score is given in equation(8):	Accuracy=Correct PredictionsTotal Predictions		(8)Table 1: Performance comparison of different pretrained models based on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L metricsModel 	BLEU 	ROUGE-1 	ROUGE-2 	ROUGE-L 	AccuracyQwen-7B 	0.83 	0.81 	0.82 	0.85 	0.96DeepSeek-LLM-7B 	0.81 	0.86 	0.82 	0.84 	0.95Gemma-7B 	0.82 	0.84 	0.81 	0.84 	0.96LLaMA-3-7B 	0.83 	0.87 	0.83 	0.86 	0.974.2.2 Comparison of Closed-Source Large ModelsAs shown in Table 2, we analyzed four of the current mainstream closed-source large models [26] [4]. The experimental results show that these models have demonstrated outstanding performance in the assertion generation task. However, there are also some issues, such as the opacity of the generation process and the high operational costs. Therefore, semiconductor companies are increasingly inclined to use open-source models for fine-tuning and local deployment. This preference mainly stems from the excellent controllability provided by the open-source framework, which can perform deep architecture optimization based on specific hardware configurations and performance requirements.Table 2: Performance comparison of different Closed-Source Large models based on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L metricsModel 	BLEU 	ROUGE-1 	ROUGE-2 	ROUGE-L 	AccuracyGPT 4 	0.82 	0.81 	0.82 	0.87 	0.96Grok 3 	0.81 	0.83 	0.80 	0.86 	0.96Gemimi 2.5 Flash 	0.81 	0.84 	0.83 	0.82 	0.95Claude Sonnet4 	0.81 	0.86 	0.83 	0.84 	0.984.2.3 LoRA Hyperparameter StudyAs shown in Table 3 and Table 4, we evaluate LoRA rank (r) configurations (r=8,16,32) on LLaMA-3-7B with fixed hyperparameters (Î±=16, dropout = 0). Performance improves with higher ranks (97% accuracy at r=16 vs. 91% accuracy at r=8), but exhibits diminishing returns: the gain from r=16 to r=32 is significantly smaller than from r=8 to r=16, while requiring double the training parameters(41M â†’ 83M). This suggests r=16 achieves the optimal balance, as the marginal improvement at r=32 does not justify its computational overhead for hardware assertion generation tasks.As shown in Table 5. We evaluate the impact of LoRA alpha values (8, 16, 32) on model performance. The results demonstrate that alpha=16 consistently outperforms the other values (alpha=8 and alpha=32) in accuracy, BLEU, and ROUGE metrics. Specifically, alpha=8 may limit the modelâ€™s learning capacity due to its small size, while alpha=32 could lead to overfitting or instability. Alpha=16 strikes an optimal balance between learning and generalization.Table 3: Performance comparison of different rank (r) settings based on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L metricsR value 	BLEU 	ROUGE-1 	ROUGE-2 	ROUGE-L 	Accuracy8 	0.79 	0.81 	0.77 	0.81 	0.9116 	0.83 	0.87 	0.83 	0.86 	0.9732 	0.84 	0.87 	0.85 	0.88 	0.97Table 4: Comparison of the number of training parameters and running time under different rank settingsR value 	Number 	Percentage 	Running time8 	21M 	0.26% 	95 min16 	41M 	0.52% 	175 min32 	83M 	1.10% 	315 minTable 5: Performance comparison of different alpha settings based on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L metricsAlpha value 	BLEU 	ROUGE-1 	ROUGE-2 	ROUGE-L 	Accuracy8 	0.78 	0.80 	0.76 	0.81 	0.8916 	0.83 	0.87 	0.83 	0.86 	0.9732 	0.81 	0.84 	0.81 	0.82 	0.93As shown in Table 6, we systematically evaluate three LoRA module configurations with fixed rank r=16: (1) Attention Layers (q_proj, k_proj, v_proj, o_proj), which govern token-to-token interactions through query-key-value transformations and attention head fusion; (2) FFN Layers (gate_proj, up_proj, down_proj), responsible for nonlinear feature space transformations via gating and dimensional scaling; and (3) All Layers, combining both groups for joint adaptation.The experimental results demonstrate that applying LoRA to All Layers yields the best performance. The combined configuration achieves a 4.3% accuracy improvement over attention-only adaptation and 6.5% accuracy over FFN-only adaptation, with only a 0.18% increase in trainable parameters compared to single-module configurations. This indicates that in the task of generating hardware assertions, it is crucial to fully adapt to the architectural components, as no single type of module alone can fully capture the complex semantics of the hardware description language.Table 6: Performance comparison of different target module groups based on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L metricsModule Group 	BLEU 	ROUGE-1 	ROUGE-2 	ROUGE-L 	AccuracyAttention Layers 	0.79 	0.86 	0.77 	0.81 	0.93FFN Layers 	0.77 	0.84 	0.75 	0.82 	0.91All Layers 	0.83 	0.87 	0.83 	0.86 	0.974.2.4 Fine-tuning platformIn our training process, we adopted the Unsloth platform, which has many advantages compared to traditional model fine-tuning frameworks such as Hugging Face Transformers and PyTorch [27]. Although these mature platforms have become the default standard for fine-tuning large language models, they often have some persistent issues, including high GPU memory consumption, low computational efficiency, and unstable performance during the Low-Rank Adaptation (LoRA) process.Unsloth directly addresses these challenges through three pioneering methods. Firstly, it uses 4-bit quantization technology to store the weights of the base model, reducing GPU memory usage by approximately 70%, effectively alleviating the memory bottleneck problem. Secondly, the platform integrates fusion kernel operations, optimizing the efficiency of core computing tasks. Finally, Unsloth introduces gradient clipping and layer normalization calibration mechanisms specifically for dealing with the numerical instability issues commonly encountered in LoRA training. These measures collectively reduce the risk of gradient explosion and overflow, ensuring a more stable and reliable training process.4.2.5 Training DetailsThe training loss is shown in Fig. 2. All experiments were conducted on an NVIDIA 4090 with an initial learning rate of 2e-4. We maintained a batch size of 8 and a sequence length limit of 2048 tokens. Each configuration ran for 2000 steps. Evaluation combined text matching metrics (BLEU, ROUGE) with formal syntax checking and functional simulation verification. The optimal configuration (r=16, all target layers) achieved 97% functional accuracy on the test set, demonstrating significant improvement over baselines.Refer to captionFigure 2: As can be seen from the graph, in the early stage of the iteration, the loss value drops rapidly, and then gradually stabilizes. Overall, it shows a trend of first decreasing rapidly and then remaining stable, indicating that the model gradually converges during the training process.Refer to captionFigure 3: After entering the Verilog code, click on â€Generate Assertionâ€ and wait for the model to respond.Refer to captionFigure 4: Model response completed, generating assertions corresponding to the Verilog code.4.2.6 UI interface deploymentTo simplify the deployment process of our model training, we developed an easy-to-operate visualization interface that can automatically generate assertions from Verilog code. This tool enables users to easily upload their Verilog code, after which the system will perform a quick parsing and generate the corresponding assertion logic. The interface aims to enhance efficiency and usability, eliminating complex configuration requirements and achieving a seamless end-to-end workflow from code input to assertion generation. A specific demonstration is shown in the Fig. 3 and Fig. 4.5 Discussion5.1 The issue of inconsistent properties in the assertionAs shown in Fig. 5, our model generated corresponding assertions based on the Verilog code. However, in fact, these assertions are different from those of the labels in the dataset, as shown in the Fig. 6. Their properties are not the same. But this does not affect the functionality of the assertions. This is just a custom attribute name. Therefore, our method of evaluating the accuracy rate only compares the content after the property, and does not require the property to be consistent.5.2 Dataset Simplicity and Future DirectionsThe extremely high accuracy demonstrated in this study is due to the overly simplistic structure and content of the dataset. This dataset mainly focuses on a series of limited hardware verification tasks. However, as the complexity of these tasks increases - for example, by introducing more diverse hardware functions or multimodal inputs - merely relying on LoRA fine-tuning is insufficient.To address these challenges, future research should delve into the collaborative integration of Direct Preference Optimization (DPO) [28] and LoRA fine-tuning. DPO differs from traditional reinforcement learning (RL) methods such as PPO [29, 30, 31], which rely on complex reward functions and iterative policy optimization. Instead, DPO reimagines the pattern of preference optimization as a more stable and computationally efficient supervised learning process. This innovative approach circumvents the severe instability problems faced by traditional RL techniques while reducing the high computational costs. Therefore, a framework that combines DPO and LoRA has the potential to achieve higher robustness and adaptability, enabling the model to handle increasingly complex and diverse verification scenarios.Refer to captionFigure 5: The â€questionâ€ is followed by Verilog code, and the â€answerâ€ is the corresponding assertion. The above is the assertion corresponding to the code generated by our model.Refer to captionFigure 6: The above assertion is the true assertion within the dataset.6 ConclusionThis paper presents an efficient method for automatically generating assertions in hardware verification based on Verilog code. This method utilizes the LoRA-based fine-tuning technology within the Unsloth platform. Our approach significantly reduces the computational cost when extracting assertions from HDL code while maintaining high accuracy and generality. Experimental results show that this method can generate assertions that are both grammatically correct and semantically meaningful. Compared with traditional techniques, our method performs better and does not require a large amount of resource demand for extensive parameter fine-tuning. These results confirm the feasibility of lightweight optimization strategies in the hardware domain and provide a practical deployment method for resource-constrained environments.Looking to the future, this research lays the foundation for more intelligent and adaptive tools in the field of automated testing and verification, especially in hardware-centric environments. Future extended versions may adopt reinforcement learning or few-shot learning models to further improve the quality and adaptability of assertions.For instance, the framework based on DPO can be utilized to implement reinforcement learning. By integrating reward functions such as grammatical validity (e.g., SMT verification), semantic correctness, and mutation coverage, the improvement rate of fault detection can be optimized to 15% - 20%. Moreover, AdapterFusion [32] can be incorporated to achieve few-shot learning. By using only approximately 500 assertion templates during training, 98% of the data requirements can be reduced, while still maintaining a performance of up to 90%. Through this approach, we can further reduce the required training data and achieve fine-tuning for more scenarios.7 ContributionYi Zhong developed the AutoAssert 1: a LoRA fine-tuned LLM model for assertion generation, Hongchao Liu did the data preprocessing and Dr. Di Zhao directed the entire project.8 AcknowledgmentWe are grateful to Ms. Dantong Liu from ARM and Mr. Xu Zhang and Mr. Hong Chen from Intel Labs China for their helpful discussions. We also thank Mr. Yuan Gao for providing the Gitee code repository of AutoAssert1.References    \bibcommenthead    Foster and Woodcock [2016]Foster, S., Woodcock, J.: Towards verification of cyber-physical systems with utp and isabelle/hol. In: Concurrency, Security, and Puzzles: Essays Dedicated to Andrew William Roscoe on the Occasion of His 60th Birthday, pp. 39â€“64. Springer, Berlin, Germany (2016)Spear [2008]Spear, C.: SystemVerilog for Verification: a Guide to Learning the Testbench Language Features. Springer, Publisher location not specified (2008)Gupta [1992]Gupta, A.: Formal hardware verification methods: A survey. Formal Methods in System Design 1(2), 151â€“238 (1992)Brown et al. [2020]Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877â€“1901 (2020)Chen et al. [2021]Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.D.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al.: Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021)An et al. [2024]An, S., Ma, Z., Lin, Z., Zheng, N., Lou, J.-G., Chen, W.: Make your llm fully utilize the context. Advances in Neural Information Processing Systems 37, 62160â€“62188 (2024)Lu et al. [2024]Lu, Y., Liu, S., Zhang, Q., Xie, Z.: Rtllm: An open-source benchmark for design rtl generation with large language model. In: 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pp. 722â€“727 (2024). IEEEMenon et al. [2025]Menon, A., Miftah, S.S., Kundu, S., Kundu, S., Srivastava, A., Raha, A., Sonnenschein, G.T., Banerjee, S., Mathaikutty, D., Basu, K.: Enhancing large language models for hardware verification: A novel systemverilog assertion dataset. arXiv preprint arXiv:2503.08923 (2025)Howard and Ruder [2018]Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146 (2018)Rajbhandari et al. [2020]Rajbhandari, S., Rasley, J., Ruwase, O., He, Y.: Zero: Memory optimizations toward training trillion parameter models. In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1â€“16 (2020). IEEEHu et al. [2022]Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. ICLR 1(2), 3 (2022)Lester et al. [2021]Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021)Bergeron [2012]Bergeron, J.: Writing Testbenches: Functional Verification of HDL Models. Springer, Publisher location not specified (2012)Pulavarthi et al. [2024]Pulavarthi, V., Nandal, D., Dan, S., Pal, D.: Assertionbench: A benchmark to evaluate large-language models for assertion generation. arXiv preprint arXiv:2406.18627 (2024)Iman [2025]Iman, M.R.H.: Advanced Techniques for Assertion-Based Verification in Hardware Designs Using Data Mining Algorithms. Springer, Publisher location not specified (2025)Yu et al. [2022]Yu, H., Lou, Y., Sun, K., Ran, D., Xie, T., Hao, D., Li, Y., Li, G., Wang, Q.: Automated assertion generation via information retrieval and its integration with deep learning. In: Proceedings of the 44th International Conference on Software Engineering, pp. 163â€“174 (2022)Xue et al. [2009]Xue, H., Yang, Q., Chen, S.: Svm: Support vector machines. In: The Top Ten Algorithms in Data Mining, pp. 51â€“74. Chapman and Hall/CRC, Publisher location not specified (2009)Wang et al. [2024]Wang, H., Xu, T., Wang, B.: Deep multiple assertions generation. In: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering, pp. 1â€“11 (2024)Floridi and Chiriatti [2020]Floridi, L., Chiriatti, M.: Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines 30, 681â€“694 (2020)Thakur et al. [2024]Thakur, S., Ahmad, B., Pearce, H., Tan, B., Dolan-Gavitt, B., Karri, R., Garg, S.: Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems 29(3), 1â€“31 (2024)[21]Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., Ma, Y.L.: Unified efficient fine-tuning of 100+ language models. arxiv 2024. arXiv preprint arXiv:2403.13372Touvron et al. [2023]Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)Bai et al. [2023]Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023)Liu et al. [2024]Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al.: Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024)Team et al. [2024]Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., RiviÃ¨re, M., Kale, M.S., Love, J., et al.: Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024)Team et al. [2023]Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., Millican, K., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)Paszke et al. [2019]Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019)Rafailov et al. [2023]Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., Finn, C.: Direct preference optimization: Your language model is secretly a reward model. Advances in neural information processing systems 36, 53728â€“53741 (2023)Schulman et al. [2017]Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)Pan et al. [2023]Pan, W., Ouyang, J., Liu, Y., Zhao, D., Su, T.: Fundamentals of Pattern Recognition and Machine Learning,Translated Work, (2023). China Machine PressZhao and et al. [2018]Zhao, D., al.: Reinforcement Learning,Translated Work, (2018). China Machine PressPfeiffer et al. [2020]Pfeiffer, J., Kamath, A., RÃ¼cklÃ©, A., Cho, K., Gurevych, I.: Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247 (2020)"
}
