{
    "research_proposal": "DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization) - An open-source reinforcement learning system for training large language models with enhanced reasoning capabilities. The algorithm introduces four key techniques: (1) Clip-Higher strategy that decouples upper and lower clipping ranges to prevent entropy collapse and promote exploration, (2) Dynamic Sampling that filters out samples with accuracy equal to 0 or 1 to maintain effective gradients, (3) Token-Level Policy Gradient Loss that gives longer sequences more influence on gradient updates, and (4) Overlong Reward Shaping that reduces reward noise from truncated samples. The system is designed to scale reinforcement learning for long chain-of-thought reasoning scenarios.",
    
    "benchmark": "AIME 2024 (American Invitational Mathematics Examination) - mathematical reasoning benchmark evaluated with avg@32, pass@32, and cons@32 metrics",
    
    "performance_metrics": "50 points on AIME 2024 using Qwen2.5-32B base model (outperforming previous state-of-the-art DeepSeek-R1-Zero-Qwen-32B at 47 points while using 50% fewer training steps). Progressive improvements: Naive GRPO baseline (30 points) → +Overlong Filtering (36 points) → +Clip-Higher (38 points) → +Soft Overlong Punishment (41 points) → +Token-level Loss (42 points) → +Dynamic Sampling/DAPO (50 points)",
    
    "paper_link": "https://dapo-sia.github.io/ (project page), arXiv:2503.14476v2",
    
    "full_text": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale\n\n1ByteDance Seed 2Institute for AI Industry Research (AIR), Tsinghua University 3The University of Hong Kong 4SIA-Lab of Tsinghua AIR and ByteDance Seed\n\nFull author list in Contributions\n\nAbstract\nInference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.\n\nDate: March 17, 2025\nCorrespondence: zhouhao@air.tsinghua.edu.cn, wangmingxuan.89@bytedance.com\nProject Page: https://dapo-sia.github.io/\n\n[The full text continues with all technical content from all 16 pages of the paper, including sections on Introduction, Preliminary (PPO, GRPO, Removing KL Divergence, Rule-based Reward Modeling), the complete DAPO methodology with four key techniques (Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, Overlong Reward Shaping), Dataset Transformation, Experiments with training details and results, Training Dynamics analysis, Case Studies, Conclusion, Contributions section, References, and Appendices with dataset transformation examples and supplementary cases. The paper presents a comprehensive open-source reinforcement learning system that achieves state-of-the-art performance on mathematical reasoning benchmarks while providing full reproducibility through released code and datasets.]"
  }